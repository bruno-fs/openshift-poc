{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from kubernetes import client\n",
    "from openshift.dynamic import DynamicClient\n",
    "from openshift.helper.userpassauth import OCPLoginConfiguration\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "# disable pesky urllib3 ssl warnings.\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "OCP_APIHOST = os.environ[\"OCP_APIHOST\"]\n",
    "OCP_USERNAME = os.environ[\"OCP_USERNAME\"]\n",
    "OCP_PASSWORD = os.environ[\"OCP_PASSWORD\"]\n",
    "QPC_BASE_URL = os.environ[\"QPC_BASE_URL\"]\n",
    "QPC_USERNAME = os.environ[\"QPC_USERNAME\"]\n",
    "QPC_PASSWORD = os.environ[\"QPC_PASSWORD\"]\n",
    "QPC_REPORT_ID = os.environ[\"QPC_REPORT_ID\"]\n",
    "OUTPUT_DIR = os.environ[\"OUTPUT_DIR\"]\n",
    "\n",
    "kubeConfig = OCPLoginConfiguration(ocp_username=OCP_USERNAME, ocp_password=OCP_PASSWORD)\n",
    "kubeConfig.host = OCP_APIHOST\n",
    "kubeConfig.verify_ssl = False\n",
    "\n",
    "kubeConfig.get_token()\n",
    "\n",
    " \n",
    "k8s_client = client.ApiClient(kubeConfig)\n",
    " \n",
    "dyn_client = DynamicClient(k8s_client)\n",
    "# v1_projects = dyn_client.resources.get(api_version='project.openshift.io/v1', kind='Project')\n",
    "# project_list = v1_projects.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prometheus-k8s-openshift-monitoring.apps.sharedocp4upi412ovn.lab.upshift.rdu2.redhat.com/api'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routes_client = dyn_client.resources.get(api_version='route.openshift.io/v1', kind='Route')\n",
    "routes = routes_client.get(namespace=\"openshift-monitoring\", field_selector=\"metadata.name=prometheus-k8s\").items\n",
    "prometheus_host = routes[0][\"spec\"][\"host\"] + routes[0][\"spec\"].get(\"path\", \"/api\")\n",
    "prometheus_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://console-openshift-console.apps.sharedocp4upi412ovn.lab.upshift.rdu2.redhat.com\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sockets</th>\n",
       "      <th>instance</th>\n",
       "      <th>label_node_hyperthread_enabled</th>\n",
       "      <th>label_node_openshift_io_os_id</th>\n",
       "      <th>label_node_role_kubernetes_io_master</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>master-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>false</td>\n",
       "      <td>rhcos</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>master-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>false</td>\n",
       "      <td>rhcos</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>master-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>false</td>\n",
       "      <td>rhcos</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>false</td>\n",
       "      <td>rhcos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>false</td>\n",
       "      <td>rhcos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>false</td>\n",
       "      <td>rhcos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sockets                                           instance  \\\n",
       "0       4  master-0.sharedocp4upi412ovn.lab.upshift.rdu2....   \n",
       "1       4  master-1.sharedocp4upi412ovn.lab.upshift.rdu2....   \n",
       "2       4  master-2.sharedocp4upi412ovn.lab.upshift.rdu2....   \n",
       "3       4  worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....   \n",
       "4       4  worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....   \n",
       "5       4  worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....   \n",
       "\n",
       "  label_node_hyperthread_enabled label_node_openshift_io_os_id  \\\n",
       "0                          false                         rhcos   \n",
       "1                          false                         rhcos   \n",
       "2                          false                         rhcos   \n",
       "3                          false                         rhcos   \n",
       "4                          false                         rhcos   \n",
       "5                          false                         rhcos   \n",
       "\n",
       "  label_node_role_kubernetes_io_master  \n",
       "0                                 true  \n",
       "1                                 true  \n",
       "2                                 true  \n",
       "3                                  NaN  \n",
       "4                                  NaN  \n",
       "5                                  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "console_client = dyn_client.resources.get(\n",
    "    api_version=\"config.openshift.io/v1\", kind=\"Console\"\n",
    ")\n",
    "console_url = console_client.get().items[0][\"status\"][\"consoleURL\"]\n",
    "print(console_url)\n",
    "\n",
    "# prometheus_query=\"node_role_os_version_machine:cpu_capacity_sockets:sum\"\n",
    "#prometheus_query=\"workload:capacity_physical_cpu_cores:sum\" # https://github.com/openshift/cluster-monitoring-operator/blob/1c595854ab4acbefd70446b54ae314016f400d8e/jsonnet/rules.libsonnet#L249-L251\n",
    "# prometheus_query=\"node_role_os_version_machine:cpu_capacity_cores:sum\" # https://github.com/openshift/cluster-monitoring-operator/blob/f365468055303edc8a4bb93e494bcbdedcf587bd/manifests/0000_50_cluster-monitoring-operator_04-config.yaml#L248-L253\n",
    "# slightly modified version of what cluster-monitoring-operator uses for getting cpu_capacity_sockets\n",
    "prometheus_query = '''\n",
    "count by(instance, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) \n",
    "(max \n",
    "    by(node, instance, package, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra)\n",
    "    (cluster:cpu_core_node_labels)\n",
    ")\n",
    "'''\n",
    "# other interesting metrics\n",
    "# - node_cpu_info\n",
    "# - machine_cpu_sockets\n",
    "# - machine_cpu_cores\n",
    "# - machine_cpu_physical_cores\n",
    "# - node_network_info (macaddresses)\n",
    "# - cluster_infrastructure_provider\n",
    "\n",
    "# r.status_code, r.json()\n",
    "\n",
    "def prometheus_json_to_df(query, value_key):\n",
    "    response = requests.get(\n",
    "        f\"https://{prometheus_host}/v1/query?query={query}\",\n",
    "        verify=False,\n",
    "        headers=kubeConfig.api_key,\n",
    "    )\n",
    "    assert response.ok\n",
    "    return pd.DataFrame(\n",
    "        {value_key:d[\"value\"][1], **d[\"metric\"]}\n",
    "        for d in response.json()[\"data\"][\"result\"]\n",
    "    )\n",
    "\n",
    "prometheus_json_to_df(prometheus_query, \"sockets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>?</th>\n",
       "      <th>__name__</th>\n",
       "      <th>cachesize</th>\n",
       "      <th>container</th>\n",
       "      <th>core</th>\n",
       "      <th>cpu</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>family</th>\n",
       "      <th>instance</th>\n",
       "      <th>job</th>\n",
       "      <th>microcode</th>\n",
       "      <th>model</th>\n",
       "      <th>model_name</th>\n",
       "      <th>namespace</th>\n",
       "      <th>package</th>\n",
       "      <th>pod</th>\n",
       "      <th>service</th>\n",
       "      <th>stepping</th>\n",
       "      <th>vendor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>node-exporter-pj88z</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>node-exporter-bvfx9</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>node-exporter-9rnhx</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>node-exporter-nkd8c</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>node-exporter-9gljp</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>node-exporter-2g925</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>1</td>\n",
       "      <td>node-exporter-pj88z</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>1</td>\n",
       "      <td>node-exporter-bvfx9</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>1</td>\n",
       "      <td>node-exporter-9rnhx</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>1</td>\n",
       "      <td>node-exporter-nkd8c</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>1</td>\n",
       "      <td>node-exporter-9gljp</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>1</td>\n",
       "      <td>node-exporter-2g925</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>2</td>\n",
       "      <td>node-exporter-pj88z</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>2</td>\n",
       "      <td>node-exporter-bvfx9</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>2</td>\n",
       "      <td>node-exporter-9rnhx</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>2</td>\n",
       "      <td>node-exporter-nkd8c</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>2</td>\n",
       "      <td>node-exporter-9gljp</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>2</td>\n",
       "      <td>node-exporter-2g925</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>3</td>\n",
       "      <td>node-exporter-pj88z</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>3</td>\n",
       "      <td>node-exporter-bvfx9</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>3</td>\n",
       "      <td>node-exporter-9rnhx</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>3</td>\n",
       "      <td>node-exporter-nkd8c</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>3</td>\n",
       "      <td>node-exporter-9gljp</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>3</td>\n",
       "      <td>node-exporter-2g925</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ?       __name__ cachesize        container core cpu endpoint family  \\\n",
       "0   1  node_cpu_info    512 KB  kube-rbac-proxy    0   0    https     23   \n",
       "1   1  node_cpu_info    512 KB  kube-rbac-proxy    0   0    https     23   \n",
       "2   1  node_cpu_info    512 KB  kube-rbac-proxy    0   0    https     23   \n",
       "3   1  node_cpu_info    512 KB  kube-rbac-proxy    0   0    https     23   \n",
       "4   1  node_cpu_info    512 KB  kube-rbac-proxy    0   0    https     23   \n",
       "5   1  node_cpu_info    512 KB  kube-rbac-proxy    0   0    https     23   \n",
       "6   1  node_cpu_info    512 KB  kube-rbac-proxy    0   1    https     23   \n",
       "7   1  node_cpu_info    512 KB  kube-rbac-proxy    0   1    https     23   \n",
       "8   1  node_cpu_info    512 KB  kube-rbac-proxy    0   1    https     23   \n",
       "9   1  node_cpu_info    512 KB  kube-rbac-proxy    0   1    https     23   \n",
       "10  1  node_cpu_info    512 KB  kube-rbac-proxy    0   1    https     23   \n",
       "11  1  node_cpu_info    512 KB  kube-rbac-proxy    0   1    https     23   \n",
       "12  1  node_cpu_info    512 KB  kube-rbac-proxy    0   2    https     23   \n",
       "13  1  node_cpu_info    512 KB  kube-rbac-proxy    0   2    https     23   \n",
       "14  1  node_cpu_info    512 KB  kube-rbac-proxy    0   2    https     23   \n",
       "15  1  node_cpu_info    512 KB  kube-rbac-proxy    0   2    https     23   \n",
       "16  1  node_cpu_info    512 KB  kube-rbac-proxy    0   2    https     23   \n",
       "17  1  node_cpu_info    512 KB  kube-rbac-proxy    0   2    https     23   \n",
       "18  1  node_cpu_info    512 KB  kube-rbac-proxy    0   3    https     23   \n",
       "19  1  node_cpu_info    512 KB  kube-rbac-proxy    0   3    https     23   \n",
       "20  1  node_cpu_info    512 KB  kube-rbac-proxy    0   3    https     23   \n",
       "21  1  node_cpu_info    512 KB  kube-rbac-proxy    0   3    https     23   \n",
       "22  1  node_cpu_info    512 KB  kube-rbac-proxy    0   3    https     23   \n",
       "23  1  node_cpu_info    512 KB  kube-rbac-proxy    0   3    https     23   \n",
       "\n",
       "                                             instance            job  \\\n",
       "0   master-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "1   master-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "2   master-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "3   worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "4   worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "5   worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "6   master-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "7   master-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "8   master-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "9   worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "10  worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "11  worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "12  master-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "13  master-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "14  master-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "15  worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "16  worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "17  worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "18  master-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "19  master-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "20  master-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "21  worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "22  worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "23  worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "\n",
       "    microcode model                      model_name             namespace  \\\n",
       "0   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "1   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "2   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "3   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "4   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "5   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "6   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "7   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "8   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "9   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "10  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "11  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "12  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "13  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "14  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "15  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "16  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "17  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "18  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "19  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "20  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "21  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "22  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "23  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "\n",
       "   package                  pod        service stepping        vendor  \n",
       "0        0  node-exporter-pj88z  node-exporter        2  AuthenticAMD  \n",
       "1        0  node-exporter-bvfx9  node-exporter        2  AuthenticAMD  \n",
       "2        0  node-exporter-9rnhx  node-exporter        2  AuthenticAMD  \n",
       "3        0  node-exporter-nkd8c  node-exporter        2  AuthenticAMD  \n",
       "4        0  node-exporter-9gljp  node-exporter        2  AuthenticAMD  \n",
       "5        0  node-exporter-2g925  node-exporter        2  AuthenticAMD  \n",
       "6        1  node-exporter-pj88z  node-exporter        2  AuthenticAMD  \n",
       "7        1  node-exporter-bvfx9  node-exporter        2  AuthenticAMD  \n",
       "8        1  node-exporter-9rnhx  node-exporter        2  AuthenticAMD  \n",
       "9        1  node-exporter-nkd8c  node-exporter        2  AuthenticAMD  \n",
       "10       1  node-exporter-9gljp  node-exporter        2  AuthenticAMD  \n",
       "11       1  node-exporter-2g925  node-exporter        2  AuthenticAMD  \n",
       "12       2  node-exporter-pj88z  node-exporter        2  AuthenticAMD  \n",
       "13       2  node-exporter-bvfx9  node-exporter        2  AuthenticAMD  \n",
       "14       2  node-exporter-9rnhx  node-exporter        2  AuthenticAMD  \n",
       "15       2  node-exporter-nkd8c  node-exporter        2  AuthenticAMD  \n",
       "16       2  node-exporter-9gljp  node-exporter        2  AuthenticAMD  \n",
       "17       2  node-exporter-2g925  node-exporter        2  AuthenticAMD  \n",
       "18       3  node-exporter-pj88z  node-exporter        2  AuthenticAMD  \n",
       "19       3  node-exporter-bvfx9  node-exporter        2  AuthenticAMD  \n",
       "20       3  node-exporter-9rnhx  node-exporter        2  AuthenticAMD  \n",
       "21       3  node-exporter-nkd8c  node-exporter        2  AuthenticAMD  \n",
       "22       3  node-exporter-9gljp  node-exporter        2  AuthenticAMD  \n",
       "23       3  node-exporter-2g925  node-exporter        2  AuthenticAMD  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prometheus_json_to_df(\"node_cpu_info\", \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>?</th>\n",
       "      <th>__name__</th>\n",
       "      <th>address</th>\n",
       "      <th>broadcast</th>\n",
       "      <th>container</th>\n",
       "      <th>device</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>instance</th>\n",
       "      <th>job</th>\n",
       "      <th>namespace</th>\n",
       "      <th>operstate</th>\n",
       "      <th>pod</th>\n",
       "      <th>service</th>\n",
       "      <th>duplex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>lo</td>\n",
       "      <td>https</td>\n",
       "      <td>master-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-pj88z</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>lo</td>\n",
       "      <td>https</td>\n",
       "      <td>master-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-bvfx9</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>lo</td>\n",
       "      <td>https</td>\n",
       "      <td>master-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-9rnhx</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>lo</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-nkd8c</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>lo</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-9gljp</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>lo</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-2g925</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>0a:f2:eb:9c:58:fa</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>genev_sys_6081</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-nkd8c</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>2e:4d:54:f6:20:3b</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovs-system</td>\n",
       "      <td>https</td>\n",
       "      <td>master-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-9rnhx</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>42:bc:2b:12:79:0b</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>genev_sys_6081</td>\n",
       "      <td>https</td>\n",
       "      <td>master-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-bvfx9</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>5a:c6:b1:b9:b1:08</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovs-system</td>\n",
       "      <td>https</td>\n",
       "      <td>master-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-bvfx9</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>62:e9:b0:21:15:7c</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>genev_sys_6081</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-9gljp</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>96:35:10:66:ec:30</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>genev_sys_6081</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-2g925</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>9a:92:76:d3:ba:0a</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovs-system</td>\n",
       "      <td>https</td>\n",
       "      <td>master-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-pj88z</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>c2:9a:c7:c5:1e:db</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovs-system</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-nkd8c</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>c6:d8:7c:79:b2:01</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovs-system</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-2g925</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>de:c2:45:b0:c9:3c</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>genev_sys_6081</td>\n",
       "      <td>https</td>\n",
       "      <td>master-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-pj88z</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>e2:b8:48:6d:63:98</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>genev_sys_6081</td>\n",
       "      <td>https</td>\n",
       "      <td>master-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-9rnhx</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>ee:df:0d:ca:58:f2</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovs-system</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-9gljp</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:4b:0d:93</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ens3</td>\n",
       "      <td>https</td>\n",
       "      <td>master-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>up</td>\n",
       "      <td>node-exporter-9rnhx</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:6f:db:d8</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ens3</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>up</td>\n",
       "      <td>node-exporter-2g925</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:76:18:e9</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ens3</td>\n",
       "      <td>https</td>\n",
       "      <td>master-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>up</td>\n",
       "      <td>node-exporter-bvfx9</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:77:95:fb</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ens3</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>up</td>\n",
       "      <td>node-exporter-9gljp</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:c1:31:3f</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ens3</td>\n",
       "      <td>https</td>\n",
       "      <td>master-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>up</td>\n",
       "      <td>node-exporter-pj88z</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:e2:fe:8b</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ens3</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>up</td>\n",
       "      <td>node-exporter-nkd8c</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ?           __name__            address          broadcast  \\\n",
       "0   1  node_network_info  00:00:00:00:00:00  00:00:00:00:00:00   \n",
       "1   1  node_network_info  00:00:00:00:00:00  00:00:00:00:00:00   \n",
       "2   1  node_network_info  00:00:00:00:00:00  00:00:00:00:00:00   \n",
       "3   1  node_network_info  00:00:00:00:00:00  00:00:00:00:00:00   \n",
       "4   1  node_network_info  00:00:00:00:00:00  00:00:00:00:00:00   \n",
       "5   1  node_network_info  00:00:00:00:00:00  00:00:00:00:00:00   \n",
       "6   1  node_network_info  0a:f2:eb:9c:58:fa  ff:ff:ff:ff:ff:ff   \n",
       "7   1  node_network_info  2e:4d:54:f6:20:3b  ff:ff:ff:ff:ff:ff   \n",
       "8   1  node_network_info  42:bc:2b:12:79:0b  ff:ff:ff:ff:ff:ff   \n",
       "9   1  node_network_info  5a:c6:b1:b9:b1:08  ff:ff:ff:ff:ff:ff   \n",
       "10  1  node_network_info  62:e9:b0:21:15:7c  ff:ff:ff:ff:ff:ff   \n",
       "11  1  node_network_info  96:35:10:66:ec:30  ff:ff:ff:ff:ff:ff   \n",
       "12  1  node_network_info  9a:92:76:d3:ba:0a  ff:ff:ff:ff:ff:ff   \n",
       "13  1  node_network_info  c2:9a:c7:c5:1e:db  ff:ff:ff:ff:ff:ff   \n",
       "14  1  node_network_info  c6:d8:7c:79:b2:01  ff:ff:ff:ff:ff:ff   \n",
       "15  1  node_network_info  de:c2:45:b0:c9:3c  ff:ff:ff:ff:ff:ff   \n",
       "16  1  node_network_info  e2:b8:48:6d:63:98  ff:ff:ff:ff:ff:ff   \n",
       "17  1  node_network_info  ee:df:0d:ca:58:f2  ff:ff:ff:ff:ff:ff   \n",
       "18  1  node_network_info  fa:16:3e:4b:0d:93  ff:ff:ff:ff:ff:ff   \n",
       "19  1  node_network_info  fa:16:3e:6f:db:d8  ff:ff:ff:ff:ff:ff   \n",
       "20  1  node_network_info  fa:16:3e:76:18:e9  ff:ff:ff:ff:ff:ff   \n",
       "21  1  node_network_info  fa:16:3e:77:95:fb  ff:ff:ff:ff:ff:ff   \n",
       "22  1  node_network_info  fa:16:3e:c1:31:3f  ff:ff:ff:ff:ff:ff   \n",
       "23  1  node_network_info  fa:16:3e:e2:fe:8b  ff:ff:ff:ff:ff:ff   \n",
       "\n",
       "          container          device endpoint  \\\n",
       "0   kube-rbac-proxy              lo    https   \n",
       "1   kube-rbac-proxy              lo    https   \n",
       "2   kube-rbac-proxy              lo    https   \n",
       "3   kube-rbac-proxy              lo    https   \n",
       "4   kube-rbac-proxy              lo    https   \n",
       "5   kube-rbac-proxy              lo    https   \n",
       "6   kube-rbac-proxy  genev_sys_6081    https   \n",
       "7   kube-rbac-proxy      ovs-system    https   \n",
       "8   kube-rbac-proxy  genev_sys_6081    https   \n",
       "9   kube-rbac-proxy      ovs-system    https   \n",
       "10  kube-rbac-proxy  genev_sys_6081    https   \n",
       "11  kube-rbac-proxy  genev_sys_6081    https   \n",
       "12  kube-rbac-proxy      ovs-system    https   \n",
       "13  kube-rbac-proxy      ovs-system    https   \n",
       "14  kube-rbac-proxy      ovs-system    https   \n",
       "15  kube-rbac-proxy  genev_sys_6081    https   \n",
       "16  kube-rbac-proxy  genev_sys_6081    https   \n",
       "17  kube-rbac-proxy      ovs-system    https   \n",
       "18  kube-rbac-proxy            ens3    https   \n",
       "19  kube-rbac-proxy            ens3    https   \n",
       "20  kube-rbac-proxy            ens3    https   \n",
       "21  kube-rbac-proxy            ens3    https   \n",
       "22  kube-rbac-proxy            ens3    https   \n",
       "23  kube-rbac-proxy            ens3    https   \n",
       "\n",
       "                                             instance            job  \\\n",
       "0   master-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "1   master-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "2   master-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "3   worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "4   worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "5   worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "6   worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "7   master-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "8   master-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "9   master-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "10  worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "11  worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "12  master-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "13  worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "14  worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "15  master-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "16  master-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "17  worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "18  master-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "19  worker-2.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "20  master-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "21  worker-1.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "22  master-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "23  worker-0.sharedocp4upi412ovn.lab.upshift.rdu2....  node-exporter   \n",
       "\n",
       "               namespace operstate                  pod        service  \\\n",
       "0   openshift-monitoring   unknown  node-exporter-pj88z  node-exporter   \n",
       "1   openshift-monitoring   unknown  node-exporter-bvfx9  node-exporter   \n",
       "2   openshift-monitoring   unknown  node-exporter-9rnhx  node-exporter   \n",
       "3   openshift-monitoring   unknown  node-exporter-nkd8c  node-exporter   \n",
       "4   openshift-monitoring   unknown  node-exporter-9gljp  node-exporter   \n",
       "5   openshift-monitoring   unknown  node-exporter-2g925  node-exporter   \n",
       "6   openshift-monitoring   unknown  node-exporter-nkd8c  node-exporter   \n",
       "7   openshift-monitoring      down  node-exporter-9rnhx  node-exporter   \n",
       "8   openshift-monitoring   unknown  node-exporter-bvfx9  node-exporter   \n",
       "9   openshift-monitoring      down  node-exporter-bvfx9  node-exporter   \n",
       "10  openshift-monitoring   unknown  node-exporter-9gljp  node-exporter   \n",
       "11  openshift-monitoring   unknown  node-exporter-2g925  node-exporter   \n",
       "12  openshift-monitoring      down  node-exporter-pj88z  node-exporter   \n",
       "13  openshift-monitoring      down  node-exporter-nkd8c  node-exporter   \n",
       "14  openshift-monitoring      down  node-exporter-2g925  node-exporter   \n",
       "15  openshift-monitoring   unknown  node-exporter-pj88z  node-exporter   \n",
       "16  openshift-monitoring   unknown  node-exporter-9rnhx  node-exporter   \n",
       "17  openshift-monitoring      down  node-exporter-9gljp  node-exporter   \n",
       "18  openshift-monitoring        up  node-exporter-9rnhx  node-exporter   \n",
       "19  openshift-monitoring        up  node-exporter-2g925  node-exporter   \n",
       "20  openshift-monitoring        up  node-exporter-bvfx9  node-exporter   \n",
       "21  openshift-monitoring        up  node-exporter-9gljp  node-exporter   \n",
       "22  openshift-monitoring        up  node-exporter-pj88z  node-exporter   \n",
       "23  openshift-monitoring        up  node-exporter-nkd8c  node-exporter   \n",
       "\n",
       "     duplex  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "5       NaN  \n",
       "6       NaN  \n",
       "7       NaN  \n",
       "8       NaN  \n",
       "9       NaN  \n",
       "10      NaN  \n",
       "11      NaN  \n",
       "12      NaN  \n",
       "13      NaN  \n",
       "14      NaN  \n",
       "15      NaN  \n",
       "16      NaN  \n",
       "17      NaN  \n",
       "18  unknown  \n",
       "19  unknown  \n",
       "20  unknown  \n",
       "21  unknown  \n",
       "22  unknown  \n",
       "23  unknown  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prometheus_json_to_df(\"node_network_info\", \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " {'status': 'success',\n",
       "  'data': {'aggregator_openapi_v2_regeneration_count': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of OpenAPI v2 spec regeneration count broken down by causing APIService name and reason.',\n",
       "     'unit': ''}],\n",
       "   'aggregator_openapi_v2_regeneration_duration': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Gauge of OpenAPI v2 spec regeneration duration in seconds.',\n",
       "     'unit': ''}],\n",
       "   'aggregator_unavailable_apiservice': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Gauge of APIServices which are marked as unavailable broken down by APIService name.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_alerts': [{'type': 'gauge',\n",
       "     'help': 'How many alerts by state.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_alerts_invalid_total': [{'type': 'counter',\n",
       "     'help': 'The total number of received alerts that were invalid.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_alerts_received_total': [{'type': 'counter',\n",
       "     'help': 'The total number of received alerts.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_build_info': [{'type': 'gauge',\n",
       "     'help': \"A metric with a constant '1' value labeled by version, revision, branch, and goversion from which alertmanager was built.\",\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_alive_messages_total': [{'type': 'counter',\n",
       "     'help': 'Total number of received alive messages.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_enabled': [{'type': 'gauge',\n",
       "     'help': 'Indicates whether the clustering is enabled or not.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_failed_peers': [{'type': 'gauge',\n",
       "     'help': 'Number indicating the current number of failed peers in the cluster.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_health_score': [{'type': 'gauge',\n",
       "     'help': \"Health score of the cluster. Lower values are better and zero means 'totally healthy'.\",\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_members': [{'type': 'gauge',\n",
       "     'help': 'Number indicating current number of members in cluster.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_messages_pruned_total': [{'type': 'counter',\n",
       "     'help': 'Total number of cluster messages pruned.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_messages_queued': [{'type': 'gauge',\n",
       "     'help': 'Number of cluster messages which are queued.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_messages_received_size_total': [{'type': 'counter',\n",
       "     'help': 'Total size of cluster messages received.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_messages_received_total': [{'type': 'counter',\n",
       "     'help': 'Total number of cluster messages received.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_messages_sent_size_total': [{'type': 'counter',\n",
       "     'help': 'Total size of cluster messages sent.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_messages_sent_total': [{'type': 'counter',\n",
       "     'help': 'Total number of cluster messages sent.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_peer_info': [{'type': 'gauge',\n",
       "     'help': \"A metric with a constant '1' value labeled by peer name.\",\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_peers_joined_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of peers that have joined.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_peers_left_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of peers that have left.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_peers_update_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of peers that have updated metadata.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_pings_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of latencies for ping messages.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_reconnections_failed_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of failed cluster peer reconnection attempts.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_reconnections_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of cluster peer reconnections.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_refresh_join_failed_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of failed cluster peer joined attempts via refresh.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_refresh_join_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of cluster peer joined via refresh.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_config_hash': [{'type': 'gauge',\n",
       "     'help': 'Hash of the currently loaded alertmanager configuration.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_config_last_reload_success_timestamp_seconds': [{'type': 'gauge',\n",
       "     'help': 'Timestamp of the last successful configuration reload.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_config_last_reload_successful': [{'type': 'gauge',\n",
       "     'help': 'Whether the last configuration reload attempt was successful.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_dispatcher_aggregation_groups': [{'type': 'gauge',\n",
       "     'help': 'Number of active aggregation groups',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_dispatcher_alert_processing_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Summary of latencies for the processing of alerts.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_http_concurrency_limit_exceeded_total': [{'type': 'counter',\n",
       "     'help': 'Total number of times an HTTP request failed because the concurrency limit was reached.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_http_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of latencies for HTTP requests.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_http_requests_in_flight': [{'type': 'gauge',\n",
       "     'help': 'Current number of HTTP requests being processed.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_http_response_size_bytes': [{'type': 'histogram',\n",
       "     'help': 'Histogram of response size for HTTP requests.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_integrations': [{'type': 'gauge',\n",
       "     'help': 'Number of configured integrations.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_gc_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Duration of the last notification log garbage collection cycle.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_gossip_messages_propagated_total': [{'type': 'counter',\n",
       "     'help': 'Number of received gossip messages that have been further gossiped.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_queries_total': [{'type': 'counter',\n",
       "     'help': 'Number of notification log queries were received.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_query_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Duration of notification log query evaluation.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_query_errors_total': [{'type': 'counter',\n",
       "     'help': 'Number notification log received queries that failed.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_snapshot_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Duration of the last notification log snapshot.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_snapshot_size_bytes': [{'type': 'gauge',\n",
       "     'help': 'Size of the last notification log snapshot in bytes.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_notification_latency_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency of notifications in seconds.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_notification_requests_failed_total': [{'type': 'counter',\n",
       "     'help': 'The total number of failed notification requests.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_notification_requests_total': [{'type': 'counter',\n",
       "     'help': 'The total number of attempted notification requests.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_notifications_failed_total': [{'type': 'counter',\n",
       "     'help': 'The total number of failed notifications.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_notifications_total': [{'type': 'counter',\n",
       "     'help': 'The total number of attempted notifications.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_oversize_gossip_message_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Duration of oversized gossip message requests.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_oversized_gossip_message_dropped_total': [{'type': 'counter',\n",
       "     'help': 'Number of oversized gossip messages that were dropped due to a full message queue.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_oversized_gossip_message_failure_total': [{'type': 'counter',\n",
       "     'help': 'Number of oversized gossip message sends that failed.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_oversized_gossip_message_sent_total': [{'type': 'counter',\n",
       "     'help': 'Number of oversized gossip message sent.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_peer_position': [{'type': 'gauge',\n",
       "     'help': \"Position the Alertmanager instance believes it's in. The position determines a peer's behavior in the cluster.\",\n",
       "     'unit': ''}],\n",
       "   'alertmanager_receivers': [{'type': 'gauge',\n",
       "     'help': 'Number of configured receivers.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences': [{'type': 'gauge',\n",
       "     'help': 'How many silences by state.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_gc_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Duration of the last silence garbage collection cycle.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_gossip_messages_propagated_total': [{'type': 'counter',\n",
       "     'help': 'Number of received gossip messages that have been further gossiped.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_queries_total': [{'type': 'counter',\n",
       "     'help': 'How many silence queries were received.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_query_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Duration of silence query evaluation.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_query_errors_total': [{'type': 'counter',\n",
       "     'help': 'How many silence received queries did not succeed.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_snapshot_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Duration of the last silence snapshot.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_snapshot_size_bytes': [{'type': 'gauge',\n",
       "     'help': 'Size of the last silence snapshot in bytes.',\n",
       "     'unit': ''}],\n",
       "   'apiextensions_openapi_v2_regeneration_count': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of OpenAPI v2 spec regeneration count broken down by causing CRD name and reason.',\n",
       "     'unit': ''}],\n",
       "   'apiextensions_openapi_v3_regeneration_count': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of OpenAPI v3 spec regeneration count broken down by group, version, causing CRD and reason.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_admission_controller_admission_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[STABLE] Admission controller latency histogram in seconds, identified by name and broken out for each operation and API resource and type (validate or admit).',\n",
       "     'unit': ''}],\n",
       "   'apiserver_admission_step_admission_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[STABLE] Admission sub-step latency histogram in seconds, broken out for each operation and API resource and step type (validate or admit).',\n",
       "     'unit': ''}],\n",
       "   'apiserver_admission_step_admission_duration_seconds_summary': [{'type': 'summary',\n",
       "     'help': '[ALPHA] Admission sub-step latency summary in seconds, broken out for each operation and API resource and step type (validate or admit).',\n",
       "     'unit': ''}],\n",
       "   'apiserver_admission_webhook_admission_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[STABLE] Admission webhook latency histogram in seconds, identified by name and broken out for each operation and API resource and type (validate or admit).',\n",
       "     'unit': ''}],\n",
       "   'apiserver_admission_webhook_request_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Admission webhook request total, identified by name and broken out for each admission type (validating or mutating) and operation. Additional labels specify whether the request was rejected or not and an HTTP status code. Codes greater than 600 are truncated to 600, to keep the metrics cardinality bounded.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_audit_event_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of audit events generated and sent to the audit backend.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_audit_level_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of policy levels for audit events (1 per request).',\n",
       "     'unit': ''}],\n",
       "   'apiserver_audit_requests_rejected_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of apiserver requests rejected due to an error in audit logging backend.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_cache_list_fetched_objects_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of objects read from watch cache in the course of serving a LIST request',\n",
       "     'unit': ''}],\n",
       "   'apiserver_cache_list_returned_objects_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of objects returned for a LIST request from watch cache',\n",
       "     'unit': ''}],\n",
       "   'apiserver_cache_list_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of LIST requests served from watch cache',\n",
       "     'unit': ''}],\n",
       "   'apiserver_cel_compilation_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] ',\n",
       "     'unit': ''}],\n",
       "   'apiserver_cel_evaluation_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] ',\n",
       "     'unit': ''}],\n",
       "   'apiserver_client_certificate_expiration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Distribution of the remaining lifetime on the certificate used to authenticate a request.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_crd_webhook_conversion_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] CRD webhook conversion duration in seconds',\n",
       "     'unit': ''}],\n",
       "   'apiserver_current_inflight_requests': [{'type': 'gauge',\n",
       "     'help': '[STABLE] Maximal number of currently used inflight request limit of this apiserver per request kind in last second.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_current_inqueue_requests': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Maximal number of queued requests in this apiserver per request kind in last second.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_delegated_authn_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Request latency in seconds. Broken down by status code.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_delegated_authn_request_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of HTTP requests partitioned by status code.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_delegated_authz_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Request latency in seconds. Broken down by status code.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_delegated_authz_request_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of HTTP requests partitioned by status code.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_envelope_encryption_dek_cache_fill_percent': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Percent of the cache slots currently occupied by cached DEKs.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_current_executing_requests': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of requests in initial (for a WATCH) or any (for a non-WATCH) execution stage in the API Priority and Fairness subsystem',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_current_inqueue_requests': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of requests currently pending in queues of the API Priority and Fairness subsystem',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_current_r': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] R(time of last change)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_dispatch_r': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] R(time of last dispatch)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_dispatched_requests_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of requests executed by API Priority and Fairness subsystem',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_latest_s': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] S(most recently dispatched request)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_next_discounted_s_bounds': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] min and max, over queues, of S(oldest waiting request in queue) - estimated work in progress',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_next_s_bounds': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] min and max, over queues, of S(oldest waiting request in queue)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_priority_level_request_utilization': [{'type': 'histogram',\n",
       "     'help': 'EXPERIMENTAL: [ALPHA] Observations, at the end of every nanosecond, of number of requests (as a fraction of the relevant limit) waiting or in any stage of execution (but only initial stage for WATCHes)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_priority_level_seat_utilization': [{'type': 'histogram',\n",
       "     'help': 'EXPERIMENTAL: [ALPHA] Observations, at the end of every nanosecond, of utilization of seats for any stage of execution (but only initial stage for WATCHes)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_read_vs_write_current_requests': [{'type': 'histogram',\n",
       "     'help': 'EXPERIMENTAL: [ALPHA] Observations, at the end of every nanosecond, of the number of requests (as a fraction of the relevant limit) waiting or in regular stage of execution',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_read_vs_write_request_count_samples': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Periodic observations of the number of requests waiting or in regular stage of execution',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_read_vs_write_request_count_watermarks': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Watermarks of the number of requests waiting or in regular stage of execution',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_request_concurrency_in_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Concurrency (number of seats) occupied by the currently executing (initial stage for a WATCH, any stage otherwise) requests in the API Priority and Fairness subsystem',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_request_concurrency_limit': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Shared concurrency limit in the API Priority and Fairness subsystem',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_request_execution_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration of initial stage (for a WATCH) or any (for a non-WATCH) stage of request execution in the API Priority and Fairness subsystem',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_request_queue_length_after_enqueue': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Length of queue in the API Priority and Fairness subsystem, as seen by each request after it is enqueued',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_request_wait_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Length of time a request spent waiting in its queue',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_seat_fair_frac': [{'type': 'gauge',\n",
       "     'help': \"[ALPHA] Fair fraction of server's concurrency to allocate to each priority level that can use it\",\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_watch_count_samples': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] count of watchers for mutating requests in API Priority and Fairness',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_work_estimated_seats': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of estimated seats (maximum of initial and final seats) associated with requests in API Priority and Fairness',\n",
       "     'unit': ''}],\n",
       "   'apiserver_init_events_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of init events processed in watch cache broken by resource type.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_kube_aggregator_x509_insecure_sha1_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counts the number of requests to servers with insecure SHA1 signatures in their serving certificate OR the number of connection failures due to the insecure SHA1 signatures (either/or, based on the runtime environment)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_kube_aggregator_x509_missing_san_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counts the number of requests to servers missing SAN extension in their serving certificate OR the number of connection failures due to the lack of x509 certificate SAN extension missing (either/or, based on the runtime environment)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_longrunning_requests': [{'type': 'gauge',\n",
       "     'help': '[STABLE] Gauge of all active long-running apiserver requests broken out by verb, group, version, resource, scope and component. Not all requests are tracked this way.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_aborts_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of requests which apiserver aborted possibly due to a timeout, for each group, version, verb, resource, subresource and scope',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[STABLE] Response latency distribution in seconds for each verb, dry run value, group, version, resource, subresource, scope and component.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_filter_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Request filter latency distribution in seconds, for each filter type',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_post_timeout_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Tracks the activity of the request handlers after the associated requests have been timed out by the apiserver',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_slo_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Response latency distribution (not counting webhook duration) in seconds for each verb, group, version, resource, subresource, scope and component.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_terminations_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of requests which apiserver terminated in self-defense.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_timestamp_comparison_time': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Time taken for comparison of old vs new objects in UPDATE or PATCH requests',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_total': [{'type': 'counter',\n",
       "     'help': '[STABLE] Counter of apiserver requests broken out for each verb, dry run value, group, version, resource, scope, component, and HTTP response code.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_requested_deprecated_apis': [{'type': 'gauge',\n",
       "     'help': '[STABLE] Gauge of deprecated APIs that have been requested, broken out by API group, version, resource, subresource, and removed_release.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_response_sizes': [{'type': 'histogram',\n",
       "     'help': '[STABLE] Response size distribution in bytes for each group, version, verb, resource, subresource, scope and component.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_selfrequest_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of apiserver self-requests broken out for each verb, API resource and subresource.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_data_key_generation_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Latencies in seconds of data encryption key(DEK) generation operations.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_data_key_generation_failures_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Total number of failed data encryption key(DEK) generation operations.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_envelope_transformation_cache_misses_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Total number of cache misses while accessing key decryption key(KEK).',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_list_evaluated_objects_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of objects tested in the course of serving a LIST request from storage',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_list_fetched_objects_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of objects read from storage in the course of serving a LIST request',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_list_returned_objects_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of objects returned for a LIST request from storage',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_list_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of LIST requests served from storage',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_objects': [{'type': 'gauge',\n",
       "     'help': '[STABLE] Number of stored objects at the time of last check split by kind.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_terminated_watchers_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of watchers closed due to unresponsiveness broken by resource type.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_tls_handshake_errors_total': [{'type': 'counter',\n",
       "     'help': \"[ALPHA] Number of requests dropped with 'TLS handshake error from' error\",\n",
       "     'unit': ''}],\n",
       "   'apiserver_watch_cache_events_dispatched_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of events dispatched in watch cache broken by resource type.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_watch_cache_initializations_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of watch cache initializations broken by resource type.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_watch_events_sizes': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Watch event size distribution in bytes',\n",
       "     'unit': ''}],\n",
       "   'apiserver_watch_events_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of events sent in watch clients',\n",
       "     'unit': ''}],\n",
       "   'apiserver_webhooks_x509_insecure_sha1_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counts the number of requests to servers with insecure SHA1 signatures in their serving certificate OR the number of connection failures due to the insecure SHA1 signatures (either/or, based on the runtime environment)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_webhooks_x509_missing_san_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counts the number of requests to servers missing SAN extension in their serving certificate OR the number of connection failures due to the lack of x509 certificate SAN extension missing (either/or, based on the runtime environment)',\n",
       "     'unit': ''}],\n",
       "   'argocd_redis_request_duration': [{'type': 'histogram',\n",
       "     'help': 'Redis requests duration.',\n",
       "     'unit': ''}],\n",
       "   'argocd_redis_request_total': [{'type': 'counter',\n",
       "     'help': 'Number of kubernetes requests executed during application reconciliation.',\n",
       "     'unit': ''}],\n",
       "   'attachdetach_controller_forced_detaches': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of times the A/D Controller performed a forced detach',\n",
       "     'unit': ''}],\n",
       "   'authenticated_user_requests': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of authenticated requests broken out by username.',\n",
       "     'unit': ''}],\n",
       "   'authentication_attempts': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of authenticated attempts.',\n",
       "     'unit': ''}],\n",
       "   'authentication_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Authentication duration in seconds broken out by result.',\n",
       "     'unit': ''}],\n",
       "   'authentication_token_cache_active_fetch_count': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] ',\n",
       "     'unit': ''}],\n",
       "   'authentication_token_cache_fetch_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] ',\n",
       "     'unit': ''}],\n",
       "   'authentication_token_cache_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] ',\n",
       "     'unit': ''}],\n",
       "   'authentication_token_cache_request_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] ',\n",
       "     'unit': ''}],\n",
       "   'cadvisor_version_info': [{'type': 'gauge',\n",
       "     'help': \"A metric with a constant '1' value labeled by kernel version, OS version, docker version, cadvisor version & cadvisor revision.\",\n",
       "     'unit': ''}],\n",
       "   'catalog_source_count': [{'type': 'gauge',\n",
       "     'help': 'Number of catalog sources',\n",
       "     'unit': ''}],\n",
       "   'catalogsource_ready': [{'type': 'gauge',\n",
       "     'help': 'State of a CatalogSource. 1 indicates that the CatalogSource is in a READY state. 0 indicates CatalogSource is in a Non READY state.',\n",
       "     'unit': ''}],\n",
       "   'cco_controller_reconcile_seconds': [{'type': 'histogram',\n",
       "     'help': 'Distribution of the length of time each controllers reconcile loop takes.',\n",
       "     'unit': ''}],\n",
       "   'cco_credentials_mode': [{'type': 'gauge',\n",
       "     'help': 'Track current mode the cloud-credentials-operator is functioning under.',\n",
       "     'unit': ''}],\n",
       "   'cco_credentials_requests': [{'type': 'gauge',\n",
       "     'help': 'Total number of credentials requests.',\n",
       "     'unit': ''}],\n",
       "   'cco_credentials_requests_conditions': [{'type': 'gauge',\n",
       "     'help': 'Credentials requests with asserted conditions.',\n",
       "     'unit': ''}],\n",
       "   'certwatcher_read_certificate_errors_total': [{'type': 'counter',\n",
       "     'help': 'Total number of certificate read errors',\n",
       "     'unit': ''}],\n",
       "   'certwatcher_read_certificate_total': [{'type': 'counter',\n",
       "     'help': 'Total number of certificate reads',\n",
       "     'unit': ''}],\n",
       "   'cluster_default_node_selector': [{'type': 'gauge',\n",
       "     'help': 'Reports whether the cluster scheduler is configured with a default node selector.',\n",
       "     'unit': ''}],\n",
       "   'cluster_feature_set': [{'type': 'gauge',\n",
       "     'help': 'Reports the feature set the cluster is configured to expose. name corresponds to the featureSet field of the cluster. The value is 1 if a cloud provider is supported.',\n",
       "     'unit': ''}],\n",
       "   'cluster_infrastructure_provider': [{'type': 'gauge',\n",
       "     'help': 'Reports whether the cluster is configured with an infrastructure provider. type is unset if no cloud provider is recognized or set to the constant used by the Infrastructure config. region is set when the cluster clearly identifies a region within the provider. The value is 1 if a cloud provider is set or 0 if it is unset.',\n",
       "     'unit': ''}],\n",
       "   'cluster_installer': [{'type': 'gauge',\n",
       "     'help': \"Reports info about the installation process and, if applicable, the install tool. The type is either 'openshift-install', indicating that openshift-install was used to install the cluster, or 'other', indicating that an unknown process installed the cluster. The invoker is 'user' by default, but it may be overridden by a consuming tool. The version reported is that of the openshift-install that was used to generate the manifests and, if applicable, provision the infrastructure.\",\n",
       "     'unit': ''}],\n",
       "   'cluster_legacy_scheduler_policy': [{'type': 'gauge',\n",
       "     'help': 'Reports whether the cluster scheduler is configured with a legacy KubeScheduler Policy.',\n",
       "     'unit': ''}],\n",
       "   'cluster_master_schedulable': [{'type': 'gauge',\n",
       "     'help': 'Reports whether the cluster master nodes are schedulable.',\n",
       "     'unit': ''}],\n",
       "   'cluster_monitoring_operator_last_reconciliation_successful': [{'type': 'gauge',\n",
       "     'help': 'Latest reconciliation state. Set to 1 if last reconciliation succeeded, else 0.',\n",
       "     'unit': ''}],\n",
       "   'cluster_monitoring_operator_reconcile_attempts_total': [{'type': 'counter',\n",
       "     'help': 'Number of attempts to reconcile the operator configuration',\n",
       "     'unit': ''}],\n",
       "   'cluster_operator_condition_transitions': [{'type': 'gauge',\n",
       "     'help': 'Reports the number of times that a condition on a cluster operator changes status',\n",
       "     'unit': ''}],\n",
       "   'cluster_operator_conditions': [{'type': 'gauge',\n",
       "     'help': 'Report the conditions for active cluster operators. 0 is False and 1 is True.',\n",
       "     'unit': ''}],\n",
       "   'cluster_operator_payload_errors': [{'type': 'gauge',\n",
       "     'help': 'Report the number of errors encountered applying the payload.',\n",
       "     'unit': ''}],\n",
       "   'cluster_operator_up': [{'type': 'gauge',\n",
       "     'help': 'Reports key highlights of the active cluster operators.',\n",
       "     'unit': ''}],\n",
       "   'cluster_proxy_enabled': [{'type': 'gauge',\n",
       "     'help': 'Reports whether the cluster has been configured to use a proxy. type is which type of proxy configuration has been set - http for an http proxy, https for an https proxy, and trusted_ca if a custom CA was specified.',\n",
       "     'unit': ''}],\n",
       "   'cluster_version': [{'type': 'gauge',\n",
       "     'help': \"Reports the version of the cluster in terms of seconds since\\nthe epoch. Type 'current' is the version being applied and\\nthe value is the creation date of the payload. The type\\n'desired' is returned if spec.desiredUpdate is set but the\\noperator has not yet updated and the value is the most \\nrecent status transition time. The type 'failure' is set \\nif an error is preventing sync or upgrade with the last \\ntransition timestamp of the condition. The type 'completed' \\nis the timestamp when the last image was successfully\\napplied. The type 'cluster' is the creation date of the\\ncluster version object and the current version. The type\\n'updating' is set when the cluster is transitioning to a\\nnew version but has not reached the completed state and\\nis the time the update was started. The type 'initial' is\\nset to the oldest entry in the history. The from_version label\\nwill be set to the last completed version, the initial\\nversion for 'cluster', or empty for 'initial'.\\n.\",\n",
       "     'unit': ''}],\n",
       "   'cluster_version_available_updates': [{'type': 'gauge',\n",
       "     'help': 'Report the count of available versions for an upstream and channel.',\n",
       "     'unit': ''}],\n",
       "   'cluster_version_capability': [{'type': 'gauge',\n",
       "     'help': 'Report currently enabled cluster capabilities.  0 is disabled, and 1 is enabled.',\n",
       "     'unit': ''}],\n",
       "   'cluster_version_operator_update_retrieval_timestamp_seconds': [{'type': 'gauge',\n",
       "     'help': 'Reports when updates were last successfully retrieved.',\n",
       "     'unit': ''}],\n",
       "   'cluster_version_payload': [{'type': 'gauge',\n",
       "     'help': 'Report the number of entries in the payload.',\n",
       "     'unit': ''}],\n",
       "   'console_url': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] URL of the console exposed on the cluster',\n",
       "     'unit': ''}],\n",
       "   'container_blkio_device_usage_total': [{'type': 'counter',\n",
       "     'help': 'Blkio Device bytes usage',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_cfs_periods_total': [{'type': 'counter',\n",
       "     'help': 'Number of elapsed enforcement period intervals.',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_cfs_throttled_periods_total': [{'type': 'counter',\n",
       "     'help': 'Number of throttled period intervals.',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_cfs_throttled_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Total time duration the container has been throttled.',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_load_average_10s': [{'type': 'gauge',\n",
       "     'help': 'Value of container cpu load average over the last 10 seconds.',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_system_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative system cpu time consumed in seconds.',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_usage_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative cpu time consumed in seconds.',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_user_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative user cpu time consumed in seconds.',\n",
       "     'unit': ''}],\n",
       "   'container_file_descriptors': [{'type': 'gauge',\n",
       "     'help': 'Number of open file descriptors for the container.',\n",
       "     'unit': ''}],\n",
       "   'container_fs_inodes_free': [{'type': 'gauge',\n",
       "     'help': 'Number of available Inodes',\n",
       "     'unit': ''}],\n",
       "   'container_fs_inodes_total': [{'type': 'gauge',\n",
       "     'help': 'Number of Inodes',\n",
       "     'unit': ''}],\n",
       "   'container_fs_io_current': [{'type': 'gauge',\n",
       "     'help': 'Number of I/Os currently in progress',\n",
       "     'unit': ''}],\n",
       "   'container_fs_io_time_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of seconds spent doing I/Os',\n",
       "     'unit': ''}],\n",
       "   'container_fs_io_time_weighted_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative weighted I/O time in seconds',\n",
       "     'unit': ''}],\n",
       "   'container_fs_limit_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes that can be consumed by the container on this filesystem.',\n",
       "     'unit': ''}],\n",
       "   'container_fs_read_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of seconds spent reading',\n",
       "     'unit': ''}],\n",
       "   'container_fs_reads_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of bytes read',\n",
       "     'unit': ''}],\n",
       "   'container_fs_reads_merged_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of reads merged',\n",
       "     'unit': ''}],\n",
       "   'container_fs_reads_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of reads completed',\n",
       "     'unit': ''}],\n",
       "   'container_fs_sector_reads_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of sector reads completed',\n",
       "     'unit': ''}],\n",
       "   'container_fs_sector_writes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of sector writes completed',\n",
       "     'unit': ''}],\n",
       "   'container_fs_usage_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes that are consumed by the container on this filesystem.',\n",
       "     'unit': ''}],\n",
       "   'container_fs_write_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of seconds spent writing',\n",
       "     'unit': ''}],\n",
       "   'container_fs_writes_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of bytes written',\n",
       "     'unit': ''}],\n",
       "   'container_fs_writes_merged_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of writes merged',\n",
       "     'unit': ''}],\n",
       "   'container_fs_writes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of writes completed',\n",
       "     'unit': ''}],\n",
       "   'container_last_seen': [{'type': 'gauge',\n",
       "     'help': 'Last time a container was seen by the exporter',\n",
       "     'unit': ''}],\n",
       "   'container_memory_cache': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes of page cache memory.',\n",
       "     'unit': ''}],\n",
       "   'container_memory_failcnt': [{'type': 'counter',\n",
       "     'help': 'Number of memory usage hits limits',\n",
       "     'unit': ''}],\n",
       "   'container_memory_failures_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of memory allocation failures.',\n",
       "     'unit': ''}],\n",
       "   'container_memory_mapped_file': [{'type': 'gauge',\n",
       "     'help': 'Size of memory mapped files in bytes.',\n",
       "     'unit': ''}],\n",
       "   'container_memory_max_usage_bytes': [{'type': 'gauge',\n",
       "     'help': 'Maximum memory usage recorded in bytes',\n",
       "     'unit': ''}],\n",
       "   'container_memory_rss': [{'type': 'gauge',\n",
       "     'help': 'Size of RSS in bytes.',\n",
       "     'unit': ''}],\n",
       "   'container_memory_swap': [{'type': 'gauge',\n",
       "     'help': 'Container swap usage in bytes.',\n",
       "     'unit': ''}],\n",
       "   'container_memory_usage_bytes': [{'type': 'gauge',\n",
       "     'help': 'Current memory usage in bytes, including all memory regardless of when it was accessed',\n",
       "     'unit': ''}],\n",
       "   'container_memory_working_set_bytes': [{'type': 'gauge',\n",
       "     'help': 'Current working set in bytes.',\n",
       "     'unit': ''}],\n",
       "   'container_network_receive_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of bytes received',\n",
       "     'unit': ''}],\n",
       "   'container_network_receive_errors_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of errors encountered while receiving',\n",
       "     'unit': ''}],\n",
       "   'container_network_receive_packets_dropped_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of packets dropped while receiving',\n",
       "     'unit': ''}],\n",
       "   'container_network_receive_packets_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of packets received',\n",
       "     'unit': ''}],\n",
       "   'container_network_transmit_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of bytes transmitted',\n",
       "     'unit': ''}],\n",
       "   'container_network_transmit_errors_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of errors encountered while transmitting',\n",
       "     'unit': ''}],\n",
       "   'container_network_transmit_packets_dropped_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of packets dropped while transmitting',\n",
       "     'unit': ''}],\n",
       "   'container_network_transmit_packets_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of packets transmitted',\n",
       "     'unit': ''}],\n",
       "   'container_oom_events_total': [{'type': 'counter',\n",
       "     'help': 'Count of out of memory events observed for the container',\n",
       "     'unit': ''}],\n",
       "   'container_processes': [{'type': 'gauge',\n",
       "     'help': 'Number of processes running inside the container.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_containers_oom_total': [{'type': 'counter',\n",
       "     'help': 'Amount of containers killed because they ran out of memory (OOM)',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_image_layer_reuse_total': [{'type': 'counter',\n",
       "     'help': 'Reused (not pulled) local image layer count by name',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_image_pulls_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Bytes transferred by CRI-O image pulls',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_image_pulls_failure_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative number of CRI-O image pull failures by error.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_image_pulls_layer_size': [{'type': 'histogram',\n",
       "     'help': 'Bytes transferred by CRI-O image pulls per layer',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_image_pulls_skipped_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Bytes skipped by CRI-O image pulls',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_image_pulls_success_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative number of CRI-O image pull successes.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations': [{'type': 'counter',\n",
       "     'help': '[DEPRECATED: in favour of `operations_total`] Cumulative number of CRI-O operations by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_errors': [{'type': 'counter',\n",
       "     'help': '[DEPRECATED: in favour of `operations_errors_total`] Cumulative number of CRI-O operation errors by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_errors_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative number of CRI-O operation errors by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_latency_microseconds': [{'type': 'gauge',\n",
       "     'help': '[DEPRECATED: in favour of `operations_latency_seconds`] Latency in microseconds of individual CRI calls for CRI-O operations. Broken down by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_latency_microseconds_total': [{'type': 'summary',\n",
       "     'help': '[DEPRECATED:  in favour of `operations_latency_seconds_total`] Latency in microseconds of CRI-O operations. Broken down by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_latency_seconds': [{'type': 'gauge',\n",
       "     'help': 'Latency in seconds of individual CRI calls for CRI-O operations. Broken down by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_latency_seconds_total': [{'type': 'summary',\n",
       "     'help': 'Latency in seconds of CRI-O operations. Broken down by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative number of CRI-O operations by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_processes_defunct': [{'type': 'gauge',\n",
       "     'help': 'Total number of defunct processes in the node',\n",
       "     'unit': ''}],\n",
       "   'container_scrape_error': [{'type': 'gauge',\n",
       "     'help': '1 if there was an error while getting container metrics, 0 otherwise',\n",
       "     'unit': ''}],\n",
       "   'container_sockets': [{'type': 'gauge',\n",
       "     'help': 'Number of open sockets for the container.',\n",
       "     'unit': ''}],\n",
       "   'container_spec_cpu_period': [{'type': 'gauge',\n",
       "     'help': 'CPU period of the container.',\n",
       "     'unit': ''}],\n",
       "   'container_spec_cpu_quota': [{'type': 'gauge',\n",
       "     'help': 'CPU quota of the container.',\n",
       "     'unit': ''}],\n",
       "   'container_spec_cpu_shares': [{'type': 'gauge',\n",
       "     'help': 'CPU share of the container.',\n",
       "     'unit': ''}],\n",
       "   'container_spec_memory_limit_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory limit for the container.',\n",
       "     'unit': ''}],\n",
       "   'container_spec_memory_reservation_limit_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory reservation limit for the container.',\n",
       "     'unit': ''}],\n",
       "   'container_spec_memory_swap_limit_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory swap limit for the container.',\n",
       "     'unit': ''}],\n",
       "   'container_start_time_seconds': [{'type': 'gauge',\n",
       "     'help': 'Start time of the container since unix epoch in seconds.',\n",
       "     'unit': ''}],\n",
       "   'container_tasks_state': [{'type': 'gauge',\n",
       "     'help': 'Number of tasks in given state',\n",
       "     'unit': ''}],\n",
       "   'container_threads': [{'type': 'gauge',\n",
       "     'help': 'Number of threads running inside the container',\n",
       "     'unit': ''}],\n",
       "   'container_threads_max': [{'type': 'gauge',\n",
       "     'help': 'Maximum number of threads allowed inside the container, infinity if value is zero',\n",
       "     'unit': ''}],\n",
       "   'container_ulimits_soft': [{'type': 'gauge',\n",
       "     'help': 'Soft ulimit values for the container root process. Unlimited if -1, except priority and nice',\n",
       "     'unit': ''}],\n",
       "   'containerd_cri_input_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Size of logs received',\n",
       "     'unit': ''}],\n",
       "   'containerd_cri_input_entries_total': [{'type': 'counter',\n",
       "     'help': 'Number of log entries received',\n",
       "     'unit': ''}],\n",
       "   'containerd_cri_output_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Size of logs successfully written to disk',\n",
       "     'unit': ''}],\n",
       "   'containerd_cri_output_entries_total': [{'type': 'counter',\n",
       "     'help': 'Number of log entries successfully written to disk',\n",
       "     'unit': ''}],\n",
       "   'containerd_cri_split_entries_total': [{'type': 'counter',\n",
       "     'help': 'Number of extra log entries created by splitting the original log entry. This happens when the original log entry exceeds length limit. This metric does not count the original log entry.',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_active_workers': [{'type': 'gauge',\n",
       "     'help': 'Number of currently used workers per controller',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_max_concurrent_reconciles': [{'type': 'gauge',\n",
       "     'help': 'Maximum number of concurrent reconciles per controller',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_reconcile_errors_total': [{'type': 'counter',\n",
       "     'help': 'Total number of reconciliation errors per controller',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_reconcile_time_seconds': [{'type': 'histogram',\n",
       "     'help': 'Length of time per reconciliation per controller',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_reconcile_total': [{'type': 'counter',\n",
       "     'help': 'Total number of reconciliations per controller',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_webhook_requests_in_flight': [{'type': 'gauge',\n",
       "     'help': 'Current number of admission requests being served.',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_webhook_requests_total': [{'type': 'counter',\n",
       "     'help': 'Total number of admission requests by HTTP status code.',\n",
       "     'unit': ''}],\n",
       "   'coredns_build_info': [{'type': 'gauge',\n",
       "     'help': \"A metric with a constant '1' value labeled by version, revision, and goversion from which CoreDNS was built.\",\n",
       "     'unit': ''}],\n",
       "   'coredns_cache_entries': [{'type': 'gauge',\n",
       "     'help': 'The number of elements in the cache.',\n",
       "     'unit': ''}],\n",
       "   'coredns_cache_hits_total': [{'type': 'counter',\n",
       "     'help': 'The count of cache hits.',\n",
       "     'unit': ''}],\n",
       "   'coredns_cache_misses_total': [{'type': 'counter',\n",
       "     'help': 'The count of cache misses. Deprecated, derive misses from cache hits/requests counters.',\n",
       "     'unit': ''}],\n",
       "   'coredns_cache_requests_total': [{'type': 'counter',\n",
       "     'help': 'The count of cache requests.',\n",
       "     'unit': ''}],\n",
       "   'coredns_dns_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of the time (in seconds) each request took per zone.',\n",
       "     'unit': ''}],\n",
       "   'coredns_dns_request_size_bytes': [{'type': 'histogram',\n",
       "     'help': 'Size of the EDNS0 UDP buffer in bytes (64K for TCP) per zone and protocol.',\n",
       "     'unit': ''}],\n",
       "   'coredns_dns_requests_total': [{'type': 'counter',\n",
       "     'help': 'Counter of DNS requests made per zone, protocol and family.',\n",
       "     'unit': ''}],\n",
       "   'coredns_dns_response_size_bytes': [{'type': 'histogram',\n",
       "     'help': 'Size of the returned response in bytes.',\n",
       "     'unit': ''}],\n",
       "   'coredns_dns_responses_total': [{'type': 'counter',\n",
       "     'help': 'Counter of response status codes.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_conn_cache_hits_total': [{'type': 'counter',\n",
       "     'help': 'Counter of connection cache hits per upstream and protocol.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_conn_cache_misses_total': [{'type': 'counter',\n",
       "     'help': 'Counter of connection cache misses per upstream and protocol.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_healthcheck_broken_total': [{'type': 'counter',\n",
       "     'help': 'Counter of the number of complete failures of the healthchecks.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_max_concurrent_rejects_total': [{'type': 'counter',\n",
       "     'help': 'Counter of the number of queries rejected because the concurrent queries were at maximum.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of the time each request took.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_requests_total': [{'type': 'counter',\n",
       "     'help': 'Counter of requests made per upstream.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_responses_total': [{'type': 'counter',\n",
       "     'help': 'Counter of responses received per upstream.',\n",
       "     'unit': ''}],\n",
       "   'coredns_health_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of the time (in seconds) each request took.',\n",
       "     'unit': ''}],\n",
       "   'coredns_health_request_failures_total': [{'type': 'counter',\n",
       "     'help': 'The number of times the health check failed.',\n",
       "     'unit': ''}],\n",
       "   'coredns_hosts_reload_timestamp_seconds': [{'type': 'gauge',\n",
       "     'help': 'The timestamp of the last reload of hosts file.',\n",
       "     'unit': ''}],\n",
       "   'coredns_kubernetes_dns_programming_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of the time (in seconds) it took to program a dns instance.',\n",
       "     'unit': ''}],\n",
       "   'coredns_local_localhost_requests_total': [{'type': 'counter',\n",
       "     'help': 'Counter of localhost.<domain> requests.',\n",
       "     'unit': ''}],\n",
       "   'coredns_panics_total': [{'type': 'counter',\n",
       "     'help': 'A metrics that counts the number of panics.',\n",
       "     'unit': ''}],\n",
       "   'coredns_plugin_enabled': [{'type': 'gauge',\n",
       "     'help': 'A metric that indicates whether a plugin is enabled on per server and zone basis.',\n",
       "     'unit': ''}],\n",
       "   'coredns_reload_failed_total': [{'type': 'counter',\n",
       "     'help': 'Counter of the number of failed reload attempts.',\n",
       "     'unit': ''}],\n",
       "   'cronjob_controller_cronjob_job_creation_skew_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Time between when a cronjob is scheduled to be run, and when the corresponding job is created',\n",
       "     'unit': ''}],\n",
       "   'cronjob_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for cronjob_controller',\n",
       "     'unit': ''}],\n",
       "   'csv_abnormal': [{'type': 'gauge',\n",
       "     'help': 'CSV is not installed',\n",
       "     'unit': ''}],\n",
       "   'csv_count': [{'type': 'gauge',\n",
       "     'help': 'Number of CSVs successfully registered',\n",
       "     'unit': ''}],\n",
       "   'csv_succeeded': [{'type': 'gauge',\n",
       "     'help': 'Successful CSV install',\n",
       "     'unit': ''}],\n",
       "   'csv_upgrade_count': [{'type': 'counter',\n",
       "     'help': 'Monotonic count of CSV upgrades',\n",
       "     'unit': ''}],\n",
       "   'daemon_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for daemon_controller',\n",
       "     'unit': ''}],\n",
       "   'default_storage_class_count': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of default storage classes currently configured.',\n",
       "     'unit': ''}],\n",
       "   'deployment_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for deployment_controller',\n",
       "     'unit': ''}],\n",
       "   'disabled_metric_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] The count of disabled metrics.',\n",
       "     'unit': ''}],\n",
       "   'endpoint_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for endpoint_controller',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_changes': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of EndpointSlice changes',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_desired_endpoint_slices': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of EndpointSlices that would exist with perfect endpoint allocation',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_endpoints_added_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of endpoints added on each Service sync',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_endpoints_desired': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of endpoints desired',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_endpoints_removed_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of endpoints removed on each Service sync',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_endpointslices_changed_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of EndpointSlices changed on each Service sync',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_num_endpoint_slices': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of EndpointSlices',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for endpoint_slice_controller',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_syncs': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of EndpointSlice syncs',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_addresses_skipped_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of addresses skipped on each Endpoints sync due to being invalid or exceeding MaxEndpointsPerSubset',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_desired_endpoint_slices': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of EndpointSlices that would exist with perfect endpoint allocation',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_endpoints_added_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of endpoints added on each Endpoints sync',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_endpoints_desired': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of endpoints desired',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_endpoints_removed_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of endpoints removed on each Endpoints sync',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_endpoints_sync_duration': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration of syncEndpoints() in seconds',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_endpoints_updated_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of endpoints updated on each Endpoints sync',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_num_endpoint_slices': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of EndpointSlices',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for endpoint_slice_mirroring_controller',\n",
       "     'unit': ''}],\n",
       "   'ephemeral_volume_controller_create_failures_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of PersistenVolumeClaims creation requests',\n",
       "     'unit': ''}],\n",
       "   'ephemeral_volume_controller_create_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of PersistenVolumeClaims creation requests',\n",
       "     'unit': ''}],\n",
       "   'etcd_bookmark_counts': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of etcd bookmarks (progress notify events) split by kind.',\n",
       "     'unit': ''}],\n",
       "   'etcd_cluster_version': [{'type': 'gauge',\n",
       "     'help': \"Which version is running. 1 for 'cluster_version' label with current cluster version\",\n",
       "     'unit': ''}],\n",
       "   'etcd_db_total_size_in_bytes': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Total size of the etcd database file physically allocated in bytes.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_auth_revision': [{'type': 'gauge',\n",
       "     'help': 'The current revision of auth store.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_disk_backend_commit_rebalance_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of commit.rebalance called by bboltdb backend.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_disk_backend_commit_spill_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of commit.spill called by bboltdb backend.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_disk_backend_commit_write_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of commit.write called by bboltdb backend.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_lease_granted_total': [{'type': 'counter',\n",
       "     'help': 'The total number of granted leases.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_lease_renewed_total': [{'type': 'counter',\n",
       "     'help': 'The number of renewed leases seen by the leader.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_lease_revoked_total': [{'type': 'counter',\n",
       "     'help': 'The total number of revoked leases.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_lease_ttl_total': [{'type': 'histogram',\n",
       "     'help': 'Bucketed histogram of lease TTLs.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_compact_revision': [{'type': 'gauge',\n",
       "     'help': 'The revision of the last compaction in store.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_current_revision': [{'type': 'gauge',\n",
       "     'help': 'The current revision of store.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_db_compaction_keys_total': [{'type': 'counter',\n",
       "     'help': 'Total number of db keys compacted.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_db_compaction_last': [{'type': 'gauge',\n",
       "     'help': 'The unix time of the last db compaction. Resets to 0 on start.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_db_compaction_pause_duration_milliseconds': [{'type': 'histogram',\n",
       "     'help': 'Bucketed histogram of db compaction pause duration.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_db_compaction_total_duration_milliseconds': [{'type': 'histogram',\n",
       "     'help': 'Bucketed histogram of db compaction total duration.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_events_total': [{'type': 'counter',\n",
       "     'help': 'Total number of events sent by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_index_compaction_pause_duration_milliseconds': [{'type': 'histogram',\n",
       "     'help': 'Bucketed histogram of index compaction pause duration.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_keys_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of keys.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_pending_events_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of pending events to be sent.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_range_total': [{'type': 'counter',\n",
       "     'help': 'Total number of ranges seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_slow_watcher_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of unsynced slow watchers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_total_put_size_in_bytes': [{'type': 'gauge',\n",
       "     'help': 'The total size of put kv pairs seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_watch_stream_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of watch streams.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_watcher_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of watchers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_raft_terms_total': [{'type': 'counter',\n",
       "     'help': 'Number of etcd raft terms as observed by each member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_server_lease_expired_total': [{'type': 'counter',\n",
       "     'help': 'The total number of expired leases.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_snap_save_marshalling_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The marshalling cost distributions of save called by snapshot.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_snap_save_total_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The total latency distributions of save called by snapshot.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_store_expires_total': [{'type': 'counter',\n",
       "     'help': 'Total number of expired keys.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_store_reads_total': [{'type': 'counter',\n",
       "     'help': 'Total number of reads action by (get/getRecursive), local to this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_store_watch_requests_total': [{'type': 'counter',\n",
       "     'help': 'Total number of incoming watch requests (new or reestablished).',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_store_watchers': [{'type': 'gauge',\n",
       "     'help': 'Count of currently active watchers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_store_writes_total': [{'type': 'counter',\n",
       "     'help': 'Total number of writes (e.g. set/compareAndDelete) seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_disk_backend_commit_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of commit called by backend.',\n",
       "     'unit': ''}],\n",
       "   'etcd_disk_backend_defrag_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distribution of backend defragmentation.',\n",
       "     'unit': ''}],\n",
       "   'etcd_disk_backend_snapshot_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distribution of backend snapshots.',\n",
       "     'unit': ''}],\n",
       "   'etcd_disk_defrag_inflight': [{'type': 'gauge',\n",
       "     'help': 'Whether or not defrag is active on the member. 1 means active, 0 means not.',\n",
       "     'unit': ''}],\n",
       "   'etcd_disk_wal_fsync_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of fsync called by WAL.',\n",
       "     'unit': ''}],\n",
       "   'etcd_disk_wal_write_bytes_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of bytes written in WAL.',\n",
       "     'unit': ''}],\n",
       "   'etcd_grpc_proxy_cache_hits_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of cache hits',\n",
       "     'unit': ''}],\n",
       "   'etcd_grpc_proxy_cache_keys_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of keys/ranges cached',\n",
       "     'unit': ''}],\n",
       "   'etcd_grpc_proxy_cache_misses_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of cache misses',\n",
       "     'unit': ''}],\n",
       "   'etcd_grpc_proxy_events_coalescing_total': [{'type': 'counter',\n",
       "     'help': 'Total number of events coalescing',\n",
       "     'unit': ''}],\n",
       "   'etcd_grpc_proxy_watchers_coalescing_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of current watchers coalescing',\n",
       "     'unit': ''}],\n",
       "   'etcd_lease_object_counts': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of objects attached to a single etcd lease.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_db_open_read_transactions': [{'type': 'gauge',\n",
       "     'help': 'The number of currently open read transactions',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_db_total_size_in_bytes': [{'type': 'gauge',\n",
       "     'help': 'Total size of the underlying database physically allocated in bytes.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_db_total_size_in_use_in_bytes': [{'type': 'gauge',\n",
       "     'help': 'Total size of the underlying database logically in use in bytes.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_delete_total': [{'type': 'counter',\n",
       "     'help': 'Total number of deletes seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_hash_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distribution of storage hash operation.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_hash_rev_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distribution of storage hash by revision operation.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_put_total': [{'type': 'counter',\n",
       "     'help': 'Total number of puts seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_range_total': [{'type': 'counter',\n",
       "     'help': 'Total number of ranges seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_txn_total': [{'type': 'counter',\n",
       "     'help': 'Total number of txns seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_active_peers': [{'type': 'gauge',\n",
       "     'help': 'The current number of active peer connections.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_client_grpc_received_bytes_total': [{'type': 'counter',\n",
       "     'help': 'The total number of bytes received from grpc clients.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_client_grpc_sent_bytes_total': [{'type': 'counter',\n",
       "     'help': 'The total number of bytes sent to grpc clients.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_disconnected_peers_total': [{'type': 'counter',\n",
       "     'help': 'The total number of disconnected peers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_peer_received_bytes_total': [{'type': 'counter',\n",
       "     'help': 'The total number of bytes received from peers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_peer_round_trip_time_seconds': [{'type': 'histogram',\n",
       "     'help': 'Round-Trip-Time histogram between peers',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_peer_sent_bytes_total': [{'type': 'counter',\n",
       "     'help': 'The total number of bytes sent to peers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_peer_sent_failures_total': [{'type': 'counter',\n",
       "     'help': 'The total number of send failures from peers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Etcd request latency in seconds for each operation and object type.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_apply_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of v2 apply called by backend.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_client_requests_total': [{'type': 'counter',\n",
       "     'help': 'The total number of client requests per client version.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_go_version': [{'type': 'gauge',\n",
       "     'help': \"Which Go version server is running with. 1 for 'server_go_version' label with current version.\",\n",
       "     'unit': ''}],\n",
       "   'etcd_server_has_leader': [{'type': 'gauge',\n",
       "     'help': 'Whether or not a leader exists. 1 is existence, 0 is not.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_health_failures': [{'type': 'counter',\n",
       "     'help': 'The total number of failed health checks',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_health_success': [{'type': 'counter',\n",
       "     'help': 'The total number of successful health checks',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_heartbeat_send_failures_total': [{'type': 'counter',\n",
       "     'help': 'The total number of leader heartbeat send failures (likely overloaded from slow disk).',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_id': [{'type': 'gauge',\n",
       "     'help': \"Server or member ID in hexadecimal format. 1 for 'server_id' label with current ID.\",\n",
       "     'unit': ''}],\n",
       "   'etcd_server_is_leader': [{'type': 'gauge',\n",
       "     'help': 'Whether or not this member is a leader. 1 if is, 0 otherwise.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_is_learner': [{'type': 'gauge',\n",
       "     'help': 'Whether or not this member is a learner. 1 if is, 0 otherwise.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_leader_changes_seen_total': [{'type': 'counter',\n",
       "     'help': 'The number of leader changes seen.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_learner_promote_successes': [{'type': 'counter',\n",
       "     'help': 'The total number of successful learner promotions while this member is leader.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_proposals_applied_total': [{'type': 'gauge',\n",
       "     'help': 'The total number of consensus proposals applied.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_proposals_committed_total': [{'type': 'gauge',\n",
       "     'help': 'The total number of consensus proposals committed.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_proposals_failed_total': [{'type': 'counter',\n",
       "     'help': 'The total number of failed proposals seen.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_proposals_pending': [{'type': 'gauge',\n",
       "     'help': 'The current number of pending proposals to commit.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_quota_backend_bytes': [{'type': 'gauge',\n",
       "     'help': 'Current backend storage quota size in bytes.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_read_indexes_failed_total': [{'type': 'counter',\n",
       "     'help': 'The total number of failed read indexes seen.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_slow_apply_total': [{'type': 'counter',\n",
       "     'help': 'The total number of slow apply requests (likely overloaded from slow disk).',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_slow_read_indexes_total': [{'type': 'counter',\n",
       "     'help': \"The total number of pending read indexes not in sync with leader's or timed out read index requests.\",\n",
       "     'unit': ''}],\n",
       "   'etcd_server_snapshot_apply_in_progress_total': [{'type': 'gauge',\n",
       "     'help': '1 if the server is applying the incoming snapshot. 0 if none.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_version': [{'type': 'gauge',\n",
       "     'help': \"Which version is running. 1 for 'server_version' label with current version.\",\n",
       "     'unit': ''}],\n",
       "   'etcd_snap_db_fsync_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of fsyncing .snap.db file',\n",
       "     'unit': ''}],\n",
       "   'etcd_snap_db_save_total_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The total latency distributions of v3 snapshot save',\n",
       "     'unit': ''}],\n",
       "   'etcd_snap_fsync_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of fsync called by snap.',\n",
       "     'unit': ''}],\n",
       "   'event_recorder_total_events_count': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Total count of events processed by this event recorder per involved object',\n",
       "     'unit': ''}],\n",
       "   'field_validation_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Response latency distribution in seconds for each field validation value and whether field validation is enabled or not',\n",
       "     'unit': ''}],\n",
       "   'garbagecollector_controller_resources_sync_error_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of garbage collector resources sync errors',\n",
       "     'unit': ''}],\n",
       "   'gc_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for gc_controller',\n",
       "     'unit': ''}],\n",
       "   'get_token_count': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of total Token() requests to the alternate token source',\n",
       "     'unit': ''}],\n",
       "   'get_token_fail_count': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of failed Token() requests to the alternate token source',\n",
       "     'unit': ''}],\n",
       "   'go_cgo_go_to_c_calls_calls_total': [{'type': 'counter',\n",
       "     'help': 'Count of calls made from Go to C by the current process.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_cycles_automatic_gc_cycles_total': [{'type': 'counter',\n",
       "     'help': 'Count of completed GC cycles generated by the Go runtime.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_cycles_forced_gc_cycles_total': [{'type': 'counter',\n",
       "     'help': 'Count of completed GC cycles forced by the application.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_cycles_total_gc_cycles_total': [{'type': 'counter',\n",
       "     'help': 'Count of all completed GC cycles.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'A summary of the pause duration of garbage collection cycles.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_allocs_by_size_bytes': [{'type': 'histogram',\n",
       "     'help': 'Distribution of heap allocations by approximate size. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_allocs_by_size_bytes_total': [{'type': 'histogram',\n",
       "     'help': 'Distribution of heap allocations by approximate size. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_allocs_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative sum of memory allocated to the heap by the application.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_allocs_objects_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of heap allocations triggered by the application. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_frees_by_size_bytes': [{'type': 'histogram',\n",
       "     'help': 'Distribution of freed heap allocations by approximate size. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_frees_by_size_bytes_total': [{'type': 'histogram',\n",
       "     'help': 'Distribution of freed heap allocations by approximate size. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_frees_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative sum of heap memory freed by the garbage collector.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_frees_objects_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of heap allocations whose storage was freed by the garbage collector. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_goal_bytes': [{'type': 'gauge',\n",
       "     'help': 'Heap size target for the end of the GC cycle.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_objects_objects': [{'type': 'gauge',\n",
       "     'help': 'Number of objects, live or unswept, occupying heap memory.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_tiny_allocs_objects_total': [{'type': 'counter',\n",
       "     'help': 'Count of small allocations that are packed together into blocks. These allocations are counted separately from other allocations because each individual allocation is not tracked by the runtime, only their block. Each block is already accounted for in allocs-by-size and frees-by-size.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_limiter_last_enabled_gc_cycle': [{'type': 'gauge',\n",
       "     'help': \"GC cycle the last time the GC CPU limiter was enabled. This metric is useful for diagnosing the root cause of an out-of-memory error, because the limiter trades memory for CPU time when the GC's CPU time gets too high. This is most likely to occur with use of SetMemoryLimit. The first GC cycle is cycle 1, so a value of 0 indicates that it was never enabled.\",\n",
       "     'unit': ''}],\n",
       "   'go_gc_pauses_seconds': [{'type': 'histogram',\n",
       "     'help': 'Distribution individual GC-related stop-the-world pause latencies.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_pauses_seconds_total': [{'type': 'histogram',\n",
       "     'help': 'Distribution individual GC-related stop-the-world pause latencies.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_stack_starting_size_bytes': [{'type': 'gauge',\n",
       "     'help': 'The stack size of new goroutines.',\n",
       "     'unit': ''}],\n",
       "   'go_goroutines': [{'type': 'gauge',\n",
       "     'help': 'Number of goroutines that currently exist.',\n",
       "     'unit': ''}],\n",
       "   'go_info': [{'type': 'gauge',\n",
       "     'help': 'Information about the Go environment.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_heap_free_bytes': [{'type': 'gauge',\n",
       "     'help': \"Memory that is completely free and eligible to be returned to the underlying system, but has not been. This metric is the runtime's estimate of free address space that is backed by physical memory.\",\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_heap_objects_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory occupied by live objects and dead objects that have not yet been marked free by the garbage collector.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_heap_released_bytes': [{'type': 'gauge',\n",
       "     'help': \"Memory that is completely free and has been returned to the underlying system. This metric is the runtime's estimate of free address space that is still mapped into the process, but is not backed by physical memory.\",\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_heap_stacks_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory allocated from the heap that is reserved for stack space, whether or not it is currently in-use.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_heap_unused_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory that is reserved for heap objects but is not currently used to hold heap objects.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_metadata_mcache_free_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory that is reserved for runtime mcache structures, but not in-use.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_metadata_mcache_inuse_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory that is occupied by runtime mcache structures that are currently being used.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_metadata_mspan_free_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory that is reserved for runtime mspan structures, but not in-use.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_metadata_mspan_inuse_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory that is occupied by runtime mspan structures that are currently being used.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_metadata_other_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory that is reserved for or used to hold runtime metadata.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_os_stacks_bytes': [{'type': 'gauge',\n",
       "     'help': 'Stack memory allocated by the underlying operating system.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_other_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory used by execution trace buffers, structures for debugging the runtime, finalizer and profiler specials, and more.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_profiling_buckets_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory that is used by the stack trace hash map used for profiling.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_total_bytes': [{'type': 'gauge',\n",
       "     'help': 'All memory mapped by the Go runtime into the current process as read-write. Note that this does not include memory mapped by code called via cgo or via the syscall package. Sum of all metrics in /memory/classes.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_alloc_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes allocated and still in use.',\n",
       "     'unit': ''},\n",
       "    {'type': 'counter',\n",
       "     'help': 'Total number of bytes allocated, even if freed.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_alloc_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Total number of bytes allocated, even if freed.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_buck_hash_sys_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes used by the profiling bucket hash table.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_frees': [{'type': 'counter',\n",
       "     'help': 'Total number of frees.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_frees_total': [{'type': 'counter',\n",
       "     'help': 'Total number of frees.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_gc_cpu_fraction': [{'type': 'gauge',\n",
       "     'help': \"The fraction of this program's available CPU time used by the GC since the program started.\",\n",
       "     'unit': ''}],\n",
       "   'go_memstats_gc_sys_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes used for garbage collection system metadata.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_heap_alloc_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of heap bytes allocated and still in use.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_heap_idle_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of heap bytes waiting to be used.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_heap_inuse_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of heap bytes that are in use.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_heap_objects': [{'type': 'gauge',\n",
       "     'help': 'Number of allocated objects.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_heap_released_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of heap bytes released to OS.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_heap_sys_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of heap bytes obtained from system.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_last_gc_time_seconds': [{'type': 'gauge',\n",
       "     'help': 'Number of seconds since 1970 of last garbage collection.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_lookups': [{'type': 'counter',\n",
       "     'help': 'Total number of pointer lookups.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_lookups_total': [{'type': 'counter',\n",
       "     'help': 'Total number of pointer lookups.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_mallocs': [{'type': 'counter',\n",
       "     'help': 'Total number of mallocs.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_mallocs_total': [{'type': 'counter',\n",
       "     'help': 'Total number of mallocs.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_mcache_inuse_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes in use by mcache structures.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_mcache_sys_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes used for mcache structures obtained from system.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_mspan_inuse_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes in use by mspan structures.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_mspan_sys_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes used for mspan structures obtained from system.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_next_gc_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of heap bytes when next garbage collection will take place.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_other_sys_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes used for other system allocations.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_stack_inuse_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes in use by the stack allocator.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_stack_sys_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes obtained from system for stack allocator.',\n",
       "     'unit': ''}],\n",
       "   'go_memstats_sys_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes obtained from system.',\n",
       "     'unit': ''}],\n",
       "   'go_sched_gomaxprocs_threads': [{'type': 'gauge',\n",
       "     'help': 'The current runtime.GOMAXPROCS setting, or the number of operating system threads that can execute user-level Go code simultaneously.',\n",
       "     'unit': ''}],\n",
       "   'go_sched_goroutines_goroutines': [{'type': 'gauge',\n",
       "     'help': 'Count of live goroutines.',\n",
       "     'unit': ''}],\n",
       "   'go_sched_latencies_seconds': [{'type': 'histogram',\n",
       "     'help': 'Distribution of the time goroutines have spent in the scheduler in a runnable state before actually running.',\n",
       "     'unit': ''}],\n",
       "   'go_threads': [{'type': 'gauge',\n",
       "     'help': 'Number of OS threads created.',\n",
       "     'unit': ''}],\n",
       "   'grpc_client_handled': [{'type': 'counter',\n",
       "     'help': 'Total number of RPCs completed by the client, regardless of success or failure.',\n",
       "     'unit': ''}],\n",
       "   'grpc_client_handled_total': [{'type': 'counter',\n",
       "     'help': 'Total number of RPCs completed by the client, regardless of success or failure.',\n",
       "     'unit': ''}],\n",
       "   'grpc_client_handling_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of response latency (seconds) of the gRPC until it is finished by the application.',\n",
       "     'unit': ''}],\n",
       "   'grpc_client_msg_received': [{'type': 'counter',\n",
       "     'help': 'Total number of RPC stream messages received by the client.',\n",
       "     'unit': ''}],\n",
       "   'grpc_client_msg_received_total': [{'type': 'counter',\n",
       "     'help': 'Total number of RPC stream messages received by the client.',\n",
       "     'unit': ''}],\n",
       "   'grpc_client_msg_sent': [{'type': 'counter',\n",
       "     'help': 'Total number of gRPC stream messages sent by the client.',\n",
       "     'unit': ''}],\n",
       "   'grpc_client_msg_sent_total': [{'type': 'counter',\n",
       "     'help': 'Total number of gRPC stream messages sent by the client.',\n",
       "     'unit': ''}],\n",
       "   'grpc_client_started': [{'type': 'counter',\n",
       "     'help': 'Total number of RPCs started on the client.',\n",
       "     'unit': ''}],\n",
       "   'grpc_client_started_total': [{'type': 'counter',\n",
       "     'help': 'Total number of RPCs started on the client.',\n",
       "     'unit': ''}],\n",
       "   'grpc_req_panics_recovered': [{'type': 'counter',\n",
       "     'help': 'Total number of gRPC requests recovered from internal panic.',\n",
       "     'unit': ''}],\n",
       "   'grpc_server_handled': [{'type': 'counter',\n",
       "     'help': 'Total number of RPCs completed on the server, regardless of success or failure.',\n",
       "     'unit': ''}],\n",
       "   'grpc_server_handled_total': [{'type': 'counter',\n",
       "     'help': 'Total number of RPCs completed on the server, regardless of success or failure.',\n",
       "     'unit': ''}],\n",
       "   'grpc_server_handling_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of response latency (seconds) of gRPC that had been application-level handled by the server.',\n",
       "     'unit': ''}],\n",
       "   'grpc_server_msg_received': [{'type': 'counter',\n",
       "     'help': 'Total number of RPC stream messages received on the server.',\n",
       "     'unit': ''}],\n",
       "   'grpc_server_msg_received_total': [{'type': 'counter',\n",
       "     'help': 'Total number of RPC stream messages received on the server.',\n",
       "     'unit': ''}],\n",
       "   'grpc_server_msg_sent': [{'type': 'counter',\n",
       "     'help': 'Total number of gRPC stream messages sent by the server.',\n",
       "     'unit': ''}],\n",
       "   'grpc_server_msg_sent_total': [{'type': 'counter',\n",
       "     'help': 'Total number of gRPC stream messages sent by the server.',\n",
       "     'unit': ''}],\n",
       "   'grpc_server_started': [{'type': 'counter',\n",
       "     'help': 'Total number of RPCs started on the server.',\n",
       "     'unit': ''}],\n",
       "   'grpc_server_started_total': [{'type': 'counter',\n",
       "     'help': 'Total number of RPCs started on the server.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_bytes_in_total': [{'type': 'gauge',\n",
       "     'help': 'Current total of incoming bytes.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_bytes_out_total': [{'type': 'gauge',\n",
       "     'help': 'Current total of outgoing bytes.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_connection_errors_total': [{'type': 'gauge',\n",
       "     'help': 'Total of connection errors.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_connections_reused_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of connections reused.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_connections_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of connections.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_current_queue': [{'type': 'gauge',\n",
       "     'help': 'Current number of queued requests not assigned to any server.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_current_session_rate': [{'type': 'gauge',\n",
       "     'help': 'Current number of sessions per second over last elapsed second.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_current_sessions': [{'type': 'gauge',\n",
       "     'help': 'Current number of active sessions.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_http_average_connect_latency_milliseconds': [{'type': 'gauge',\n",
       "     'help': 'Average connect latency of the last 1024 requests in milliseconds.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_http_average_queue_latency_milliseconds': [{'type': 'gauge',\n",
       "     'help': 'Average latency to be dequeued of the last 1024 requests in milliseconds.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_http_average_response_latency_milliseconds': [{'type': 'gauge',\n",
       "     'help': 'Average response latency of the last 1024 requests in milliseconds.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_http_responses_total': [{'type': 'gauge',\n",
       "     'help': 'Total of HTTP responses.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_max_session_rate': [{'type': 'gauge',\n",
       "     'help': 'Maximum number of sessions per second.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_max_sessions': [{'type': 'gauge',\n",
       "     'help': 'Maximum observed number of active sessions.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_response_errors_total': [{'type': 'gauge',\n",
       "     'help': 'Total of response errors.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_backend_up': [{'type': 'gauge',\n",
       "     'help': 'Current health status of the backend (1 = UP, 0 = DOWN).',\n",
       "     'unit': ''}],\n",
       "   'haproxy_exporter_csv_parse_failures': [{'type': 'counter',\n",
       "     'help': 'Number of errors while parsing CSV.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_exporter_scrape_interval': [{'type': 'gauge',\n",
       "     'help': 'The time in seconds before another scrape is allowed, proportional to size of data.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_exporter_server_threshold': [{'type': 'gauge',\n",
       "     'help': 'Number of servers tracked and the current threshold value.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_exporter_total_scrapes': [{'type': 'counter',\n",
       "     'help': 'Current total HAProxy scrapes.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_frontend_bytes_in_total': [{'type': 'gauge',\n",
       "     'help': 'Current total of incoming bytes.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_frontend_bytes_out_total': [{'type': 'gauge',\n",
       "     'help': 'Current total of outgoing bytes.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_frontend_connections_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of connections.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_frontend_current_session_rate': [{'type': 'gauge',\n",
       "     'help': 'Current number of sessions per second over last elapsed second.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_frontend_current_sessions': [{'type': 'gauge',\n",
       "     'help': 'Current number of active sessions.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_frontend_http_responses_total': [{'type': 'gauge',\n",
       "     'help': 'Total of HTTP responses.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_frontend_max_session_rate': [{'type': 'gauge',\n",
       "     'help': 'Maximum observed number of sessions per second.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_frontend_max_sessions': [{'type': 'gauge',\n",
       "     'help': 'Maximum observed number of active sessions.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_process_cpu_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Total user and system CPU time spent in seconds.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_process_max_fds': [{'type': 'gauge',\n",
       "     'help': 'Maximum number of open file descriptors.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_process_resident_memory_bytes': [{'type': 'gauge',\n",
       "     'help': 'Resident memory size in bytes.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_process_start_time_seconds': [{'type': 'gauge',\n",
       "     'help': 'Start time of the process since unix epoch in seconds.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_process_virtual_memory_bytes': [{'type': 'gauge',\n",
       "     'help': 'Virtual memory size in bytes.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_process_virtual_memory_max_bytes': [{'type': 'gauge',\n",
       "     'help': 'Maximum amount of virtual memory available in bytes.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_bytes_in_total': [{'type': 'gauge',\n",
       "     'help': 'Current total of incoming bytes.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_bytes_out_total': [{'type': 'gauge',\n",
       "     'help': 'Current total of outgoing bytes.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_check_failures_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of failed health checks.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_connection_errors_total': [{'type': 'gauge',\n",
       "     'help': 'Total of connection errors.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_connections_reused_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of connections reused.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_connections_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of connections.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_current_queue': [{'type': 'gauge',\n",
       "     'help': 'Current number of queued requests assigned to this server.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_current_session_rate': [{'type': 'gauge',\n",
       "     'help': 'Current number of sessions per second over last elapsed second.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_current_sessions': [{'type': 'gauge',\n",
       "     'help': 'Current number of active sessions.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_downtime_seconds_total': [{'type': 'gauge',\n",
       "     'help': 'Total downtime in seconds.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_http_average_connect_latency_milliseconds': [{'type': 'gauge',\n",
       "     'help': 'Average connect latency of the last 1024 requests in milliseconds.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_http_average_queue_latency_milliseconds': [{'type': 'gauge',\n",
       "     'help': 'Average latency to be dequeued of the last 1024 requests in milliseconds.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_http_average_response_latency_milliseconds': [{'type': 'gauge',\n",
       "     'help': 'Average response latency of the last 1024 requests in milliseconds.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_http_responses_total': [{'type': 'gauge',\n",
       "     'help': 'Total of HTTP responses.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_max_session_rate': [{'type': 'gauge',\n",
       "     'help': 'Maximum observed number of sessions per second.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_max_sessions': [{'type': 'gauge',\n",
       "     'help': 'Maximum observed number of active sessions.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_response_errors_total': [{'type': 'gauge',\n",
       "     'help': 'Total of response errors.',\n",
       "     'unit': ''}],\n",
       "   'haproxy_server_up': [{'type': 'gauge',\n",
       "     'help': 'Current health status of the server (1 = UP, 0 = DOWN).',\n",
       "     'unit': ''}],\n",
       "   'haproxy_up': [{'type': 'gauge',\n",
       "     'help': 'Was the last scrape of haproxy successful.',\n",
       "     'unit': ''}],\n",
       "   'health_statuses_insights': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Information about the cluster health status as detected by Insights tooling.',\n",
       "     'unit': ''}],\n",
       "   'hidden_metric_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] The count of hidden metrics.',\n",
       "     'unit': ''}],\n",
       "   'http_inflight_requests': [{'type': 'gauge',\n",
       "     'help': 'Current number of HTTP requests the handler is responding to.',\n",
       "     'unit': ''}],\n",
       "   'http_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'A histogram of requests for kube-state-metrics metrics handler.',\n",
       "     'unit': ''},\n",
       "    {'type': 'histogram',\n",
       "     'help': 'Tracks the latencies for HTTP requests.',\n",
       "     'unit': ''}],\n",
       "   'http_request_size_bytes': [{'type': 'summary',\n",
       "     'help': 'Tracks the size of HTTP requests.',\n",
       "     'unit': ''}],\n",
       "   'http_requests': [{'type': 'counter',\n",
       "     'help': 'Tracks the number of HTTP requests.',\n",
       "     'unit': ''}],\n",
       "   'http_response_size_bytes': [{'type': 'summary',\n",
       "     'help': 'Tracks the size of HTTP responses.',\n",
       "     'unit': ''}],\n",
       "   'image_registry_image_stream_tags_total': [{'type': 'gauge',\n",
       "     'help': \"Number of image stream tags. Source is either 'imported' or 'pushed'. 'location' label shows if the tag lives in one of the 'openshift' namespaces or 'other'\",\n",
       "     'unit': ''}],\n",
       "   'image_registry_operator_image_pruner_install_status': [{'type': 'gauge',\n",
       "     'help': 'Installation status code related to the automatic image pruning feature. 0 = not installed, 1 = suspended, 2 = enabled',\n",
       "     'unit': ''}],\n",
       "   'image_registry_operator_storage_reconfigured_total': [{'type': 'counter',\n",
       "     'help': \"Total times the image registry's storage was reconfigured.\",\n",
       "     'unit': ''}],\n",
       "   'image_registry_storage_type': [{'type': 'gauge',\n",
       "     'help': 'Holds the storage in use for the image registry',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_build_info': [{'type': 'gauge',\n",
       "     'help': \"A metric with a constant '1' value labeled by major, minor, git commit & git version from which the image registry was built.\",\n",
       "     'unit': ''}],\n",
       "   'imageregistry_digest_cache_requests_total': [{'type': 'counter',\n",
       "     'help': 'Total number of requests without scope to the digest cache.',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_digest_cache_scoped_requests_total': [{'type': 'counter',\n",
       "     'help': 'Total number of scoped requests to the digest cache.',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_http_in_flight_requests': [{'type': 'gauge',\n",
       "     'help': 'A gauge of requests currently being served by the registry.',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_http_request_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'A histogram of latencies for requests to the registry.',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_http_request_size_bytes': [{'type': 'summary',\n",
       "     'help': 'A histogram of sizes of requests to the registry.',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_http_requests_total': [{'type': 'counter',\n",
       "     'help': 'A counter for requests to the registry.',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_http_response_size_bytes': [{'type': 'summary',\n",
       "     'help': 'A histogram of response sizes for requests to the registry.',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_pullthrough_blobstore_cache_requests_total': [{'type': 'counter',\n",
       "     'help': 'Total number of requests to the BlobStore cache.',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_pullthrough_repository_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Latency of operations with remote registries in seconds.',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_pullthrough_repository_errors_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative number of failed operations with remote registries.',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_request_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Request latency in seconds for each operation.',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_storage_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Latency of operations with the storage.',\n",
       "     'unit': ''}],\n",
       "   'imageregistry_storage_errors_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative number of failed operations with the storage.',\n",
       "     'unit': ''}],\n",
       "   'ingress_canary_check_duration': [{'type': 'histogram',\n",
       "     'help': 'Canary endpoint request time in ms',\n",
       "     'unit': ''}],\n",
       "   'ingress_canary_endpoint_wrong_port_echo': [{'type': 'counter',\n",
       "     'help': 'The ingress canary application received a test request on an incorrect port which may indicate that the router is \"wedged\"',\n",
       "     'unit': ''}],\n",
       "   'ingress_canary_route_reachable': [{'type': 'gauge',\n",
       "     'help': 'A gauge set to 0 or 1 to signify whether or not the canary application is reachable via a route',\n",
       "     'unit': ''}],\n",
       "   'ingress_controller_aws_nlb_active': [{'type': 'gauge',\n",
       "     'help': 'Report the number of active NLBs on AWS clusters.',\n",
       "     'unit': ''}],\n",
       "   'ingress_controller_conditions': [{'type': 'gauge',\n",
       "     'help': 'Report the conditions for ingress controllers. 0 is False and 1 is True.',\n",
       "     'unit': ''}],\n",
       "   'insightsclient_last_gather_time': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] ',\n",
       "     'unit': ''}],\n",
       "   'install_plan_count': [{'type': 'gauge',\n",
       "     'help': 'Number of install plans',\n",
       "     'unit': ''}],\n",
       "   'installplan_warnings_total': [{'type': 'counter',\n",
       "     'help': 'monotonic count of resources that generated warnings when applied as part of an InstallPlan (for example, due to deprecation)',\n",
       "     'unit': ''}],\n",
       "   'job_controller_job_finished_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] The number of finished job',\n",
       "     'unit': ''}],\n",
       "   'job_controller_job_pods_finished_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] The number of finished Pods that are fully tracked',\n",
       "     'unit': ''}],\n",
       "   'job_controller_job_sync_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] The time it took to sync a job',\n",
       "     'unit': ''}],\n",
       "   'job_controller_job_sync_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] The number of job syncs',\n",
       "     'unit': ''}],\n",
       "   'job_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for job_controller',\n",
       "     'unit': ''}],\n",
       "   'kube_apiserver_clusterip_allocator_allocated_ips': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Gauge measuring the number of allocated IPs for Services',\n",
       "     'unit': ''}],\n",
       "   'kube_apiserver_clusterip_allocator_allocation_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of Cluster IPs allocations',\n",
       "     'unit': ''}],\n",
       "   'kube_apiserver_clusterip_allocator_available_ips': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Gauge measuring the number of available IPs for Services',\n",
       "     'unit': ''}],\n",
       "   'kube_apiserver_pod_logs_pods_logs_backend_tls_failure_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Total number of requests for pods/logs that failed due to kubelet server TLS verification',\n",
       "     'unit': ''}],\n",
       "   'kube_apiserver_pod_logs_pods_logs_insecure_backend_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Total number of requests for pods/logs sliced by usage type: enforce_tls, skip_tls_allowed, skip_tls_denied',\n",
       "     'unit': ''}],\n",
       "   'kube_certificatesigningrequest_cert_length': [{'type': 'gauge',\n",
       "     'help': 'Length of the issued cert',\n",
       "     'unit': ''}],\n",
       "   'kube_certificatesigningrequest_condition': [{'type': 'gauge',\n",
       "     'help': 'The number of each certificatesigningrequest condition',\n",
       "     'unit': ''}],\n",
       "   'kube_certificatesigningrequest_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_configmap_info': [{'type': 'gauge',\n",
       "     'help': 'Information about configmap.',\n",
       "     'unit': ''}],\n",
       "   'kube_configmap_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_cronjob_info': [{'type': 'gauge',\n",
       "     'help': 'Info about cronjob.',\n",
       "     'unit': ''}],\n",
       "   'kube_cronjob_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_cronjob_next_schedule_time': [{'type': 'gauge',\n",
       "     'help': \"Next time the cronjob should be scheduled. The time after lastScheduleTime, or after the cron job's creation time if it's never been scheduled. Use this to determine if the job is delayed.\",\n",
       "     'unit': ''}],\n",
       "   'kube_cronjob_spec_failed_job_history_limit': [{'type': 'gauge',\n",
       "     'help': 'Failed job history limit tells the controller how many failed jobs should be preserved.',\n",
       "     'unit': ''}],\n",
       "   'kube_cronjob_spec_starting_deadline_seconds': [{'type': 'gauge',\n",
       "     'help': 'Deadline in seconds for starting the job if it misses scheduled time for any reason.',\n",
       "     'unit': ''}],\n",
       "   'kube_cronjob_spec_successful_job_history_limit': [{'type': 'gauge',\n",
       "     'help': 'Successful job history limit tells the controller how many completed jobs should be preserved.',\n",
       "     'unit': ''}],\n",
       "   'kube_cronjob_spec_suspend': [{'type': 'gauge',\n",
       "     'help': 'Suspend flag tells the controller to suspend subsequent executions.',\n",
       "     'unit': ''}],\n",
       "   'kube_cronjob_status_active': [{'type': 'gauge',\n",
       "     'help': 'Active holds pointers to currently running jobs.',\n",
       "     'unit': ''}],\n",
       "   'kube_cronjob_status_last_schedule_time': [{'type': 'gauge',\n",
       "     'help': 'LastScheduleTime keeps information of when was the last time the job was successfully scheduled.',\n",
       "     'unit': ''}],\n",
       "   'kube_cronjob_status_last_successful_time': [{'type': 'gauge',\n",
       "     'help': 'LastSuccessfulTime keeps information of when was the last time the job was completed successfully.',\n",
       "     'unit': ''}],\n",
       "   'kube_daemonset_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_daemonset_metadata_generation': [{'type': 'gauge',\n",
       "     'help': 'Sequence number representing a specific generation of the desired state.',\n",
       "     'unit': ''}],\n",
       "   'kube_daemonset_status_current_number_scheduled': [{'type': 'gauge',\n",
       "     'help': 'The number of nodes running at least one daemon pod and are supposed to.',\n",
       "     'unit': ''}],\n",
       "   'kube_daemonset_status_desired_number_scheduled': [{'type': 'gauge',\n",
       "     'help': 'The number of nodes that should be running the daemon pod.',\n",
       "     'unit': ''}],\n",
       "   'kube_daemonset_status_number_available': [{'type': 'gauge',\n",
       "     'help': 'The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and available',\n",
       "     'unit': ''}],\n",
       "   'kube_daemonset_status_number_misscheduled': [{'type': 'gauge',\n",
       "     'help': 'The number of nodes running a daemon pod but are not supposed to.',\n",
       "     'unit': ''}],\n",
       "   'kube_daemonset_status_number_ready': [{'type': 'gauge',\n",
       "     'help': 'The number of nodes that should be running the daemon pod and have one or more of the daemon pod running and ready.',\n",
       "     'unit': ''}],\n",
       "   'kube_daemonset_status_number_unavailable': [{'type': 'gauge',\n",
       "     'help': 'The number of nodes that should be running the daemon pod and have none of the daemon pod running and available',\n",
       "     'unit': ''}],\n",
       "   'kube_daemonset_status_observed_generation': [{'type': 'gauge',\n",
       "     'help': 'The most recent generation observed by the daemon set controller.',\n",
       "     'unit': ''}],\n",
       "   'kube_daemonset_status_updated_number_scheduled': [{'type': 'gauge',\n",
       "     'help': 'The total number of nodes that are running updated daemon pod',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_metadata_generation': [{'type': 'gauge',\n",
       "     'help': 'Sequence number representing a specific generation of the desired state.',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_spec_paused': [{'type': 'gauge',\n",
       "     'help': 'Whether the deployment is paused and will not be processed by the deployment controller.',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_spec_replicas': [{'type': 'gauge',\n",
       "     'help': 'Number of desired pods for a deployment.',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_spec_strategy_rollingupdate_max_surge': [{'type': 'gauge',\n",
       "     'help': 'Maximum number of replicas that can be scheduled above the desired number of replicas during a rolling update of a deployment.',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_spec_strategy_rollingupdate_max_unavailable': [{'type': 'gauge',\n",
       "     'help': 'Maximum number of unavailable replicas during a rolling update of a deployment.',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_status_condition': [{'type': 'gauge',\n",
       "     'help': 'The current status conditions of a deployment.',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_status_observed_generation': [{'type': 'gauge',\n",
       "     'help': 'The generation observed by the deployment controller.',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_status_replicas': [{'type': 'gauge',\n",
       "     'help': 'The number of replicas per deployment.',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_status_replicas_available': [{'type': 'gauge',\n",
       "     'help': 'The number of available replicas per deployment.',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_status_replicas_ready': [{'type': 'gauge',\n",
       "     'help': 'The number of ready replicas per deployment.',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_status_replicas_unavailable': [{'type': 'gauge',\n",
       "     'help': 'The number of unavailable replicas per deployment.',\n",
       "     'unit': ''}],\n",
       "   'kube_deployment_status_replicas_updated': [{'type': 'gauge',\n",
       "     'help': 'The number of updated replicas per deployment.',\n",
       "     'unit': ''}],\n",
       "   'kube_endpoint_address': [{'type': 'gauge',\n",
       "     'help': 'Information about Endpoint available and non available addresses.',\n",
       "     'unit': ''}],\n",
       "   'kube_endpoint_address_available': [{'type': 'gauge',\n",
       "     'help': '(Deprecated since v2.6.0) Number of addresses available in endpoint.',\n",
       "     'unit': ''}],\n",
       "   'kube_endpoint_address_not_ready': [{'type': 'gauge',\n",
       "     'help': '(Deprecated since v2.6.0) Number of addresses not ready in endpoint',\n",
       "     'unit': ''}],\n",
       "   'kube_endpoint_info': [{'type': 'gauge',\n",
       "     'help': 'Information about endpoint.',\n",
       "     'unit': ''}],\n",
       "   'kube_endpoint_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_endpoint_ports': [{'type': 'gauge',\n",
       "     'help': 'Information about the Endpoint ports.',\n",
       "     'unit': ''}],\n",
       "   'kube_horizontalpodautoscaler_info': [{'type': 'gauge',\n",
       "     'help': 'Information about this autoscaler.',\n",
       "     'unit': ''}],\n",
       "   'kube_horizontalpodautoscaler_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_horizontalpodautoscaler_metadata_generation': [{'type': 'gauge',\n",
       "     'help': 'The generation observed by the HorizontalPodAutoscaler controller.',\n",
       "     'unit': ''}],\n",
       "   'kube_horizontalpodautoscaler_spec_max_replicas': [{'type': 'gauge',\n",
       "     'help': 'Upper limit for the number of pods that can be set by the autoscaler; cannot be smaller than MinReplicas.',\n",
       "     'unit': ''}],\n",
       "   'kube_horizontalpodautoscaler_spec_min_replicas': [{'type': 'gauge',\n",
       "     'help': 'Lower limit for the number of pods that can be set by the autoscaler, default 1.',\n",
       "     'unit': ''}],\n",
       "   'kube_horizontalpodautoscaler_spec_target_metric': [{'type': 'gauge',\n",
       "     'help': 'The metric specifications used by this autoscaler when calculating the desired replica count.',\n",
       "     'unit': ''}],\n",
       "   'kube_horizontalpodautoscaler_status_condition': [{'type': 'gauge',\n",
       "     'help': 'The condition of this autoscaler.',\n",
       "     'unit': ''}],\n",
       "   'kube_horizontalpodautoscaler_status_current_replicas': [{'type': 'gauge',\n",
       "     'help': 'Current number of replicas of pods managed by this autoscaler.',\n",
       "     'unit': ''}],\n",
       "   'kube_horizontalpodautoscaler_status_desired_replicas': [{'type': 'gauge',\n",
       "     'help': 'Desired number of replicas of pods managed by this autoscaler.',\n",
       "     'unit': ''}],\n",
       "   'kube_horizontalpodautoscaler_status_target_metric': [{'type': 'gauge',\n",
       "     'help': 'The current metric status used by this autoscaler when calculating the desired replica count.',\n",
       "     'unit': ''}],\n",
       "   'kube_ingress_info': [{'type': 'gauge',\n",
       "     'help': 'Information about ingress.',\n",
       "     'unit': ''}],\n",
       "   'kube_ingress_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_ingress_path': [{'type': 'gauge',\n",
       "     'help': 'Ingress host, paths and backend service information.',\n",
       "     'unit': ''}],\n",
       "   'kube_ingress_tls': [{'type': 'gauge',\n",
       "     'help': 'Ingress TLS host and secret information.',\n",
       "     'unit': ''}],\n",
       "   'kube_job_complete': [{'type': 'gauge',\n",
       "     'help': 'The job has completed its execution.',\n",
       "     'unit': ''}],\n",
       "   'kube_job_failed': [{'type': 'gauge',\n",
       "     'help': 'The job has failed its execution.',\n",
       "     'unit': ''}],\n",
       "   'kube_job_info': [{'type': 'gauge',\n",
       "     'help': 'Information about job.',\n",
       "     'unit': ''}],\n",
       "   'kube_job_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_job_owner': [{'type': 'gauge',\n",
       "     'help': \"Information about the Job's owner.\",\n",
       "     'unit': ''}],\n",
       "   'kube_job_spec_active_deadline_seconds': [{'type': 'gauge',\n",
       "     'help': 'The duration in seconds relative to the startTime that the job may be active before the system tries to terminate it.',\n",
       "     'unit': ''}],\n",
       "   'kube_job_spec_completions': [{'type': 'gauge',\n",
       "     'help': 'The desired number of successfully finished pods the job should be run with.',\n",
       "     'unit': ''}],\n",
       "   'kube_job_spec_parallelism': [{'type': 'gauge',\n",
       "     'help': 'The maximum desired number of pods the job should run at any given time.',\n",
       "     'unit': ''}],\n",
       "   'kube_job_status_active': [{'type': 'gauge',\n",
       "     'help': 'The number of actively running pods.',\n",
       "     'unit': ''}],\n",
       "   'kube_job_status_completion_time': [{'type': 'gauge',\n",
       "     'help': 'CompletionTime represents time when the job was completed.',\n",
       "     'unit': ''}],\n",
       "   'kube_job_status_failed': [{'type': 'gauge',\n",
       "     'help': 'The number of pods which reached Phase Failed and the reason for failure.',\n",
       "     'unit': ''}],\n",
       "   'kube_job_status_start_time': [{'type': 'gauge',\n",
       "     'help': 'StartTime represents time when the job was acknowledged by the Job Manager.',\n",
       "     'unit': ''}],\n",
       "   'kube_job_status_succeeded': [{'type': 'gauge',\n",
       "     'help': 'The number of pods which reached Phase Succeeded.',\n",
       "     'unit': ''}],\n",
       "   'kube_lease_owner': [{'type': 'gauge',\n",
       "     'help': \"Information about the Lease's owner.\",\n",
       "     'unit': ''}],\n",
       "   'kube_lease_renew_time': [{'type': 'gauge',\n",
       "     'help': 'Kube lease renew time.',\n",
       "     'unit': ''}],\n",
       "   'kube_limitrange': [{'type': 'gauge',\n",
       "     'help': 'Information about limit range.',\n",
       "     'unit': ''}],\n",
       "   'kube_mutatingwebhookconfiguration_info': [{'type': 'gauge',\n",
       "     'help': 'Information about the MutatingWebhookConfiguration.',\n",
       "     'unit': ''}],\n",
       "   'kube_namespace_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_namespace_status_condition': [{'type': 'gauge',\n",
       "     'help': 'The condition of a namespace.',\n",
       "     'unit': ''}],\n",
       "   'kube_namespace_status_phase': [{'type': 'gauge',\n",
       "     'help': 'kubernetes namespace status phase.',\n",
       "     'unit': ''}],\n",
       "   'kube_networkpolicy_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_networkpolicy_spec_egress_rules': [{'type': 'gauge',\n",
       "     'help': 'Number of egress rules on the networkpolicy',\n",
       "     'unit': ''}],\n",
       "   'kube_networkpolicy_spec_ingress_rules': [{'type': 'gauge',\n",
       "     'help': 'Number of ingress rules on the networkpolicy',\n",
       "     'unit': ''}],\n",
       "   'kube_node_info': [{'type': 'gauge',\n",
       "     'help': 'Information about a cluster node.',\n",
       "     'unit': ''}],\n",
       "   'kube_node_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_node_role': [{'type': 'gauge',\n",
       "     'help': 'The role of a cluster node.',\n",
       "     'unit': ''}],\n",
       "   'kube_node_spec_taint': [{'type': 'gauge',\n",
       "     'help': 'The taint of a cluster node.',\n",
       "     'unit': ''}],\n",
       "   'kube_node_spec_unschedulable': [{'type': 'gauge',\n",
       "     'help': 'Whether a node can schedule new pods.',\n",
       "     'unit': ''}],\n",
       "   'kube_node_status_allocatable': [{'type': 'gauge',\n",
       "     'help': 'The allocatable for different resources of a node that are available for scheduling.',\n",
       "     'unit': ''}],\n",
       "   'kube_node_status_capacity': [{'type': 'gauge',\n",
       "     'help': 'The capacity for different resources of a node.',\n",
       "     'unit': ''}],\n",
       "   'kube_node_status_condition': [{'type': 'gauge',\n",
       "     'help': 'The condition of a cluster node.',\n",
       "     'unit': ''}],\n",
       "   'kube_persistentvolume_capacity_bytes': [{'type': 'gauge',\n",
       "     'help': 'Persistentvolume capacity in bytes.',\n",
       "     'unit': ''}],\n",
       "   'kube_persistentvolume_claim_ref': [{'type': 'gauge',\n",
       "     'help': 'Information about the Persistent Volume Claim Reference.',\n",
       "     'unit': ''}],\n",
       "   'kube_persistentvolume_info': [{'type': 'gauge',\n",
       "     'help': 'Information about persistentvolume.',\n",
       "     'unit': ''}],\n",
       "   'kube_persistentvolume_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_persistentvolume_status_phase': [{'type': 'gauge',\n",
       "     'help': 'The phase indicates if a volume is available, bound to a claim, or released by a claim.',\n",
       "     'unit': ''}],\n",
       "   'kube_persistentvolumeclaim_access_mode': [{'type': 'gauge',\n",
       "     'help': 'The access mode(s) specified by the persistent volume claim.',\n",
       "     'unit': ''}],\n",
       "   'kube_persistentvolumeclaim_info': [{'type': 'gauge',\n",
       "     'help': 'Information about persistent volume claim.',\n",
       "     'unit': ''}],\n",
       "   'kube_persistentvolumeclaim_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_persistentvolumeclaim_resource_requests_storage_bytes': [{'type': 'gauge',\n",
       "     'help': 'The capacity of storage requested by the persistent volume claim.',\n",
       "     'unit': ''}],\n",
       "   'kube_persistentvolumeclaim_status_condition': [{'type': 'gauge',\n",
       "     'help': 'Information about status of different conditions of persistent volume claim.',\n",
       "     'unit': ''}],\n",
       "   'kube_persistentvolumeclaim_status_phase': [{'type': 'gauge',\n",
       "     'help': 'The phase the persistent volume claim is currently in.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_container_info': [{'type': 'gauge',\n",
       "     'help': 'Information about a container in a pod.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_container_resource_limits': [{'type': 'gauge',\n",
       "     'help': 'The number of requested limit resource by a container.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_container_resource_requests': [{'type': 'gauge',\n",
       "     'help': 'The number of requested request resource by a container.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_container_state_started': [{'type': 'gauge',\n",
       "     'help': 'Start time in unix timestamp for a pod container.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_container_status_last_terminated_reason': [{'type': 'gauge',\n",
       "     'help': 'Describes the last reason the container was in terminated state.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_container_status_ready': [{'type': 'gauge',\n",
       "     'help': 'Describes whether the containers readiness check succeeded.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_container_status_restarts_total': [{'type': 'counter',\n",
       "     'help': 'The number of container restarts per container.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_container_status_terminated_reason': [{'type': 'gauge',\n",
       "     'help': 'Describes the reason the container is currently in terminated state.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_container_status_waiting': [{'type': 'gauge',\n",
       "     'help': 'Describes whether the container is currently in waiting state.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_container_status_waiting_reason': [{'type': 'gauge',\n",
       "     'help': 'Describes the reason the container is currently in waiting state.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_deletion_timestamp': [{'type': 'gauge',\n",
       "     'help': 'Unix deletion timestamp',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_info': [{'type': 'gauge',\n",
       "     'help': 'Information about pod.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_init_container_info': [{'type': 'gauge',\n",
       "     'help': 'Information about an init container in a pod.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_init_container_resource_limits': [{'type': 'gauge',\n",
       "     'help': 'The number of requested limit resource by an init container.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_init_container_resource_requests': [{'type': 'gauge',\n",
       "     'help': 'The number of requested request resource by an init container.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_init_container_status_last_terminated_reason': [{'type': 'gauge',\n",
       "     'help': 'Describes the last reason the init container was in terminated state.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_init_container_status_ready': [{'type': 'gauge',\n",
       "     'help': 'Describes whether the init containers readiness check succeeded.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_init_container_status_restarts_total': [{'type': 'counter',\n",
       "     'help': 'The number of restarts for the init container.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_init_container_status_terminated_reason': [{'type': 'gauge',\n",
       "     'help': 'Describes the reason the init container is currently in terminated state.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_init_container_status_waiting': [{'type': 'gauge',\n",
       "     'help': 'Describes whether the init container is currently in waiting state.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_init_container_status_waiting_reason': [{'type': 'gauge',\n",
       "     'help': 'Describes the reason the init container is currently in waiting state.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_ips': [{'type': 'gauge', 'help': 'Pod IP addresses', 'unit': ''}],\n",
       "   'kube_pod_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_overhead_cpu_cores': [{'type': 'gauge',\n",
       "     'help': 'The pod overhead in regards to cpu cores associated with running a pod.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_overhead_memory_bytes': [{'type': 'gauge',\n",
       "     'help': 'The pod overhead in regards to memory associated with running a pod.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_owner': [{'type': 'gauge',\n",
       "     'help': \"Information about the Pod's owner.\",\n",
       "     'unit': ''}],\n",
       "   'kube_pod_resource_limit': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Resources limit for workloads on the cluster, broken down by pod. This shows the resource usage the scheduler and kubelet expect per pod for resources along with the unit for the resource if any.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_resource_request': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Resources requested by workloads on the cluster, broken down by pod. This shows the resource usage the scheduler and kubelet expect per pod for resources along with the unit for the resource if any.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_runtimeclass_name_info': [{'type': 'gauge',\n",
       "     'help': 'The runtimeclass associated with the pod.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_spec_volumes_persistentvolumeclaims_info': [{'type': 'gauge',\n",
       "     'help': 'Information about persistentvolumeclaim volumes in a pod.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_spec_volumes_persistentvolumeclaims_readonly': [{'type': 'gauge',\n",
       "     'help': 'Describes whether a persistentvolumeclaim is mounted read only.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_start_time': [{'type': 'gauge',\n",
       "     'help': 'Start time in unix timestamp for a pod.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_status_phase': [{'type': 'gauge',\n",
       "     'help': 'The pods current phase.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_status_ready': [{'type': 'gauge',\n",
       "     'help': 'Describes whether the pod is ready to serve requests.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_status_reason': [{'type': 'gauge',\n",
       "     'help': 'The pod status reasons',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_status_scheduled_time': [{'type': 'gauge',\n",
       "     'help': 'Unix timestamp when pod moved into scheduled status',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_status_unschedulable': [{'type': 'gauge',\n",
       "     'help': 'Describes the unschedulable status for the pod.',\n",
       "     'unit': ''}],\n",
       "   'kube_pod_tolerations': [{'type': 'gauge',\n",
       "     'help': 'Information about the pod tolerations',\n",
       "     'unit': ''}],\n",
       "   'kube_poddisruptionbudget_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_poddisruptionbudget_status_current_healthy': [{'type': 'gauge',\n",
       "     'help': 'Current number of healthy pods',\n",
       "     'unit': ''}],\n",
       "   'kube_poddisruptionbudget_status_desired_healthy': [{'type': 'gauge',\n",
       "     'help': 'Minimum desired number of healthy pods',\n",
       "     'unit': ''}],\n",
       "   'kube_poddisruptionbudget_status_expected_pods': [{'type': 'gauge',\n",
       "     'help': 'Total number of pods counted by this disruption budget',\n",
       "     'unit': ''}],\n",
       "   'kube_poddisruptionbudget_status_observed_generation': [{'type': 'gauge',\n",
       "     'help': 'Most recent generation observed when updating this PDB status',\n",
       "     'unit': ''}],\n",
       "   'kube_poddisruptionbudget_status_pod_disruptions_allowed': [{'type': 'gauge',\n",
       "     'help': 'Number of pod disruptions that are currently allowed',\n",
       "     'unit': ''}],\n",
       "   'kube_replicaset_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_replicaset_owner': [{'type': 'gauge',\n",
       "     'help': \"Information about the ReplicaSet's owner.\",\n",
       "     'unit': ''}],\n",
       "   'kube_replicaset_spec_replicas': [{'type': 'gauge',\n",
       "     'help': 'Number of desired pods for a ReplicaSet.',\n",
       "     'unit': ''}],\n",
       "   'kube_replicaset_status_fully_labeled_replicas': [{'type': 'gauge',\n",
       "     'help': 'The number of fully labeled replicas per ReplicaSet.',\n",
       "     'unit': ''}],\n",
       "   'kube_replicaset_status_ready_replicas': [{'type': 'gauge',\n",
       "     'help': 'The number of ready replicas per ReplicaSet.',\n",
       "     'unit': ''}],\n",
       "   'kube_replicaset_status_replicas': [{'type': 'gauge',\n",
       "     'help': 'The number of replicas per ReplicaSet.',\n",
       "     'unit': ''}],\n",
       "   'kube_replicationcontroller_metadata_generation': [{'type': 'gauge',\n",
       "     'help': 'Sequence number representing a specific generation of the desired state.',\n",
       "     'unit': ''}],\n",
       "   'kube_replicationcontroller_owner': [{'type': 'gauge',\n",
       "     'help': \"Information about the ReplicationController's owner.\",\n",
       "     'unit': ''}],\n",
       "   'kube_replicationcontroller_spec_replicas': [{'type': 'gauge',\n",
       "     'help': 'Number of desired pods for a ReplicationController.',\n",
       "     'unit': ''}],\n",
       "   'kube_replicationcontroller_status_available_replicas': [{'type': 'gauge',\n",
       "     'help': 'The number of available replicas per ReplicationController.',\n",
       "     'unit': ''}],\n",
       "   'kube_replicationcontroller_status_fully_labeled_replicas': [{'type': 'gauge',\n",
       "     'help': 'The number of fully labeled replicas per ReplicationController.',\n",
       "     'unit': ''}],\n",
       "   'kube_replicationcontroller_status_observed_generation': [{'type': 'gauge',\n",
       "     'help': 'The generation observed by the ReplicationController controller.',\n",
       "     'unit': ''}],\n",
       "   'kube_replicationcontroller_status_ready_replicas': [{'type': 'gauge',\n",
       "     'help': 'The number of ready replicas per ReplicationController.',\n",
       "     'unit': ''}],\n",
       "   'kube_replicationcontroller_status_replicas': [{'type': 'gauge',\n",
       "     'help': 'The number of replicas per ReplicationController.',\n",
       "     'unit': ''}],\n",
       "   'kube_resourcequota': [{'type': 'gauge',\n",
       "     'help': 'Information about resource quota.',\n",
       "     'unit': ''}],\n",
       "   'kube_secret_info': [{'type': 'gauge',\n",
       "     'help': 'Information about secret.',\n",
       "     'unit': ''}],\n",
       "   'kube_secret_type': [{'type': 'gauge',\n",
       "     'help': 'Type about secret.',\n",
       "     'unit': ''}],\n",
       "   'kube_service_info': [{'type': 'gauge',\n",
       "     'help': 'Information about service.',\n",
       "     'unit': ''}],\n",
       "   'kube_service_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_service_spec_external_ip': [{'type': 'gauge',\n",
       "     'help': 'Service external ips. One series for each ip',\n",
       "     'unit': ''}],\n",
       "   'kube_service_spec_type': [{'type': 'gauge',\n",
       "     'help': 'Type about service.',\n",
       "     'unit': ''}],\n",
       "   'kube_service_status_load_balancer_ingress': [{'type': 'gauge',\n",
       "     'help': 'Service load balancer ingress status',\n",
       "     'unit': ''}],\n",
       "   'kube_state_metrics_build_info': [{'type': 'gauge',\n",
       "     'help': \"A metric with a constant '1' value labeled by version, revision, branch, and goversion from which kube_state_metrics was built.\",\n",
       "     'unit': ''}],\n",
       "   'kube_state_metrics_list_total': [{'type': 'counter',\n",
       "     'help': 'Number of total resource list in kube-state-metrics',\n",
       "     'unit': ''}],\n",
       "   'kube_state_metrics_shard_ordinal': [{'type': 'gauge',\n",
       "     'help': 'Current sharding ordinal/index of this instance',\n",
       "     'unit': ''}],\n",
       "   'kube_state_metrics_total_shards': [{'type': 'gauge',\n",
       "     'help': 'Number of total shards this instance is aware of',\n",
       "     'unit': ''}],\n",
       "   'kube_state_metrics_watch_total': [{'type': 'counter',\n",
       "     'help': 'Number of total resource watches in kube-state-metrics',\n",
       "     'unit': ''}],\n",
       "   'kube_statefulset_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_statefulset_metadata_generation': [{'type': 'gauge',\n",
       "     'help': 'Sequence number representing a specific generation of the desired state for the StatefulSet.',\n",
       "     'unit': ''}],\n",
       "   'kube_statefulset_replicas': [{'type': 'gauge',\n",
       "     'help': 'Number of desired pods for a StatefulSet.',\n",
       "     'unit': ''}],\n",
       "   'kube_statefulset_status_current_revision': [{'type': 'gauge',\n",
       "     'help': 'Indicates the version of the StatefulSet used to generate Pods in the sequence [0,currentReplicas).',\n",
       "     'unit': ''}],\n",
       "   'kube_statefulset_status_observed_generation': [{'type': 'gauge',\n",
       "     'help': 'The generation observed by the StatefulSet controller.',\n",
       "     'unit': ''}],\n",
       "   'kube_statefulset_status_replicas': [{'type': 'gauge',\n",
       "     'help': 'The number of replicas per StatefulSet.',\n",
       "     'unit': ''}],\n",
       "   'kube_statefulset_status_replicas_available': [{'type': 'gauge',\n",
       "     'help': 'The number of available replicas per StatefulSet.',\n",
       "     'unit': ''}],\n",
       "   'kube_statefulset_status_replicas_current': [{'type': 'gauge',\n",
       "     'help': 'The number of current replicas per StatefulSet.',\n",
       "     'unit': ''}],\n",
       "   'kube_statefulset_status_replicas_ready': [{'type': 'gauge',\n",
       "     'help': 'The number of ready replicas per StatefulSet.',\n",
       "     'unit': ''}],\n",
       "   'kube_statefulset_status_replicas_updated': [{'type': 'gauge',\n",
       "     'help': 'The number of updated replicas per StatefulSet.',\n",
       "     'unit': ''}],\n",
       "   'kube_statefulset_status_update_revision': [{'type': 'gauge',\n",
       "     'help': 'Indicates the version of the StatefulSet used to generate Pods in the sequence [replicas-updatedReplicas,replicas)',\n",
       "     'unit': ''}],\n",
       "   'kube_storageclass_info': [{'type': 'gauge',\n",
       "     'help': 'Information about storageclass.',\n",
       "     'unit': ''}],\n",
       "   'kube_storageclass_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_validatingwebhookconfiguration_info': [{'type': 'gauge',\n",
       "     'help': 'Information about the ValidatingWebhookConfiguration.',\n",
       "     'unit': ''}],\n",
       "   'kube_volumeattachment_info': [{'type': 'gauge',\n",
       "     'help': 'Information about volumeattachment.',\n",
       "     'unit': ''}],\n",
       "   'kube_volumeattachment_labels': [{'type': 'gauge',\n",
       "     'help': 'Kubernetes labels converted to Prometheus labels.',\n",
       "     'unit': ''}],\n",
       "   'kube_volumeattachment_spec_source_persistentvolume': [{'type': 'gauge',\n",
       "     'help': 'PersistentVolume source reference.',\n",
       "     'unit': ''}],\n",
       "   'kube_volumeattachment_status_attached': [{'type': 'gauge',\n",
       "     'help': 'Information about volumeattachment.',\n",
       "     'unit': ''}],\n",
       "   'kube_volumeattachment_status_attachment_metadata': [{'type': 'gauge',\n",
       "     'help': 'volumeattachment metadata.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_certificate_manager_client_expiration_renew_errors': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of certificate renewal errors.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_certificate_manager_client_ttl_seconds': [{'type': 'gauge',\n",
       "     'help': \"[ALPHA] Gauge of the TTL (time-to-live) of the Kubelet's client certificate. The value is in seconds until certificate expiry (negative if already expired). If client certificate is invalid or unused, the value will be +INF.\",\n",
       "     'unit': ''}],\n",
       "   'kubelet_certificate_manager_server_rotation_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Histogram of the number of seconds the previous certificate lived before being rotated.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_certificate_manager_server_ttl_seconds': [{'type': 'gauge',\n",
       "     'help': \"[ALPHA] Gauge of the shortest TTL (time-to-live) of the Kubelet's serving certificate. The value is in seconds until certificate expiry (negative if already expired). If serving certificate is invalid or unused, the value will be +INF.\",\n",
       "     'unit': ''}],\n",
       "   'kubelet_cgroup_manager_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration in seconds for cgroup manager operations. Broken down by method.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_container_log_filesystem_used_bytes': [{'type': 'gauge',\n",
       "     'help': \"[ALPHA] Bytes used by the container's logs on the filesystem.\",\n",
       "     'unit': ''}],\n",
       "   'kubelet_containers_per_pod_count': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] The number of containers per pod.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_graceful_shutdown_end_time_seconds': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Last graceful shutdown start time since unix epoch in seconds',\n",
       "     'unit': ''}],\n",
       "   'kubelet_graceful_shutdown_start_time_seconds': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Last graceful shutdown start time since unix epoch in seconds',\n",
       "     'unit': ''}],\n",
       "   'kubelet_http_inflight_requests': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of the inflight http requests',\n",
       "     'unit': ''}],\n",
       "   'kubelet_http_requests_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration in seconds to serve http requests',\n",
       "     'unit': ''}],\n",
       "   'kubelet_http_requests_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of the http requests received since the server started',\n",
       "     'unit': ''}],\n",
       "   'kubelet_managed_ephemeral_containers': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Current number of ephemeral containers in pods managed by this kubelet. Ephemeral containers will be ignored if disabled by the EphemeralContainers feature gate, and this number will be 0.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_node_name': [{'type': 'gauge',\n",
       "     'help': \"[ALPHA] The node's name. The count is always 1.\",\n",
       "     'unit': ''}],\n",
       "   'kubelet_pleg_discard_events': [{'type': 'counter',\n",
       "     'help': '[ALPHA] The number of discard events in PLEG.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_pleg_last_seen_seconds': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Timestamp in seconds when PLEG was last seen active.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_pleg_relist_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration in seconds for relisting pods in PLEG.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_pleg_relist_interval_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Interval in seconds between relisting in PLEG.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_pod_start_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration in seconds from kubelet seeing a pod for the first time to the pod starting to run',\n",
       "     'unit': ''}],\n",
       "   'kubelet_pod_worker_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration in seconds to sync a single pod. Broken down by operation type: create, update, or sync',\n",
       "     'unit': ''}],\n",
       "   'kubelet_pod_worker_start_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration in seconds from kubelet seeing a pod to starting a worker.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_run_podsandbox_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration in seconds of the run_podsandbox operations. Broken down by RuntimeClass.Handler.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_running_containers': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of containers currently running',\n",
       "     'unit': ''}],\n",
       "   'kubelet_running_pods': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of pods that have a running pod sandbox',\n",
       "     'unit': ''}],\n",
       "   'kubelet_runtime_operations_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration in seconds of runtime operations. Broken down by operation type.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_runtime_operations_errors_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Cumulative number of runtime operation errors by operation type.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_runtime_operations_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Cumulative number of runtime operations by operation type.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_server_expiration_renew_errors': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of certificate renewal errors.',\n",
       "     'unit': ''}],\n",
       "   'kubelet_started_containers_errors_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Cumulative number of errors when starting containers',\n",
       "     'unit': ''}],\n",
       "   'kubelet_started_containers_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Cumulative number of containers started',\n",
       "     'unit': ''}],\n",
       "   'kubelet_started_pods_errors_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Cumulative number of errors when starting pods',\n",
       "     'unit': ''}],\n",
       "   'kubelet_started_pods_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Cumulative number of pods started',\n",
       "     'unit': ''}],\n",
       "   'kubelet_volume_metric_collection_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration in seconds to calculate volume stats',\n",
       "     'unit': ''}],\n",
       "   'kubernetes_build_info': [{'type': 'gauge',\n",
       "     'help': \"[ALPHA] A metric with a constant '1' value labeled by major, minor, git version, git commit, git tree state, build date, Go version, and compiler from which Kubernetes was built, and platform on which it is running.\",\n",
       "     'unit': ''}],\n",
       "   'leader_election_master_status': [{'type': 'gauge',\n",
       "     'help': \"[ALPHA] Gauge of if the reporting system is master of the relevant lease, 0 indicates backup, 1 indicates master. 'name' is the string used to identify the lease. Please make sure to group by name.\",\n",
       "     'unit': ''}],\n",
       "   'machine_config_controller_paused_pool_kubelet_ca': [{'type': 'gauge',\n",
       "     'help': 'Set to the unix timestamp in utc of the current certificate expiry date if a certificate rotation is pending in specified paused pool',\n",
       "     'unit': ''}],\n",
       "   'machine_cpu_cores': [{'type': 'gauge',\n",
       "     'help': 'Number of logical CPU cores.',\n",
       "     'unit': ''}],\n",
       "   'machine_cpu_physical_cores': [{'type': 'gauge',\n",
       "     'help': 'Number of physical CPU cores.',\n",
       "     'unit': ''}],\n",
       "   'machine_cpu_sockets': [{'type': 'gauge',\n",
       "     'help': 'Number of CPU sockets.',\n",
       "     'unit': ''}],\n",
       "   'machine_memory_bytes': [{'type': 'gauge',\n",
       "     'help': 'Amount of memory installed on the machine.',\n",
       "     'unit': ''}],\n",
       "   'machine_nvm_avg_power_budget_watts': [{'type': 'gauge',\n",
       "     'help': 'NVM power budget.',\n",
       "     'unit': ''}],\n",
       "   'machine_nvm_capacity': [{'type': 'gauge',\n",
       "     'help': 'NVM capacity value labeled by NVM mode (memory mode or app direct mode).',\n",
       "     'unit': ''}],\n",
       "   'machine_scrape_error': [{'type': 'gauge',\n",
       "     'help': '1 if there was an error while getting machine metrics, 0 otherwise.',\n",
       "     'unit': ''}],\n",
       "   'mapi_current_pending_csr': [{'type': 'gauge',\n",
       "     'help': 'Count of pending CSRs at the cluster level',\n",
       "     'unit': ''}],\n",
       "   'mapi_machine_items': [{'type': 'gauge',\n",
       "     'help': 'Count of machine objects currently at the apiserver',\n",
       "     'unit': ''}],\n",
       "   'mapi_machineset_items': [{'type': 'gauge',\n",
       "     'help': 'Count of machinesets at the apiserver',\n",
       "     'unit': ''}],\n",
       "   'mapi_mao_collector_up': [{'type': 'gauge',\n",
       "     'help': 'Machine API Operator metrics are being collected and reported successfully',\n",
       "     'unit': ''}],\n",
       "   'mapi_max_pending_csr': [{'type': 'gauge',\n",
       "     'help': 'Threshold value of the pending CSRs beyond which any new CSR requests will be ignored ',\n",
       "     'unit': ''}],\n",
       "   'mcd_drain_err': [{'type': 'gauge',\n",
       "     'help': 'logs failed drain',\n",
       "     'unit': ''}],\n",
       "   'mcd_host_os_and_version': [{'type': 'gauge',\n",
       "     'help': 'os that MCD is running on and version if RHCOS',\n",
       "     'unit': ''}],\n",
       "   'mcd_kubelet_state': [{'type': 'gauge',\n",
       "     'help': 'state of kubelet health monitor',\n",
       "     'unit': ''}],\n",
       "   'mcd_pivot_err': [{'type': 'gauge',\n",
       "     'help': 'errors encountered during pivot',\n",
       "     'unit': ''}],\n",
       "   'mcd_reboot_err': [{'type': 'gauge', 'help': '', 'unit': ''}],\n",
       "   'mcd_state': [{'type': 'gauge',\n",
       "     'help': 'state of daemon on specified node',\n",
       "     'unit': ''}],\n",
       "   'mcd_update_state': [{'type': 'gauge',\n",
       "     'help': 'completed update config or error',\n",
       "     'unit': ''}],\n",
       "   'namespace_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for namespace_controller',\n",
       "     'unit': ''}],\n",
       "   'net_conntrack_dialer_conn_attempted_total': [{'type': 'counter',\n",
       "     'help': 'Total number of connections attempted by the given dialer a given name.',\n",
       "     'unit': ''}],\n",
       "   'net_conntrack_dialer_conn_closed_total': [{'type': 'counter',\n",
       "     'help': 'Total number of connections closed which originated from the dialer of a given name.',\n",
       "     'unit': ''}],\n",
       "   'net_conntrack_dialer_conn_established_total': [{'type': 'counter',\n",
       "     'help': 'Total number of connections successfully established by the given dialer a given name.',\n",
       "     'unit': ''}],\n",
       "   'net_conntrack_dialer_conn_failed_total': [{'type': 'counter',\n",
       "     'help': 'Total number of connections failed to dial by the dialer a given name.',\n",
       "     'unit': ''}],\n",
       "   'net_conntrack_listener_conn_accepted_total': [{'type': 'counter',\n",
       "     'help': 'Total number of connections opened to the listener of a given name.',\n",
       "     'unit': ''}],\n",
       "   'net_conntrack_listener_conn_closed_total': [{'type': 'counter',\n",
       "     'help': 'Total number of connections closed that were made to the listener of a given name.',\n",
       "     'unit': ''}],\n",
       "   'network_attachment_definition_enabled_instance_up': [{'type': 'gauge',\n",
       "     'help': 'Metric to identify clusters with network attachment definition enabled instances.',\n",
       "     'unit': ''}],\n",
       "   'network_attachment_definition_instances': [{'type': 'gauge',\n",
       "     'help': 'Metric to get number of instance using network attachment definition in the cluster.',\n",
       "     'unit': ''}],\n",
       "   'node_arp_entries': [{'type': 'gauge',\n",
       "     'help': 'ARP entries by device',\n",
       "     'unit': ''}],\n",
       "   'node_authorizer_graph_actions_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Histogram of duration of graph actions in node authorizer.',\n",
       "     'unit': ''}],\n",
       "   'node_boot_time_seconds': [{'type': 'gauge',\n",
       "     'help': 'Node boot time, in unixtime.',\n",
       "     'unit': ''}],\n",
       "   'node_boots_total': [{'type': 'counter',\n",
       "     'help': 'reports a single series which is the number of times this system has been booted excluding the current boot. If the value is zero, this is the first time the system has booted. The value is always non-negative.',\n",
       "     'unit': ''}],\n",
       "   'node_collector_evictions_total': [{'type': 'counter',\n",
       "     'help': '[STABLE] Number of Node evictions that happened since current instance of NodeController started.',\n",
       "     'unit': ''}],\n",
       "   'node_collector_unhealthy_nodes_in_zone': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Gauge measuring number of not Ready Nodes per zones.',\n",
       "     'unit': ''}],\n",
       "   'node_collector_zone_health': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Gauge measuring percentage of healthy nodes per zone.',\n",
       "     'unit': ''}],\n",
       "   'node_collector_zone_size': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Gauge measuring number of registered Nodes per zones.',\n",
       "     'unit': ''}],\n",
       "   'node_context_switches_total': [{'type': 'counter',\n",
       "     'help': 'Total number of context switches.',\n",
       "     'unit': ''}],\n",
       "   'node_cooling_device_cur_state': [{'type': 'gauge',\n",
       "     'help': 'Current throttle state of the cooling device',\n",
       "     'unit': ''}],\n",
       "   'node_cooling_device_max_state': [{'type': 'gauge',\n",
       "     'help': 'Maximum throttle state of the cooling device',\n",
       "     'unit': ''}],\n",
       "   'node_cpu_guest_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Seconds the CPUs spent in guests (VMs) for each mode.',\n",
       "     'unit': ''}],\n",
       "   'node_cpu_info': [{'type': 'gauge',\n",
       "     'help': 'CPU information from /proc/cpuinfo.',\n",
       "     'unit': ''}],\n",
       "   'node_cpu_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Seconds the CPUs spent in each mode.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_discard_time_seconds_total': [{'type': 'counter',\n",
       "     'help': 'This is the total number of seconds spent by all discards.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_discarded_sectors_total': [{'type': 'counter',\n",
       "     'help': 'The total number of sectors discarded successfully.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_discards_completed_total': [{'type': 'counter',\n",
       "     'help': 'The total number of discards completed successfully.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_discards_merged_total': [{'type': 'counter',\n",
       "     'help': 'The total number of discards merged.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_filesystem_info': [{'type': 'gauge',\n",
       "     'help': 'Info about disk filesystem.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_info': [{'type': 'gauge',\n",
       "     'help': 'Info of /sys/block/<block_device>.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_io_now': [{'type': 'gauge',\n",
       "     'help': 'The number of I/Os currently in progress.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_io_time_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Total seconds spent doing I/Os.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_io_time_weighted_seconds_total': [{'type': 'counter',\n",
       "     'help': 'The weighted # of seconds spent doing I/Os.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_read_bytes_total': [{'type': 'counter',\n",
       "     'help': 'The total number of bytes read successfully.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_read_time_seconds_total': [{'type': 'counter',\n",
       "     'help': 'The total number of seconds spent by all reads.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_reads_completed_total': [{'type': 'counter',\n",
       "     'help': 'The total number of reads completed successfully.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_reads_merged_total': [{'type': 'counter',\n",
       "     'help': 'The total number of reads merged.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_write_time_seconds_total': [{'type': 'counter',\n",
       "     'help': 'This is the total number of seconds spent by all writes.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_writes_completed_total': [{'type': 'counter',\n",
       "     'help': 'The total number of writes completed successfully.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_writes_merged_total': [{'type': 'counter',\n",
       "     'help': 'The number of writes merged.',\n",
       "     'unit': ''}],\n",
       "   'node_disk_written_bytes_total': [{'type': 'counter',\n",
       "     'help': 'The total number of bytes written successfully.',\n",
       "     'unit': ''}],\n",
       "   'node_dmi_info': [{'type': 'gauge',\n",
       "     'help': \"A metric with a constant '1' value labeled by bios_date, bios_release, bios_vendor, bios_version, board_asset_tag, board_name, board_serial, board_vendor, board_version, chassis_asset_tag, chassis_serial, chassis_vendor, chassis_version, product_family, product_name, product_serial, product_sku, product_uuid, product_version, system_vendor if provided by DMI.\",\n",
       "     'unit': ''}],\n",
       "   'node_entropy_available_bits': [{'type': 'gauge',\n",
       "     'help': 'Bits of available entropy.',\n",
       "     'unit': ''}],\n",
       "   'node_entropy_pool_size_bits': [{'type': 'gauge',\n",
       "     'help': 'Bits of entropy pool.',\n",
       "     'unit': ''}],\n",
       "   'node_exporter_build_info': [{'type': 'gauge',\n",
       "     'help': \"A metric with a constant '1' value labeled by version, revision, branch, and goversion from which node_exporter was built.\",\n",
       "     'unit': ''}],\n",
       "   'node_filefd_allocated': [{'type': 'gauge',\n",
       "     'help': 'File descriptor statistics: allocated.',\n",
       "     'unit': ''}],\n",
       "   'node_filefd_maximum': [{'type': 'gauge',\n",
       "     'help': 'File descriptor statistics: maximum.',\n",
       "     'unit': ''}],\n",
       "   'node_filesystem_avail_bytes': [{'type': 'gauge',\n",
       "     'help': 'Filesystem space available to non-root users in bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_filesystem_device_error': [{'type': 'gauge',\n",
       "     'help': 'Whether an error occurred while getting statistics for the given device.',\n",
       "     'unit': ''}],\n",
       "   'node_filesystem_files': [{'type': 'gauge',\n",
       "     'help': 'Filesystem total file nodes.',\n",
       "     'unit': ''}],\n",
       "   'node_filesystem_files_free': [{'type': 'gauge',\n",
       "     'help': 'Filesystem total free file nodes.',\n",
       "     'unit': ''}],\n",
       "   'node_filesystem_free_bytes': [{'type': 'gauge',\n",
       "     'help': 'Filesystem free space in bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_filesystem_readonly': [{'type': 'gauge',\n",
       "     'help': 'Filesystem read-only status.',\n",
       "     'unit': ''}],\n",
       "   'node_filesystem_size_bytes': [{'type': 'gauge',\n",
       "     'help': 'Filesystem size in bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_forks_total': [{'type': 'counter',\n",
       "     'help': 'Total number of forks.',\n",
       "     'unit': ''}],\n",
       "   'node_intr_total': [{'type': 'counter',\n",
       "     'help': 'Total number of interrupts serviced.',\n",
       "     'unit': ''}],\n",
       "   'node_lifecycle_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for node_lifecycle_controller',\n",
       "     'unit': ''}],\n",
       "   'node_load1': [{'type': 'gauge', 'help': '1m load average.', 'unit': ''}],\n",
       "   'node_load15': [{'type': 'gauge', 'help': '15m load average.', 'unit': ''}],\n",
       "   'node_load5': [{'type': 'gauge', 'help': '5m load average.', 'unit': ''}],\n",
       "   'node_memory_Active_anon_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Active_anon_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Active_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Active_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Active_file_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Active_file_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_AnonHugePages_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field AnonHugePages_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_AnonPages_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field AnonPages_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Bounce_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Bounce_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Buffers_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Buffers_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Cached_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Cached_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_CommitLimit_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field CommitLimit_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Committed_AS_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Committed_AS_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_DirectMap1G_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field DirectMap1G_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_DirectMap2M_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field DirectMap2M_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_DirectMap4k_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field DirectMap4k_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Dirty_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Dirty_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_FileHugePages_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field FileHugePages_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_FilePmdMapped_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field FilePmdMapped_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_HardwareCorrupted_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field HardwareCorrupted_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_HugePages_Free': [{'type': 'gauge',\n",
       "     'help': 'Memory information field HugePages_Free.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_HugePages_Rsvd': [{'type': 'gauge',\n",
       "     'help': 'Memory information field HugePages_Rsvd.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_HugePages_Surp': [{'type': 'gauge',\n",
       "     'help': 'Memory information field HugePages_Surp.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_HugePages_Total': [{'type': 'gauge',\n",
       "     'help': 'Memory information field HugePages_Total.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Hugepagesize_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Hugepagesize_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Hugetlb_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Hugetlb_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Inactive_anon_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Inactive_anon_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Inactive_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Inactive_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Inactive_file_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Inactive_file_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_KReclaimable_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field KReclaimable_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_KernelStack_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field KernelStack_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Mapped_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Mapped_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_MemAvailable_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field MemAvailable_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_MemFree_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field MemFree_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_MemTotal_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field MemTotal_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Mlocked_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Mlocked_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_NFS_Unstable_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field NFS_Unstable_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_PageTables_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field PageTables_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Percpu_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Percpu_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_SReclaimable_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field SReclaimable_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_SUnreclaim_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field SUnreclaim_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_ShmemHugePages_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field ShmemHugePages_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_ShmemPmdMapped_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field ShmemPmdMapped_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Shmem_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Shmem_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Slab_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Slab_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_SwapCached_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field SwapCached_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_SwapFree_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field SwapFree_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_SwapTotal_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field SwapTotal_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Unevictable_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Unevictable_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_VmallocChunk_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field VmallocChunk_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_VmallocTotal_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field VmallocTotal_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_VmallocUsed_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field VmallocUsed_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_WritebackTmp_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field WritebackTmp_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_memory_Writeback_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory information field Writeback_bytes.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Icmp6_InErrors': [{'type': 'unknown',\n",
       "     'help': 'Statistic Icmp6InErrors.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Icmp6_InMsgs': [{'type': 'unknown',\n",
       "     'help': 'Statistic Icmp6InMsgs.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Icmp6_OutMsgs': [{'type': 'unknown',\n",
       "     'help': 'Statistic Icmp6OutMsgs.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Icmp_InErrors': [{'type': 'unknown',\n",
       "     'help': 'Statistic IcmpInErrors.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Icmp_InMsgs': [{'type': 'unknown',\n",
       "     'help': 'Statistic IcmpInMsgs.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Icmp_OutMsgs': [{'type': 'unknown',\n",
       "     'help': 'Statistic IcmpOutMsgs.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Ip6_InOctets': [{'type': 'unknown',\n",
       "     'help': 'Statistic Ip6InOctets.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Ip6_OutOctets': [{'type': 'unknown',\n",
       "     'help': 'Statistic Ip6OutOctets.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_IpExt_InOctets': [{'type': 'unknown',\n",
       "     'help': 'Statistic IpExtInOctets.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_IpExt_OutOctets': [{'type': 'unknown',\n",
       "     'help': 'Statistic IpExtOutOctets.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Ip_Forwarding': [{'type': 'unknown',\n",
       "     'help': 'Statistic IpForwarding.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_TcpExt_ListenDrops': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpExtListenDrops.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_TcpExt_ListenOverflows': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpExtListenOverflows.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_TcpExt_SyncookiesFailed': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpExtSyncookiesFailed.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_TcpExt_SyncookiesRecv': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpExtSyncookiesRecv.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_TcpExt_SyncookiesSent': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpExtSyncookiesSent.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_TcpExt_TCPSynRetrans': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpExtTCPSynRetrans.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_TcpExt_TCPTimeouts': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpExtTCPTimeouts.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Tcp_ActiveOpens': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpActiveOpens.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Tcp_CurrEstab': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpCurrEstab.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Tcp_InErrs': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpInErrs.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Tcp_InSegs': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpInSegs.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Tcp_OutRsts': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpOutRsts.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Tcp_OutSegs': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpOutSegs.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Tcp_PassiveOpens': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpPassiveOpens.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Tcp_RetransSegs': [{'type': 'unknown',\n",
       "     'help': 'Statistic TcpRetransSegs.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Udp6_InDatagrams': [{'type': 'unknown',\n",
       "     'help': 'Statistic Udp6InDatagrams.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Udp6_InErrors': [{'type': 'unknown',\n",
       "     'help': 'Statistic Udp6InErrors.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Udp6_NoPorts': [{'type': 'unknown',\n",
       "     'help': 'Statistic Udp6NoPorts.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Udp6_OutDatagrams': [{'type': 'unknown',\n",
       "     'help': 'Statistic Udp6OutDatagrams.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Udp6_RcvbufErrors': [{'type': 'unknown',\n",
       "     'help': 'Statistic Udp6RcvbufErrors.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Udp6_SndbufErrors': [{'type': 'unknown',\n",
       "     'help': 'Statistic Udp6SndbufErrors.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_UdpLite6_InErrors': [{'type': 'unknown',\n",
       "     'help': 'Statistic UdpLite6InErrors.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_UdpLite_InErrors': [{'type': 'unknown',\n",
       "     'help': 'Statistic UdpLiteInErrors.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Udp_InDatagrams': [{'type': 'unknown',\n",
       "     'help': 'Statistic UdpInDatagrams.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Udp_InErrors': [{'type': 'unknown',\n",
       "     'help': 'Statistic UdpInErrors.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Udp_NoPorts': [{'type': 'unknown',\n",
       "     'help': 'Statistic UdpNoPorts.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Udp_OutDatagrams': [{'type': 'unknown',\n",
       "     'help': 'Statistic UdpOutDatagrams.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Udp_RcvbufErrors': [{'type': 'unknown',\n",
       "     'help': 'Statistic UdpRcvbufErrors.',\n",
       "     'unit': ''}],\n",
       "   'node_netstat_Udp_SndbufErrors': [{'type': 'unknown',\n",
       "     'help': 'Statistic UdpSndbufErrors.',\n",
       "     'unit': ''}],\n",
       "   'node_network_address_assign_type': [{'type': 'gauge',\n",
       "     'help': 'address_assign_type value of /sys/class/net/<iface>.',\n",
       "     'unit': ''}],\n",
       "   'node_network_carrier': [{'type': 'gauge',\n",
       "     'help': 'carrier value of /sys/class/net/<iface>.',\n",
       "     'unit': ''}],\n",
       "   'node_network_carrier_changes_total': [{'type': 'counter',\n",
       "     'help': 'carrier_changes_total value of /sys/class/net/<iface>.',\n",
       "     'unit': ''}],\n",
       "   'node_network_carrier_down_changes_total': [{'type': 'counter',\n",
       "     'help': 'carrier_down_changes_total value of /sys/class/net/<iface>.',\n",
       "     'unit': ''}],\n",
       "   'node_network_carrier_up_changes_total': [{'type': 'counter',\n",
       "     'help': 'carrier_up_changes_total value of /sys/class/net/<iface>.',\n",
       "     'unit': ''}],\n",
       "   'node_network_device_id': [{'type': 'gauge',\n",
       "     'help': 'device_id value of /sys/class/net/<iface>.',\n",
       "     'unit': ''}],\n",
       "   'node_network_dormant': [{'type': 'gauge',\n",
       "     'help': 'dormant value of /sys/class/net/<iface>.',\n",
       "     'unit': ''}],\n",
       "   ...}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://prometheus.io/docs/prometheus/latest/querying/api/\n",
    "r = requests.get(\n",
    "    f\"https://{prometheus_host}/v1/metadata\",\n",
    "    verify=False,\n",
    "    headers=kubeConfig.api_key,\n",
    ")\n",
    "r.status_code, r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " {'status': 'success',\n",
       "  'data': {'groups': [{'name': 'CloudCredentialOperator',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cloud-credential-operator-cloud-credential-operator-alerts-55b21d2b-48ae-4cf2-b534-848640372dbd.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'CloudCredentialOperatorTargetNamespaceMissing',\n",
       "       'query': 'cco_credentials_requests_conditions{condition=\"MissingTargetNamespace\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"At least one CredentialsRequest custom resource has specified in its .spec.secretRef.namespace field a namespace which does not presently exist. This means the Cloud Credential Operator in the openshift-cloud-credential-operator namespace cannot process the CredentialsRequest resource. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .status.condition showing a condition type of MissingTargetNamespace set to True.\",\n",
       "        'message': 'CredentialsRequest(s) pointing to non-existent namespace',\n",
       "        'summary': 'One ore more CredentialsRequest CRs are asking to save credentials to a non-existent namespace.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.0004886,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.069585919Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CloudCredentialOperatorProvisioningFailed',\n",
       "       'query': 'cco_credentials_requests_conditions{condition=\"CredentialsProvisionFailure\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"While processing a CredentialsRequest, the Cloud Credential Operator encountered an issue. Check the conditions of all CredentialsRequets with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .stats.condition showing a condition type of CredentialsProvisionFailure set to True for more details on the issue.\",\n",
       "        'message': 'CredentialsRequest(s) unable to be fulfilled',\n",
       "        'summary': 'One or more CredentialsRequest CRs are unable to be processed.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00011213,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.070077265Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CloudCredentialOperatorDeprovisioningFailed',\n",
       "       'query': 'cco_credentials_requests_conditions{condition=\"CredentialsDeprovisionFailure\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"While processing a CredentialsRequest marked for deletion, the Cloud Credential Operator encountered an issue. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .status.condition showing a condition type of CredentialsDeprovisionFailure set to True for more details on the issue.\",\n",
       "        'message': 'CredentialsRequest(s) unable to be cleaned up',\n",
       "        'summary': 'One or more CredentialsRequest CRs are unable to be deleted.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.929e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.070191669Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CloudCredentialOperatorInsufficientCloudCreds',\n",
       "       'query': 'cco_credentials_requests_conditions{condition=\"InsufficientCloudCreds\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"The Cloud Credential Operator has determined that there are insufficient permissions to process one or more CredentialsRequest CRs. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .status.condition showing a condition type of InsufficientCloudCreds set to True for more details.\",\n",
       "        'message': \"Cluster's cloud credentials insufficient for minting or passthrough\",\n",
       "        'summary': 'Problem with the available platform credentials.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 6.7828e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.07027178Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CloudCredentialOperatorStaleCredentials',\n",
       "       'query': 'cco_credentials_requests_conditions{condition=\"StaleCredentials\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"The Cloud Credential Operator (CCO) has detected one or more stale CredentialsRequest CRs that need to be manually deleted. When the CCO is in Manual credentials mode, it will not automatially clean up stale CredentialsRequest CRs (that may no longer be necessary in the present version of OpenShift because it could involve needing to clean up manually created cloud resources. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .status.condition showing a condition type of StaleCredentials set to True. Determine the appropriate steps to clean up/deprovision any previously provisioned cloud resources. Finally, delete the CredentialsRequest with an 'oc delete'.\",\n",
       "        'message': '1 or more credentials requests are stale and should be deleted. Check the status.conditions on CredentialsRequest CRs to identify the stale one(s).',\n",
       "        'summary': 'One or more CredentialsRequest CRs are stale and should be deleted.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 4.7469e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.0703404Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000820014,\n",
       "     'lastEvaluation': '2023-05-29T21:24:22.069570941Z'},\n",
       "    {'name': 'cluster-machine-approver.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-machine-approver-machineapprover-rules-89b653dc-4cd8-4c37-86bb-477ce9857161.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineApproverMaxPendingCSRsReached',\n",
       "       'query': 'mapi_current_pending_csr > mapi_max_pending_csr',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The number of pending CertificateSigningRequests has exceeded the\\nmaximum threshold (current number of machine + 100). Check the\\npending CSRs to determine which machines need approval, also check\\nthat the nodelink controller is running in the openshift-machine-api\\nnamespace.\\n',\n",
       "        'summary': 'max pending CSRs threshold reached.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000289084,\n",
       "       'lastEvaluation': '2023-05-29T21:24:29.069829069Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000299193,\n",
       "     'lastEvaluation': '2023-05-29T21:24:29.069821595Z'},\n",
       "    {'name': 'node-tuning-operator.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-node-tuning-operator-node-tuning-operator-bbd2b791-a614-407e-92eb-4b8680685d9b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'NTOPodsNotReady',\n",
       "       'query': 'kube_pod_status_ready{condition=\"true\",namespace=\"openshift-cluster-node-tuning-operator\"} == 0',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Pod {{ $labels.pod }} is not ready.\\nReview the \"Event\" objects in \"openshift-cluster-node-tuning-operator\" namespace for further details.\\n',\n",
       "        'summary': 'Pod {{ $labels.pod }} is not ready.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000277292,\n",
       "       'lastEvaluation': '2023-05-29T21:24:30.345239134Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NTODegraded',\n",
       "       'query': 'nto_degraded_info == 1',\n",
       "       'duration': 7200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The Node Tuning Operator is degraded. Review the \"node-tuning\" ClusterOperator object for further details.',\n",
       "        'summary': 'The Node Tuning Operator is degraded.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 6.1706e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:30.345517959Z',\n",
       "       'type': 'alerting'},\n",
       "      {'name': 'nto_custom_profiles:count',\n",
       "       'query': 'count by (_id) (nto_profile_calculated_total{profile!~\"openshift\",profile!~\"openshift-control-plane\",profile!~\"openshift-node\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000136857,\n",
       "       'lastEvaluation': '2023-05-29T21:24:30.345580687Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000487717,\n",
       "     'lastEvaluation': '2023-05-29T21:24:30.345231871Z'},\n",
       "    {'name': 'SamplesOperator',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-samples-operator-samples-operator-alerts-c3c34a70-5223-4f31-98fe-316ecbedb58d.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'SamplesRetriesMissingOnImagestreamImportFailing',\n",
       "       'query': 'sum(openshift_samples_failed_imagestream_import_info) > sum(openshift_samples_retry_imagestream_import_total) - sum(openshift_samples_retry_imagestream_import_total offset 30m)',\n",
       "       'duration': 7200,\n",
       "       'labels': {'namespace': 'openshift-cluster-samples-operator',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'Samples operator is detecting problems with imagestream image imports, and the periodic retries of those\\nimports are not occurring.  Contact support.  You can look at the \"openshift-samples\" ClusterOperator object\\nfor details. Most likely there are issues with the external image registry hosting the images that need to\\nbe investigated.  The list of ImageStreams that have failing imports are:\\n{{ range query \"openshift_samples_failed_imagestream_import_info > 0\" }}\\n  {{ .Labels.name }}\\n{{ end }}\\nHowever, the list of ImageStreams for which samples operator is retrying imports is:\\nretrying imports:\\n{{ range query \"openshift_samples_retry_imagestream_import_total > 0\" }}\\n   {{ .Labels.imagestreamname }}\\n{{ end }}\\n',\n",
       "        'summary': 'Samples operator is having problems with imagestream imports and its retries.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000575884,\n",
       "       'lastEvaluation': '2023-05-29T21:24:29.998977528Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SamplesImagestreamImportFailing',\n",
       "       'query': 'sum(openshift_samples_retry_imagestream_import_total) - sum(openshift_samples_retry_imagestream_import_total offset 30m) > sum(openshift_samples_failed_imagestream_import_info)',\n",
       "       'duration': 7200,\n",
       "       'labels': {'namespace': 'openshift-cluster-samples-operator',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'Samples operator is detecting problems with imagestream image imports.  You can look at the \"openshift-samples\"\\nClusterOperator object for details. Most likely there are issues with the external image registry hosting\\nthe images that needs to be investigated.  Or you can consider marking samples operator Removed if you do not\\ncare about having sample imagestreams available.  The list of ImageStreams for which samples operator is\\nretrying imports:\\n{{ range query \"openshift_samples_retry_imagestream_import_total > 0\" }}\\n   {{ .Labels.imagestreamname }}\\n{{ end }}\\n',\n",
       "        'summary': 'Samples operator is detecting problems with imagestream image imports'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000319661,\n",
       "       'lastEvaluation': '2023-05-29T21:24:29.999555206Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SamplesDegraded',\n",
       "       'query': 'openshift_samples_degraded_info == 1',\n",
       "       'duration': 7200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Samples could not be deployed and the operator is degraded. Review the \"openshift-samples\" ClusterOperator object for further details.\\n',\n",
       "        'summary': 'Samples operator is degraded.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.4168e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:29.999876089Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SamplesInvalidConfig',\n",
       "       'query': 'openshift_samples_invalidconfig_info == 1',\n",
       "       'duration': 7200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Samples operator has been given an invalid configuration.\\n',\n",
       "        'summary': 'Samples operator Invalid configuration'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 4.5846e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:29.999971139Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SamplesMissingSecret',\n",
       "       'query': 'openshift_samples_invalidsecret_info{reason=\"missing_secret\"} == 1',\n",
       "       'duration': 7200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Samples operator cannot find the samples pull secret in the openshift namespace.\\n',\n",
       "        'summary': 'Samples operator is not able to find secret'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 5.4202e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:30.000017696Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SamplesMissingTBRCredential',\n",
       "       'query': 'openshift_samples_invalidsecret_info{reason=\"missing_tbr_credential\"} == 1',\n",
       "       'duration': 7200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"The samples operator cannot find credentials for 'registry.redhat.io'. Many of the sample ImageStreams will fail to import unless the 'samplesRegistry' in the operator configuration is changed.\\n\",\n",
       "        'summary': 'Samples operator is not able to find the credentials for registry'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 5.0245e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:30.000072599Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SamplesTBRInaccessibleOnBoot',\n",
       "       'query': 'openshift_samples_tbr_inaccessible_info == 1',\n",
       "       'duration': 172800,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': \"One of two situations has occurred.  Either\\nsamples operator could not access 'registry.redhat.io' during its initial installation and it bootstrapped as removed.\\nIf this is expected, and stems from installing in a restricted network environment, please note that if you\\nplan on mirroring images associated with sample imagestreams into a registry available in your restricted\\nnetwork environment, and subsequently moving samples operator back to 'Managed' state, a list of the images\\nassociated with each image stream tag from the samples catalog is\\nprovided in the 'imagestreamtag-to-image' config map in the 'openshift-cluster-samples-operator' namespace to\\nassist the mirroring process.\\nOr, the use of allowed registries or blocked registries with global imagestream configuration will not allow\\nsamples operator to create imagestreams using the default image registry 'registry.redhat.io'.\\n\",\n",
       "        'summary': 'Samples operator is not able to access the registry on boot'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 4.0696e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:30.000123575Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001196402,\n",
       "     'lastEvaluation': '2023-05-29T21:24:29.998969573Z'},\n",
       "    {'name': 'default-storage-classes.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-storage-operator-prometheus-4712ea66-8d0a-4407-a4b8-468656ce045d.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MultipleDefaultStorageClasses',\n",
       "       'query': 'max_over_time(default_storage_class_count[5m]) > 1',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Cluster storage operator monitors all storage classes configured in the cluster\\nand checks there is not more than one default StorageClass configured.\\n',\n",
       "        'message': 'StorageClass count check is failing (there should not be more than one default StorageClass)',\n",
       "        'summary': 'More than one default StorageClass detected.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000319803,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.149972189Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000341313,\n",
       "     'lastEvaluation': '2023-05-29T21:24:34.149953965Z'},\n",
       "    {'name': 'storage-operations.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-storage-operator-prometheus-4712ea66-8d0a-4407-a4b8-468656ce045d.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'PodStartupStorageOperationsFailing',\n",
       "       'query': 'increase(storage_operation_duration_seconds_count{operation_name=~\"volume_attach|volume_mount\",status!=\"success\"}[5m]) > 0 and on () increase(storage_operation_duration_seconds_count{operation_name=~\"volume_attach|volume_mount\",status=\"success\"}[5m]) == 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'Failing storage operation \"{{ $labels.operation_name }}\" of volume plugin {{ $labels.volume_plugin }} was preventing Pods on node {{ $labels.node }}\\nfrom starting for past 5 minutes.\\nPlease investigate Pods that are \"ContainerCreating\" on the node: \"oc get pod --field-selector=spec.nodeName=ip-10-0-130-168.ec2.internal --all-namespaces | grep ContainerCreating\".\\nEvents of the Pods should contain exact error message: \"oc describe pod -n <pod namespace> <pod name>\".\\n',\n",
       "        'summary': \"Pods can't start because {{ $labels.operation_name }} of volume plugin {{ $labels.volume_plugin }} is permanently failing on node {{ $labels.node }}.\"},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000899173,\n",
       "       'lastEvaluation': '2023-05-29T21:24:26.410221529Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000911235,\n",
       "     'lastEvaluation': '2023-05-29T21:24:26.410212873Z'},\n",
       "    {'name': 'cluster-operators',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-version-cluster-version-operator-53171cf8-1870-490a-a7f9-877ce81fffba.yaml',\n",
       "     'rules': [{'state': 'firing',\n",
       "       'name': 'ClusterNotUpgradeable',\n",
       "       'query': 'max by (namespace, name, condition, endpoint) (cluster_operator_conditions{condition=\"Upgradeable\",endpoint=\"metrics\",name=\"version\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'In most cases, you will still be able to apply patch releases. Reason {{ with $cluster_operator_conditions := \"cluster_operator_conditions\" | query}}{{range $value := .}}{{if and (eq (label \"name\" $value) \"version\") (eq (label \"condition\" $value) \"Upgradeable\") (eq (label \"endpoint\" $value) \"metrics\") (eq (value $value) 0.0) (ne (len (label \"reason\" $value)) 0) }}{{label \"reason\" $value}}.{{end}}{{end}}{{end}} For more information refer to \\'oc adm upgrade\\'{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}.',\n",
       "        'summary': 'One or more cluster operators have been blocking minor version cluster upgrades for at least an hour.'},\n",
       "       'alerts': [{'labels': {'alertname': 'ClusterNotUpgradeable',\n",
       "          'condition': 'Upgradeable',\n",
       "          'endpoint': 'metrics',\n",
       "          'name': 'version',\n",
       "          'namespace': 'openshift-cluster-version',\n",
       "          'severity': 'info'},\n",
       "         'annotations': {'description': \"In most cases, you will still be able to apply patch releases. Reason MultipleReasons. For more information refer to 'oc adm upgrade' or https://console-openshift-console.apps.sharedocp4upi412ovn.lab.upshift.rdu2.redhat.com/settings/cluster/.\",\n",
       "          'summary': 'One or more cluster operators have been blocking minor version cluster upgrades for at least an hour.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2023-05-25T14:36:58.427788766Z',\n",
       "         'value': '0e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003004516,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.429408891Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ClusterOperatorDown',\n",
       "       'query': 'max by (namespace, name) (cluster_operator_up{job=\"cluster-version-operator\"} == 0)',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'The {{ $labels.name }} operator may be down or disabled, and the components it manages may be unavailable or degraded.  Cluster upgrades may not complete. For more information refer to \\'oc get -o yaml clusteroperator {{ $labels.name }}\\'{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}.',\n",
       "        'summary': 'Cluster operator has not been available for 10 minutes.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00041426,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.432422013Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ClusterOperatorDegraded',\n",
       "       'query': 'max by (namespace, name, reason) ((cluster_operator_conditions{condition=\"Degraded\",job=\"cluster-version-operator\"} or on (namespace, name) group by (namespace, name) (cluster_operator_up{job=\"cluster-version-operator\"})) == 1)',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The {{ $labels.name }} operator is degraded because {{ $labels.reason }}, and the components it manages may have reduced quality of service.  Cluster upgrades may not complete. For more information refer to \\'oc get -o yaml clusteroperator {{ $labels.name }}\\'{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}.',\n",
       "        'summary': 'Cluster operator has been degraded for 30 minutes.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000629085,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.432838447Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ClusterOperatorFlapping',\n",
       "       'query': 'max by (namespace, name) (changes(cluster_operator_up{job=\"cluster-version-operator\"}[2m]) > 2)',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The  {{ $labels.name }} operator behavior might cause upgrades to be unstable. For more information refer to \\'oc get -o yaml clusteroperator {{ $labels.name }}\\'{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}.',\n",
       "        'summary': 'Cluster operator up status is changing often.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000369686,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.433469475Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.004447884,\n",
       "     'lastEvaluation': '2023-05-29T21:24:28.429393622Z'},\n",
       "    {'name': 'cluster-version',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-version-cluster-version-operator-53171cf8-1870-490a-a7f9-877ce81fffba.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ClusterVersionOperatorDown',\n",
       "       'query': 'absent(up{job=\"cluster-version-operator\"} == 1)',\n",
       "       'duration': 600,\n",
       "       'labels': {'namespace': 'openshift-cluster-version',\n",
       "        'severity': 'critical'},\n",
       "       'annotations': {'description': 'The operator may be down or disabled. The cluster will not be kept up to date and upgrades will not be possible. Inspect the openshift-cluster-version namespace for events or changes to the cluster-version-operator deployment or pods to diagnose and repair. {{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} For more information refer to {{ label \"url\" (first $console_url ) }}/k8s/cluster/projects/openshift-cluster-version.{{ end }}{{ end }}',\n",
       "        'summary': 'Cluster version operator has disappeared from Prometheus target discovery.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000208823,\n",
       "       'lastEvaluation': '2023-05-29T21:24:31.000308188Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CannotRetrieveUpdates',\n",
       "       'query': 'max by (namespace) ((time() - cluster_version_operator_update_retrieval_timestamp_seconds) >= 3600 and ignoring (condition, name, reason) (cluster_operator_conditions{condition=\"RetrievedUpdates\",endpoint=\"metrics\",name=\"version\",reason!=\"NoChannel\"}))',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Failure to retrieve updates means that cluster administrators will need to monitor for available updates on their own or risk falling behind on security or other bugfixes. If the failure is expected, you can clear spec.channel in the ClusterVersion object to tell the cluster-version operator to not retrieve updates. Failure reason {{ with $cluster_operator_conditions := \"cluster_operator_conditions\" | query}}{{range $value := .}}{{if and (eq (label \"name\" $value) \"version\") (eq (label \"condition\" $value) \"RetrievedUpdates\") (eq (label \"endpoint\" $value) \"metrics\") (eq (value $value) 0.0)}}{{label \"reason\" $value}} {{end}}{{end}}{{end}}. {{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} For more information refer to {{ label \"url\" (first $console_url ) }}/settings/cluster/.{{ end }}{{ end }}',\n",
       "        'summary': 'Cluster version operator has not retrieved updates in {{ $value | humanizeDuration }}.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000195278,\n",
       "       'lastEvaluation': '2023-05-29T21:24:31.000518284Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'firing',\n",
       "       'name': 'UpdateAvailable',\n",
       "       'query': 'sum by (channel, namespace, upstream) (cluster_version_available_updates) > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'For more information refer to \\'oc adm upgrade\\'{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}.',\n",
       "        'summary': 'Your upstream update recommendation service recommends you update your cluster.'},\n",
       "       'alerts': [{'labels': {'alertname': 'UpdateAvailable',\n",
       "          'channel': 'stable-4.12',\n",
       "          'namespace': 'openshift-cluster-version',\n",
       "          'severity': 'info',\n",
       "          'upstream': '<default>'},\n",
       "         'annotations': {'description': \"For more information refer to 'oc adm upgrade' or https://console-openshift-console.apps.sharedocp4upi412ovn.lab.upshift.rdu2.redhat.com/settings/cluster/.\",\n",
       "          'summary': 'Your upstream update recommendation service recommends you update your cluster.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2023-05-25T14:37:00.999482434Z',\n",
       "         'value': '5e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000353266,\n",
       "       'lastEvaluation': '2023-05-29T21:24:31.000714473Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000769359,\n",
       "     'lastEvaluation': '2023-05-29T21:24:31.000300414Z'},\n",
       "    {'name': 'openshift-dns.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-dns-operator-dns-85774427-368e-426d-8005-40a99a36c41c.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'CoreDNSPanicking',\n",
       "       'query': 'increase(coredns_panics_total[10m]) > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $value }} CoreDNS panics observed on {{ $labels.instance }}',\n",
       "        'summary': 'CoreDNS panic'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000320013,\n",
       "       'lastEvaluation': '2023-05-29T21:24:15.796414518Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CoreDNSHealthCheckSlow',\n",
       "       'query': 'histogram_quantile(0.95, sum by (instance, le) (rate(coredns_health_request_duration_seconds_bucket[5m]))) > 10',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'CoreDNS Health Checks are slowing down (instance {{ $labels.instance }})',\n",
       "        'summary': 'CoreDNS health checks'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000520669,\n",
       "       'lastEvaluation': '2023-05-29T21:24:15.796735724Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CoreDNSErrorsHigh',\n",
       "       'query': '(sum by (namespace) (rate(coredns_dns_responses_total{rcode=\"SERVFAIL\"}[5m])) / sum by (namespace) (rate(coredns_dns_responses_total[5m]))) > 0.01',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage }} of requests.',\n",
       "        'summary': 'CoreDNS serverfail'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000205698,\n",
       "       'lastEvaluation': '2023-05-29T21:24:15.797257555Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001061688,\n",
       "     'lastEvaluation': '2023-05-29T21:24:15.796403418Z'},\n",
       "    {'name': 'etcd',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-etcd-operator-etcd-prometheus-rules-57d2b98b-83a1-47c3-a6aa-81dcd0a26f8f.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'etcdMembersDown',\n",
       "       'query': 'max without (endpoint) (sum without (instance) (up{job=~\".*etcd.*\"} == bool 0) or count without (To) (sum without (instance) (rate(etcd_network_peer_sent_failures_total{job=~\".*etcd.*\"}[2m])) > 0.01)) > 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": members are down ({{ $value }}).',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdMembersDown.md',\n",
       "        'summary': 'etcd cluster members are down.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000612283,\n",
       "       'lastEvaluation': '2023-05-29T21:24:20.063701079Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdNoLeader',\n",
       "       'query': 'etcd_server_has_leader{job=~\".*etcd.*\"} == 0',\n",
       "       'duration': 60,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": member {{ $labels.instance }} has no leader.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdNoLeader.md',\n",
       "        'summary': 'etcd cluster has no leader.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000110438,\n",
       "       'lastEvaluation': '2023-05-29T21:24:20.064315596Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdMemberCommunicationSlow',\n",
       "       'query': 'histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.15',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": member communication with {{ $labels.To }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}.',\n",
       "        'summary': 'etcd cluster member communication is slow.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001007065,\n",
       "       'lastEvaluation': '2023-05-29T21:24:20.064428579Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighNumberOfFailedProposals',\n",
       "       'query': 'rate(etcd_server_proposals_failed_total{job=~\".*etcd.*\"}[15m]) > 5',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": {{ $value }} proposal failures within the last 30 minutes on etcd instance {{ $labels.instance }}.',\n",
       "        'summary': 'etcd cluster has high number of proposal failures.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000131097,\n",
       "       'lastEvaluation': '2023-05-29T21:24:20.065436866Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighFsyncDurations',\n",
       "       'query': 'histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.5',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": 99th percentile fsync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.',\n",
       "        'summary': 'etcd cluster 99th percentile fsync durations are too high.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000473442,\n",
       "       'lastEvaluation': '2023-05-29T21:24:20.065569175Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighFsyncDurations',\n",
       "       'query': 'histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 1',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": 99th percentile fsync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdHighFsyncDurations.md',\n",
       "        'summary': 'etcd cluster 99th percentile fsync durations are too high.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000439678,\n",
       "       'lastEvaluation': '2023-05-29T21:24:20.066043809Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighCommitDurations',\n",
       "       'query': 'histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.25',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": 99th percentile commit durations {{ $value }}s on etcd instance {{ $labels.instance }}.',\n",
       "        'summary': 'etcd cluster 99th percentile commit durations are too high.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000473361,\n",
       "       'lastEvaluation': '2023-05-29T21:24:20.066484739Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdDatabaseQuotaLowSpace',\n",
       "       'query': '(last_over_time(etcd_mvcc_db_total_size_in_bytes[5m]) / last_over_time(etcd_server_quota_backend_bytes[5m])) * 100 > 95',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": database size exceeds the defined quota on etcd instance {{ $labels.instance }}, please defrag or increase the quota as the writes to etcd will be disabled when it is full.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdDatabaseQuotaLowSpace.md',\n",
       "        'summary': 'etcd cluster database is running full.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000174838,\n",
       "       'lastEvaluation': '2023-05-29T21:24:20.066959764Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdExcessiveDatabaseGrowth',\n",
       "       'query': 'predict_linear(etcd_mvcc_db_total_size_in_bytes[4h], 4 * 60 * 60) > etcd_server_quota_backend_bytes',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": Predicting running out of disk space in the next four hours, based on write observations within the past four hours on etcd instance {{ $labels.instance }}, please check as it might be disruptive.',\n",
       "        'summary': 'etcd cluster database growing very fast.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000397018,\n",
       "       'lastEvaluation': '2023-05-29T21:24:20.067135584Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdDatabaseHighFragmentationRatio',\n",
       "       'query': '(last_over_time(etcd_mvcc_db_total_size_in_use_in_bytes[5m]) / last_over_time(etcd_mvcc_db_total_size_in_bytes[5m])) < 0.5',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": database size in use on instance {{ $labels.instance }} is {{ $value | humanizePercentage }} of the actual allocated disk space, please run defragmentation (e.g. etcdctl defrag) to retrieve the unused fragmented disk space.',\n",
       "        'runbook_url': 'https://etcd.io/docs/v3.5/op-guide/maintenance/#defragmentation',\n",
       "        'summary': 'etcd database size in use is less than 50% of the actual allocated storage.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000163527,\n",
       "       'lastEvaluation': '2023-05-29T21:24:20.067533654Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.004007484,\n",
       "     'lastEvaluation': '2023-05-29T21:24:20.063692152Z'},\n",
       "    {'name': 'openshift-etcd.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-etcd-operator-etcd-prometheus-rules-57d2b98b-83a1-47c3-a6aa-81dcd0a26f8f.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'etcdGRPCRequestsSlow',\n",
       "       'query': 'histogram_quantile(0.99, sum without (grpc_type) (rate(grpc_server_handling_seconds_bucket{grpc_method!=\"Defragment\",grpc_type=\"unary\",job=\"etcd\"}[10m]))) > 1',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": 99th percentile of gRPC requests is {{ $value }}s on etcd instance {{ $labels.instance }} for {{ $labels.grpc_method }} method.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdGRPCRequestsSlow.md',\n",
       "        'summary': 'etcd grpc requests are slow'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.012944118,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.750193876Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighNumberOfFailedGRPCRequests',\n",
       "       'query': '(sum without (grpc_type, grpc_code) (rate(grpc_server_handled_total{grpc_code=~\"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\",job=\"etcd\"}[5m])) / (sum without (grpc_type, grpc_code) (rate(grpc_server_handled_total{job=\"etcd\"}[5m])) > 2 and on () (sum(cluster_infrastructure_provider{type!~\"ipi|BareMetal\"} == bool 1)))) * 100 > 10',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": {{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.',\n",
       "        'summary': 'etcd cluster has high number of failed grpc requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.018806654,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.763140658Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighNumberOfFailedGRPCRequests',\n",
       "       'query': '(sum without (grpc_type, grpc_code) (rate(grpc_server_handled_total{grpc_code=~\"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\",job=\"etcd\"}[5m])) / (sum without (grpc_type, grpc_code) (rate(grpc_server_handled_total{job=\"etcd\"}[5m])) > 2 and on () (sum(cluster_infrastructure_provider{type!~\"ipi|BareMetal\"} == bool 1)))) * 100 > 50',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": {{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdHighNumberOfFailedGRPCRequests.md',\n",
       "        'summary': 'etcd cluster has high number of failed grpc requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.024914701,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.781949437Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighNumberOfLeaderChanges',\n",
       "       'query': 'avg(changes(etcd_server_is_leader[10m])) > 5',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": {{ $value }} average leader changes within the last 10 minutes. Frequent elections may be a sign of insufficient resources, high network latency, or disruptions by other components and should be investigated.',\n",
       "        'summary': 'etcd cluster has high number of leader changes.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000217719,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.806867545Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdInsufficientMembers',\n",
       "       'query': 'sum without (instance, pod) (up{job=\"etcd\"} == bool 1 and etcd_server_has_leader{job=\"etcd\"} == bool 1) < ((count without (instance, pod) (up{job=\"etcd\"}) + 1) / 2)',\n",
       "       'duration': 180,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd is reporting fewer instances are available than are needed ({{ $value }}). When etcd does not have a majority of instances available the Kubernetes and OpenShift APIs will reject read and write requests and operations that preserve the health of workloads cannot be performed. This can occur when multiple control plane nodes are powered off or are unable to connect to each other via the network. Check that all control plane nodes are powered on and that network connections between each machine are functional.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdInsufficientMembers.md',\n",
       "        'summary': 'etcd is reporting that a majority of instances are unavailable.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000326686,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.807086857Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.057230276,\n",
       "     'lastEvaluation': '2023-05-29T21:24:41.750185561Z'},\n",
       "    {'name': 'GitOpsOperatorArgoCD',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-gitops-gitops-operator-argocd-alerts-133e6157-1fad-418f-84c1-35ab4ec79e1b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ArgoCDSyncAlert',\n",
       "       'query': 'argocd_app_info{namespace=\"openshift-gitops\",sync_status=\"OutOfSync\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'message': 'ArgoCD application {{ $labels.name }} is out of sync'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000207561,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.907224055Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000219053,\n",
       "     'lastEvaluation': '2023-05-29T21:24:18.907214958Z'},\n",
       "    {'name': 'imageregistry.operations.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-image-registry-image-registry-rules-fa7307d0-f6eb-4b3e-bda2-63426a1c3102.yaml',\n",
       "     'rules': [{'name': 'imageregistry:operations_count:sum',\n",
       "       'query': 'label_replace(label_replace(sum by (operation) (imageregistry_request_duration_seconds_count{operation=\"BlobStore.ServeBlob\"}), \"operation\", \"get\", \"operation\", \"(.+)\"), \"resource_type\", \"blob\", \"resource_type\", \"\")',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00074908,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.229359566Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'imageregistry:operations_count:sum',\n",
       "       'query': 'label_replace(label_replace(sum by (operation) (imageregistry_request_duration_seconds_count{operation=\"BlobStore.Create\"}), \"operation\", \"create\", \"operation\", \"(.+)\"), \"resource_type\", \"blob\", \"resource_type\", \"\")',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000225916,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.230112313Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'imageregistry:operations_count:sum',\n",
       "       'query': 'label_replace(label_replace(sum by (operation) (imageregistry_request_duration_seconds_count{operation=\"ManifestService.Get\"}), \"operation\", \"get\", \"operation\", \"(.+)\"), \"resource_type\", \"manifest\", \"resource_type\", \"\")',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000159119,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.230339491Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'imageregistry:operations_count:sum',\n",
       "       'query': 'label_replace(label_replace(sum by (operation) (imageregistry_request_duration_seconds_count{operation=\"ManifestService.Put\"}), \"operation\", \"create\", \"operation\", \"(.+)\"), \"resource_type\", \"manifest\", \"resource_type\", \"\")',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000128001,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.230500614Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001284719,\n",
       "     'lastEvaluation': '2023-05-29T21:24:39.229346772Z'},\n",
       "    {'name': 'imagestreams.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-image-registry-imagestreams-rules-a38b4e93-c0b2-4b73-9bb3-3fe679c6c5e3.yaml',\n",
       "     'rules': [{'name': 'imageregistry:imagestreamtags_count:sum',\n",
       "       'query': 'sum by (location, source) (image_registry_image_stream_tags_total)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000370508,\n",
       "       'lastEvaluation': '2023-05-29T21:24:35.950712295Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000383843,\n",
       "     'lastEvaluation': '2023-05-29T21:24:35.950702126Z'},\n",
       "    {'name': 'openshift-ingress-to-route-controller.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-ingress-operator-ingress-operator-8738376c-5277-4781-aa4b-791bb61e4cb9.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'IngressWithoutClassName',\n",
       "       'query': 'openshift_ingress_to_route_controller_ingress_without_class_name == 1',\n",
       "       'duration': 86400,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'This alert fires when there is an Ingress with an unset IngressClassName for longer than one day.',\n",
       "        'message': 'Ingress {{ $labels.name }} is missing the IngressClassName for 1 day.',\n",
       "        'summary': 'Ingress without IngressClassName for 1 day'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000337886,\n",
       "       'lastEvaluation': '2023-05-29T21:24:43.347779264Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'UnmanagedRoutes',\n",
       "       'query': 'openshift_ingress_to_route_controller_route_with_unmanaged_owner == 1',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'This alert fires when there is a Route owned by an unmanaged Ingress.',\n",
       "        'message': 'Route {{ $labels.name }} is owned by an unmanaged Ingress.',\n",
       "        'summary': 'Route owned by an Ingress no longer managed'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000223631,\n",
       "       'lastEvaluation': '2023-05-29T21:24:43.348119696Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000577147,\n",
       "     'lastEvaluation': '2023-05-29T21:24:43.347768454Z'},\n",
       "    {'name': 'openshift-ingress.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-ingress-operator-ingress-operator-8738376c-5277-4781-aa4b-791bb61e4cb9.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'HAProxyReloadFail',\n",
       "       'query': 'template_router_reload_failure == 1',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'This alert fires when HAProxy fails to reload its configuration, which will result in the router not picking up recently created or modified routes.',\n",
       "        'message': 'HAProxy reloads are failing on {{ $labels.pod }}. Router is not respecting recently created or modified routes',\n",
       "        'summary': 'HAProxy reload failure'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000201359,\n",
       "       'lastEvaluation': '2023-05-29T21:24:19.537136974Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'HAProxyDown',\n",
       "       'query': 'haproxy_up == 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'This alert fires when metrics report that HAProxy is down.',\n",
       "        'message': 'HAProxy metrics are reporting that HAProxy is down on pod {{ $labels.namespace }} / {{ $labels.pod }}',\n",
       "        'summary': 'HAProxy is down'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 5.3481e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:19.537339755Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'IngressControllerDegraded',\n",
       "       'query': 'ingress_controller_conditions{condition=\"Degraded\"} == 1',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'This alert fires when the IngressController status is degraded.',\n",
       "        'message': 'The {{ $labels.namespace }}/{{ $labels.name }} ingresscontroller is\\ndegraded: {{ $labels.reason }}.\\n',\n",
       "        'summary': 'IngressController is degraded'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 5.1567e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:19.537393968Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'IngressControllerUnavailable',\n",
       "       'query': 'ingress_controller_conditions{condition=\"Available\"} == 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'This alert fires when the IngressController is not available.',\n",
       "        'message': 'The {{ $labels.namespace }}/{{ $labels.name }} ingresscontroller is\\nunavailable: {{ $labels.reason }}.\\n',\n",
       "        'summary': 'IngressController is unavailable'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 4.246e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:19.537446236Z',\n",
       "       'type': 'alerting'},\n",
       "      {'name': 'cluster:route_metrics_controller_routes_per_shard:min',\n",
       "       'query': 'min(route_metrics_controller_routes_per_shard)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 8.5181e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:19.537489477Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:route_metrics_controller_routes_per_shard:max',\n",
       "       'query': 'max(route_metrics_controller_routes_per_shard)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 4.5565e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:19.53757561Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:route_metrics_controller_routes_per_shard:avg',\n",
       "       'query': 'avg(route_metrics_controller_routes_per_shard)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 3.8894e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:19.537621977Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:route_metrics_controller_routes_per_shard:median',\n",
       "       'query': 'quantile(0.5, route_metrics_controller_routes_per_shard)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 6.905e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:19.537661652Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:openshift_route_info:tls_termination:sum',\n",
       "       'query': 'sum by (tls_termination) (openshift_route_info)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000134934,\n",
       "       'lastEvaluation': '2023-05-29T21:24:19.537731753Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000740223,\n",
       "     'lastEvaluation': '2023-05-29T21:24:19.537128158Z'},\n",
       "    {'name': 'insights',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-insights-insights-prometheus-rules-c7713bff-efd7-4128-9475-f79d2b6c0cfa.yaml',\n",
       "     'rules': [{'state': 'firing',\n",
       "       'name': 'InsightsDisabled',\n",
       "       'query': 'max without (job, pod, service, instance) (cluster_operator_conditions{condition=\"Disabled\",name=\"insights\"} == 1)',\n",
       "       'duration': 300,\n",
       "       'labels': {'namespace': 'openshift-insights', 'severity': 'info'},\n",
       "       'annotations': {'description': 'Insights operator is disabled. In order to enable Insights and benefit from recommendations specific to your cluster, please follow steps listed in the documentation: https://docs.openshift.com/container-platform/latest/support/remote_health_monitoring/enabling-remote-health-reporting.html',\n",
       "        'summary': 'Insights operator is disabled.'},\n",
       "       'alerts': [{'labels': {'alertname': 'InsightsDisabled',\n",
       "          'condition': 'Disabled',\n",
       "          'endpoint': 'metrics',\n",
       "          'name': 'insights',\n",
       "          'namespace': 'openshift-insights',\n",
       "          'reason': 'NoToken',\n",
       "          'severity': 'info'},\n",
       "         'annotations': {'description': 'Insights operator is disabled. In order to enable Insights and benefit from recommendations specific to your cluster, please follow steps listed in the documentation: https://docs.openshift.com/container-platform/latest/support/remote_health_monitoring/enabling-remote-health-reporting.html',\n",
       "          'summary': 'Insights operator is disabled.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2023-05-25T14:37:12.592844939Z',\n",
       "         'value': '1e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000661695,\n",
       "       'lastEvaluation': '2023-05-29T21:24:42.594095318Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SimpleContentAccessNotAvailable',\n",
       "       'query': 'max without (job, pod, service, instance) (max_over_time(cluster_operator_conditions{condition=\"SCAAvailable\",name=\"insights\",reason=\"NotFound\"}[5m]) == 0)',\n",
       "       'duration': 300,\n",
       "       'labels': {'namespace': 'openshift-insights', 'severity': 'info'},\n",
       "       'annotations': {'description': 'Simple content access (SCA) is not enabled. Once enabled, Insights Operator can automatically import the SCA certificates from Red Hat OpenShift Cluster Manager making it easier to use the content provided by your Red Hat subscriptions when creating container images. See https://docs.openshift.com/container-platform/latest/cicd/builds/running-entitled-builds.html for more information.',\n",
       "        'summary': 'Simple content access certificates are not available.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000211549,\n",
       "       'lastEvaluation': '2023-05-29T21:24:42.594760189Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'InsightsRecommendationActive',\n",
       "       'query': 'insights_recommendation_active == 1',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'Insights recommendation \"{{ $labels.description }}\" with total risk \"{{ $labels.total_risk }}\" was detected on the cluster. More information is available at {{ $labels.info_link }}.',\n",
       "        'summary': 'An Insights recommendation is active for this cluster.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 5.5815e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:42.594974934Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000946161,\n",
       "     'lastEvaluation': '2023-05-29T21:24:42.594087042Z'},\n",
       "    {'name': 'pre-release-lifecycle',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-api-usage-51998373-17f7-433f-bc96-f703a2343099.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'APIRemovedInNextReleaseInUse',\n",
       "       'query': 'group by (group, version, resource, removed_release) (apiserver_requested_deprecated_apis{removed_release=\"1.26\"}) * on (group, version, resource) group_left () sum by (group, version, resource) (rate(apiserver_request_total{system_client!=\"cluster-policy-controller\",system_client!=\"kube-controller-manager\"}[4h])) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver', 'severity': 'info'},\n",
       "       'annotations': {'description': 'Deprecated API that will be removed in the next version is being used. Removing the workload that is using the {{ $labels.group }}.{{ $labels.version }}/{{ $labels.resource }} API might be necessary for a successful upgrade to the next cluster version with Kubernetes {{ $labels.removed_release }}. Refer to `oc get apirequestcounts {{ $labels.resource }}.{{ $labels.version }}.{{ $labels.group }} -o yaml` to identify the workload.',\n",
       "        'summary': 'Deprecated API that will be removed in the next version is being used.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.139858735,\n",
       "       'lastEvaluation': '2023-05-29T21:24:26.615867686Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'APIRemovedInNextEUSReleaseInUse',\n",
       "       'query': 'group by (group, version, resource, removed_release) (apiserver_requested_deprecated_apis{removed_release=~\"1[.]2[67]\"}) * on (group, version, resource) group_left () sum by (group, version, resource) (rate(apiserver_request_total{system_client!=\"cluster-policy-controller\",system_client!=\"kube-controller-manager\"}[4h])) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver', 'severity': 'info'},\n",
       "       'annotations': {'description': 'Deprecated API that will be removed in the next EUS version is being used. Removing the workload that is using the {{ $labels.group }}.{{ $labels.version }}/{{ $labels.resource }} API might be necessary for a successful upgrade to the next EUS cluster version with Kubernetes {{ $labels.removed_release }}. Refer to `oc get apirequestcounts {{ $labels.resource }}.{{ $labels.version }}.{{ $labels.group }} -o yaml` to identify the workload.',\n",
       "        'summary': 'Deprecated API that will be removed in the next EUS version is being used.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.157002958,\n",
       "       'lastEvaluation': '2023-05-29T21:24:26.755728484Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.296875437,\n",
       "     'lastEvaluation': '2023-05-29T21:24:26.615860944Z'},\n",
       "    {'name': 'apiserver-audit',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-audit-errors-68b541f2-34b4-42e1-b5cc-19a61563e505.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'AuditLogError',\n",
       "       'query': 'sum by (apiserver, instance) (rate(apiserver_audit_error_total{apiserver=~\".+-apiserver\"}[5m])) / sum by (apiserver, instance) (rate(apiserver_audit_event_total{apiserver=~\".+-apiserver\"}[5m])) > 0',\n",
       "       'duration': 60,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'An API Server had an error writing to an audit log.',\n",
       "        'summary': 'An API Server instance was unable to write audit logs. This could be\\ntriggered by the node running out of space, or a malicious actor\\ntampering with the audit logs.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00054762,\n",
       "       'lastEvaluation': '2023-05-29T21:24:15.789250793Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000562038,\n",
       "     'lastEvaluation': '2023-05-29T21:24:15.789239181Z'},\n",
       "    {'name': 'control-plane-cpu-utilization',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-cpu-utilization-f4a0d2b1-5265-4bae-8a01-191a65ed3e8c.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'HighOverallControlPlaneCPU',\n",
       "       'query': 'sum(100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])) * 100) and on (instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\")) / count(kube_node_role{role=\"master\"}) > 60',\n",
       "       'duration': 600,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'Given three control plane nodes, the overall CPU utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the remaining two must handle the load of the cluster in order to be HA. If the cluster is using more than 2/3 of all capacity, if one control plane node fails, the remaining two are likely to fail when they take the load. To fix this, increase the CPU and memory on your control plane nodes.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/ExtremelyHighIndividualControlPlaneCPU.md',\n",
       "        'summary': 'CPU utilization across all three control plane nodes is higher than two control plane nodes can sustain; a single control plane node outage may cause a cascading failure; increase available CPU.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000853977,\n",
       "       'lastEvaluation': '2023-05-29T21:24:23.62035684Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ExtremelyHighIndividualControlPlaneCPU',\n",
       "       'query': '100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])) * 100) > 90 and on (instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\")',\n",
       "       'duration': 300,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'Extreme CPU pressure can cause slow serialization and poor performance from the kube-apiserver and etcd. When this happens, there is a risk of clients seeing non-responsive API requests which are issued again causing even more CPU pressure. It can also cause failing liveness probes due to slow etcd responsiveness on the backend. If one kube-apiserver fails under this condition, chances are you will experience a cascade as the remaining kube-apiservers are also under-provisioned. To fix this, increase the CPU and memory on your control plane nodes.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/ExtremelyHighIndividualControlPlaneCPU.md',\n",
       "        'summary': 'CPU utilization on a single control plane node is very high, more CPU pressure is likely to cause a failover; increase available CPU.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000406545,\n",
       "       'lastEvaluation': '2023-05-29T21:24:23.62121239Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ExtremelyHighIndividualControlPlaneCPU',\n",
       "       'query': '100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])) * 100) > 90 and on (instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\")',\n",
       "       'duration': 3600,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'critical'},\n",
       "       'annotations': {'description': 'Extreme CPU pressure can cause slow serialization and poor performance from the kube-apiserver and etcd. When this happens, there is a risk of clients seeing non-responsive API requests which are issued again causing even more CPU pressure. It can also cause failing liveness probes due to slow etcd responsiveness on the backend. If one kube-apiserver fails under this condition, chances are you will experience a cascade as the remaining kube-apiservers are also under-provisioned. To fix this, increase the CPU and memory on your control plane nodes.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/ExtremelyHighIndividualControlPlaneCPU.md',\n",
       "        'summary': 'Sustained high CPU utilization on a single control plane node, more CPU pressure is likely to cause a failover; increase available CPU.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000707592,\n",
       "       'lastEvaluation': '2023-05-29T21:24:23.621619977Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001981841,\n",
       "     'lastEvaluation': '2023-05-29T21:24:23.620348734Z'},\n",
       "    {'name': 'apiserver-requests-in-flight',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-kube-apiserver-requests-f8cece29-6294-41ec-9a37-87ab95e77992.yaml',\n",
       "     'rules': [{'name': 'cluster:apiserver_current_inflight_requests:sum:max_over_time:2m',\n",
       "       'query': 'max_over_time(sum by (apiserver, requestKind) (apiserver_current_inflight_requests{apiserver=~\"openshift-apiserver|kube-apiserver\"})[2m:])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000500111,\n",
       "       'lastEvaluation': '2023-05-29T21:24:33.314384549Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000537513,\n",
       "     'lastEvaluation': '2023-05-29T21:24:33.314349873Z'},\n",
       "    {'name': 'kube-apiserver-slos-basic',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-kube-apiserver-slos-basic-b7e4520a-996a-4c7b-8a63-0f41af899252.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeAPIErrorBudgetBurn',\n",
       "       'query': 'sum(apiserver_request:burnrate1h) > (14.4 * 0.01) and sum(apiserver_request:burnrate5m) > (14.4 * 0.01)',\n",
       "       'duration': 120,\n",
       "       'labels': {'long': '1h',\n",
       "        'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'critical',\n",
       "        'short': '5m'},\n",
       "       'annotations': {'description': \"The API server is burning too much error budget. This alert fires when too many requests are failing with high latency. Use the 'API Performance' monitoring dashboards to narrow down the request states and latency. The 'etcd' monitoring dashboards also provides metrics to help determine etcd stability and performance.\",\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/KubeAPIErrorBudgetBurn.md',\n",
       "        'summary': 'The API server is burning too much error budget.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000497547,\n",
       "       'lastEvaluation': '2023-05-29T21:24:29.301692301Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeAPIErrorBudgetBurn',\n",
       "       'query': 'sum(apiserver_request:burnrate6h) > (6 * 0.01) and sum(apiserver_request:burnrate30m) > (6 * 0.01)',\n",
       "       'duration': 900,\n",
       "       'labels': {'long': '6h',\n",
       "        'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'critical',\n",
       "        'short': '30m'},\n",
       "       'annotations': {'description': \"The API server is burning too much error budget. This alert fires when too many requests are failing with high latency. Use the 'API Performance' monitoring dashboards to narrow down the request states and latency. The 'etcd' monitoring dashboards also provides metrics to help determine etcd stability and performance.\",\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/KubeAPIErrorBudgetBurn.md',\n",
       "        'summary': 'The API server is burning too much error budget.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000289545,\n",
       "       'lastEvaluation': '2023-05-29T21:24:29.302191501Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000820735,\n",
       "     'lastEvaluation': '2023-05-29T21:24:29.301663627Z'},\n",
       "    {'name': 'kube-apiserver.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-kube-apiserver-slos-basic-b7e4520a-996a-4c7b-8a63-0f41af899252.yaml',\n",
       "     'rules': [{'name': 'apiserver_request:burnrate5m',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[5m])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[5m])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[5m])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[5m]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[5m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[5m]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[5m]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[5m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[5m]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.064527884,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.34708001Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate30m',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[30m])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[30m]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[30m])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[30m])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[30m]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[30m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[30m]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[30m]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[30m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[30m]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[30m]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.107703941,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.411614597Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate1h',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[1h])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1h]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1h])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1h])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1h]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1h]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1h]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[1h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[1h]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1h]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.168221967,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.519324911Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate6h',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[6h])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[6h]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[6h])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[6h])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[6h]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[6h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[6h]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[6h]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[6h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[6h]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[6h]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.777279976,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.687554091Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate1h',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.023765898,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.46484002Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate30m',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.02024406,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.488609615Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate5m',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.012718112,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.508857161Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate6h',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.121733312,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.52157882Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'code_resource:apiserver_request_total:rate5m',\n",
       "       'query': 'sum by (code, resource) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.012970246,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.643319256Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'code_resource:apiserver_request_total:rate5m',\n",
       "       'query': 'sum by (code, resource) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.005323181,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.656296244Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum by (le, resource) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))) > 0',\n",
       "       'labels': {'quantile': '0.99', 'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.12766024,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.661622421Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum by (le, resource) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) > 0',\n",
       "       'labels': {'quantile': '0.99', 'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.054626204,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.789285035Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum without (instance, pod) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.105132152,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.843913663Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.9, sum without (instance, pod) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])))',\n",
       "       'labels': {'quantile': '0.9'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.105293525,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.949052988Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.5, sum without (instance, pod) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])))',\n",
       "       'labels': {'quantile': '0.5'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.109634386,\n",
       "       'lastEvaluation': '2023-05-29T21:24:19.054353346Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 1.81692251,\n",
       "     'lastEvaluation': '2023-05-29T21:24:17.347071524Z'},\n",
       "    {'name': 'kube-apiserver-slos-extended',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-kube-apiserver-slos-extended-f4825ccd-bcb8-482e-9212-c6ad3c658df3.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeAPIErrorBudgetBurn',\n",
       "       'query': 'sum(apiserver_request:burnrate1d) > (3 * 0.01) and sum(apiserver_request:burnrate2h) > (3 * 0.01)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'long': '1d',\n",
       "        'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'warning',\n",
       "        'short': '2h'},\n",
       "       'annotations': {'description': \"The API server is burning too much error budget. This alert fires when too many requests are failing with high latency. Use the 'API Performance' monitoring dashboards to narrow down the request states and latency. The 'etcd' monitoring dashboards also provides metrics to help determine etcd stability and performance.\",\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/KubeAPIErrorBudgetBurn.md',\n",
       "        'summary': 'The API server is burning too much error budget.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000522614,\n",
       "       'lastEvaluation': '2023-05-29T21:24:19.8095975Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeAPIErrorBudgetBurn',\n",
       "       'query': 'sum(apiserver_request:burnrate3d) > (1 * 0.01) and sum(apiserver_request:burnrate6h) > (1 * 0.01)',\n",
       "       'duration': 10800,\n",
       "       'labels': {'long': '3d',\n",
       "        'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'warning',\n",
       "        'short': '6h'},\n",
       "       'annotations': {'description': \"The API server is burning too much error budget. This alert fires when too many requests are failing with high latency. Use the 'API Performance' monitoring dashboards to narrow down the request states and latency. The 'etcd' monitoring dashboards also provides metrics to help determine etcd stability and performance.\",\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/KubeAPIErrorBudgetBurn.md',\n",
       "        'summary': 'The API server is burning too much error budget.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000289435,\n",
       "       'lastEvaluation': '2023-05-29T21:24:19.810121997Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000826436,\n",
       "     'lastEvaluation': '2023-05-29T21:24:19.809588473Z'},\n",
       "    {'name': 'kube-apiserver.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-kube-apiserver-slos-extended-f4825ccd-bcb8-482e-9212-c6ad3c658df3.yaml',\n",
       "     'rules': [{'name': 'apiserver_request:burnrate2h',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[2h])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[2h]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[2h])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[2h])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[2h]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[2h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[2h]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[2h]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[2h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[2h]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[2h]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.23919908,\n",
       "       'lastEvaluation': '2023-05-29T21:24:14.686566714Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate1d',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[1d])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1d]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1d])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1d])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1d]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1d]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1d]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[1d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[1d]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1d]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 1.6245475950000001,\n",
       "       'lastEvaluation': '2023-05-29T21:24:14.925772688Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate3d',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[3d])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[3d]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[3d])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[3d])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[3d]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[3d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[3d]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[3d]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[3d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[3d]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[3d]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 4.562929818,\n",
       "       'lastEvaluation': '2023-05-29T21:24:16.550327086Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate1d',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.361098576,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.113263136Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate2h',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.057535441,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.474363886Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate3d',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.957001402,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.531904035Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 7.80235347,\n",
       "     'lastEvaluation': '2023-05-29T21:24:14.686557898Z'},\n",
       "    {'name': 'cluster-version',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-operator-kube-apiserver-operator-650e6f2f-0f52-4b81-8e8c-6e6be8330501.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'TechPreviewNoUpgrade',\n",
       "       'query': 'cluster_feature_set{name!=\"\",namespace=\"openshift-kube-apiserver-operator\"} == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Cluster has enabled Technology Preview features that cannot be undone and will prevent upgrades. The TechPreviewNoUpgrade feature set is not recommended on production clusters.',\n",
       "        'summary': 'Cluster has enabled tech preview features that will prevent upgrades.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001019911,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.401193468Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001031652,\n",
       "     'lastEvaluation': '2023-05-29T21:24:17.401184642Z'},\n",
       "    {'name': 'pod-security-violation',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-podsecurity-dc8f021a-cfad-4481-afe3-4bc376fb9061.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'PodSecurityViolation',\n",
       "       'query': 'sum by (policy_level) (increase(pod_security_evaluations_total{decision=\"deny\",mode=\"audit\",resource=\"pod\"}[1d])) > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver', 'severity': 'info'},\n",
       "       'annotations': {'description': 'A workload (pod, deployment, deamonset, ...) was created somewhere in the cluster but it did not match the PodSecurity \"{{ $labels.policy_level }}\" profile defined by its namespace either via the cluster-wide configuration (which triggers on a \"restricted\" profile violations) or by the namespace local Pod Security labels. Refer to Kubernetes documentation on Pod Security Admission to learn more about these violations.',\n",
       "        'summary': \"One or more workloads users created in the cluster don't match their Pod Security profile\"},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000753858,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.727326415Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000764478,\n",
       "     'lastEvaluation': '2023-05-29T21:24:37.727317959Z'},\n",
       "    {'name': 'cluster-version',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-controller-manager-operator-kube-controller-manager-operator-688e8063-58d8-44a7-b838-dcde141a0b6f.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeControllerManagerDown',\n",
       "       'query': 'absent(up{job=\"kube-controller-manager\"} == 1)',\n",
       "       'duration': 900,\n",
       "       'labels': {'namespace': 'openshift-kube-controller-manager',\n",
       "        'severity': 'critical'},\n",
       "       'annotations': {'description': 'KubeControllerManager has disappeared from Prometheus target discovery.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-controller-manager-operator/KubeControllerManagerDown.md',\n",
       "        'summary': 'Target disappeared from Prometheus target discovery.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000250211,\n",
       "       'lastEvaluation': '2023-05-29T21:24:14.371249681Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PodDisruptionBudgetAtLimit',\n",
       "       'query': 'max by (namespace, poddisruptionbudget) (kube_poddisruptionbudget_status_current_healthy == kube_poddisruptionbudget_status_desired_healthy and on (namespace, poddisruptionbudget) kube_poddisruptionbudget_status_expected_pods > 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The pod disruption budget is at the minimum disruptions allowed level. The number of current healthy pods is equal to the desired healthy pods.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-controller-manager-operator/PodDisruptionBudgetAtLimit.md',\n",
       "        'summary': 'The pod disruption budget is preventing further disruption to pods.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000394873,\n",
       "       'lastEvaluation': '2023-05-29T21:24:14.371501145Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PodDisruptionBudgetLimit',\n",
       "       'query': 'max by (namespace, poddisruptionbudget) (kube_poddisruptionbudget_status_current_healthy < kube_poddisruptionbudget_status_desired_healthy)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'The pod disruption budget is below the minimum disruptions allowed level and is not satisfied. The number of current healthy pods is less than the desired healthy pods.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-controller-manager-operator/PodDisruptionBudgetLimit.md',\n",
       "        'summary': 'The pod disruption budget registers insufficient amount of pods.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000242387,\n",
       "       'lastEvaluation': '2023-05-29T21:24:14.371897431Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'GarbageCollectorSyncFailed',\n",
       "       'query': 'rate(garbagecollector_controller_resources_sync_error_total[5m]) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Garbage Collector had a problem with syncing and monitoring the available resources. Please see KubeControllerManager logs for more details.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-controller-manager-operator/GarbageCollectorSyncFailed.md',\n",
       "        'summary': 'There was a problem with syncing the resources for garbage collection.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.1755e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:14.372140709Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000973272,\n",
       "     'lastEvaluation': '2023-05-29T21:24:14.371240945Z'},\n",
       "    {'name': 'cluster-version',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-scheduler-operator-kube-scheduler-operator-9a16c0b3-b80d-4923-a719-31a6e920f8c7.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeSchedulerDown',\n",
       "       'query': 'absent(up{job=\"scheduler\"} == 1)',\n",
       "       'duration': 900,\n",
       "       'labels': {'namespace': 'openshift-kube-scheduler',\n",
       "        'severity': 'critical'},\n",
       "       'annotations': {'description': 'KubeScheduler has disappeared from Prometheus target discovery.',\n",
       "        'summary': 'Target disappeared from Prometheus target discovery.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000580903,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.650943368Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000613835,\n",
       "     'lastEvaluation': '2023-05-29T21:24:41.650916858Z'},\n",
       "    {'name': 'scheduler-legacy-policy-deprecated',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-scheduler-operator-kube-scheduler-operator-9a16c0b3-b80d-4923-a719-31a6e920f8c7.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'SchedulerLegacyPolicySet',\n",
       "       'query': 'cluster_legacy_scheduler_policy > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The scheduler is currently configured to use a legacy scheduler policy API. Use of the policy API is deprecated and removed in 4.10.',\n",
       "        'summary': 'Legacy scheduler policy API in use by the scheduler.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000183566,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.620428983Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000192882,\n",
       "     'lastEvaluation': '2023-05-29T21:24:21.620422201Z'},\n",
       "    {'name': 'machine-api-operator-metrics-collector-up',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules-6d01e2dc-5d83-4f90-ac4d-b131713b4ce2.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineAPIOperatorMetricsCollectionFailing',\n",
       "       'query': 'mapi_mao_collector_up == 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'For more details:  oc logs <machine-api-operator-pod-name> -n openshift-machine-api',\n",
       "        'summary': 'machine api operator metrics collection is failing.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000284877,\n",
       "       'lastEvaluation': '2023-05-29T21:24:29.491299335Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000296058,\n",
       "     'lastEvaluation': '2023-05-29T21:24:29.491292683Z'},\n",
       "    {'name': 'machine-health-check-unterminated-short-circuit',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules-6d01e2dc-5d83-4f90-ac4d-b131713b4ce2.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineHealthCheckUnterminatedShortCircuit',\n",
       "       'query': 'mapi_machinehealthcheck_short_circuit == 1',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The number of unhealthy machines has exceeded the `maxUnhealthy` limit for the check, you should check\\nthe status of machines in the cluster.\\n',\n",
       "        'summary': 'machine health check {{ $labels.name }} has been disabled by short circuit for more than 30 minutes'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000198092,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.070334843Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000208362,\n",
       "     'lastEvaluation': '2023-05-29T21:24:39.070327509Z'},\n",
       "    {'name': 'machine-not-yet-deleted',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules-6d01e2dc-5d83-4f90-ac4d-b131713b4ce2.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineNotYetDeleted',\n",
       "       'query': 'sum by (name, namespace) (avg_over_time(mapi_machine_created_timestamp_seconds{phase=\"Deleting\"}[15m])) > 0',\n",
       "       'duration': 21600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The machine is not properly deleting, this may be due to a configuration issue with the\\ninfrastructure provider, or because workloads on the node have PodDisruptionBudgets or\\nlong termination periods which are preventing deletion.\\n',\n",
       "        'summary': 'machine {{ $labels.name }} has been in Deleting phase for more than 6 hours'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000237628,\n",
       "       'lastEvaluation': '2023-05-29T21:24:38.558155747Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000247346,\n",
       "     'lastEvaluation': '2023-05-29T21:24:38.558148273Z'},\n",
       "    {'name': 'machine-with-no-running-phase',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules-6d01e2dc-5d83-4f90-ac4d-b131713b4ce2.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineWithNoRunningPhase',\n",
       "       'query': 'sum by (name, namespace) (mapi_machine_created_timestamp_seconds{phase!~\"Running|Deleting\"}) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The machine has been without a Running or Deleting phase for more than 60 minutes.\\nThe machine may not have been provisioned properly from the infrastructure provider, or\\nit might have issues with CertificateSigningRequests being approved.\\n',\n",
       "        'summary': 'machine {{ $labels.name }} is in phase: {{ $labels.phase }}'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000279927,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.574742456Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000290446,\n",
       "     'lastEvaluation': '2023-05-29T21:24:37.574734391Z'},\n",
       "    {'name': 'machine-without-valid-node-ref',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules-6d01e2dc-5d83-4f90-ac4d-b131713b4ce2.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineWithoutValidNode',\n",
       "       'query': 'sum by (name, namespace) (mapi_machine_created_timestamp_seconds unless on (node) kube_node_info) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'If the machine never became a node, you should diagnose the machine related failures.\\nIf the node was deleted from the API, you may delete the machine if appropriate.\\n',\n",
       "        'summary': 'machine {{ $labels.name }} does not have valid node reference'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000450377,\n",
       "       'lastEvaluation': '2023-05-29T21:24:36.62855395Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000462049,\n",
       "     'lastEvaluation': '2023-05-29T21:24:36.628545905Z'},\n",
       "    {'name': 'mcc-paused-pool-kubelet-ca',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-controller-1d6310c2-a79a-4d62-af61-01ed3ea396e2.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineConfigControllerPausedPoolKubeletCA',\n",
       "       'query': 'max by (namespace, pool) (last_over_time(machine_config_controller_paused_pool_kubelet_ca[5m])) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"Machine config pools have a 'pause' feature, which allows config to be rendered, but prevents it from being rolled out to the nodes. This alert indicates that a certificate rotation has taken place, and the new kubelet-ca certificate bundle has been rendered into a machine config, but because the pool '{{$labels.pool}}' is paused, the config cannot be rolled out to the nodes in that pool. You will notice almost immediately that for nodes in pool '{{$labels.pool}}', pod logs will not be visible in the console and interactive commands (oc log, oc exec, oc debug, oc attach) will not work. You must unpause machine config pool '{{$labels.pool}}' to let the certificates through before the kube-apiserver-to-kubelet-signer certificate expires on {{ $value | humanizeTimestamp }} or this pool's nodes will cease to function properly.\",\n",
       "        'runbook_url': 'https://github.com/openshift/blob/master/alerts/machine-config-operator/MachineConfigControllerPausedPoolKubeletCA.md',\n",
       "        'summary': \"Paused machine configuration pool '{{$labels.pool}}' is blocking a necessary certificate rotation and must be unpaused before the current kube-apiserver-to-kubelet-signer certificate expires on {{ $value | humanizeTimestamp }}.\"},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000349499,\n",
       "       'lastEvaluation': '2023-05-29T21:24:16.626056476Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'MachineConfigControllerPausedPoolKubeletCA',\n",
       "       'query': 'max by (namespace, pool) (last_over_time(machine_config_controller_paused_pool_kubelet_ca[5m]) - time()) < (86400 * 14) and max by (namespace, pool) (last_over_time(machine_config_controller_paused_pool_kubelet_ca[5m])) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': \"Machine config pools have a 'pause' feature, which allows config to be rendered, but prevents it from being rolled out to the nodes. This alert indicates that a certificate rotation has taken place, and the new kubelet-ca certificate bundle has been rendered into a machine config, but because the pool '{{$labels.pool}}' is paused, the config cannot be rolled out to the nodes in that pool. You will notice almost immediately that for nodes in pool '{{$labels.pool}}', pod logs will not be visible in the console and interactive commands (oc log, oc exec, oc debug, oc attach) will not work. You must unpause machine config pool '{{$labels.pool}}' to let the certificates through before the kube-apiserver-to-kubelet-signer certificate expires. You have approximately {{ $value | humanizeDuration }} remaining before this happens and nodes in '{{$labels.pool}}' cease to function properly.\",\n",
       "        'runbook_url': 'https://github.com/openshift/blob/master/alerts/machine-config-operator/MachineConfigControllerPausedPoolKubeletCA.md',\n",
       "        'summary': \"Paused machine configuration pool '{{$labels.pool}}' is blocking a necessary certificate rotation and must be unpaused before the current kube-apiserver-to-kubelet-signer certificate expires in {{ $value | humanizeDuration }}.\"},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000229772,\n",
       "       'lastEvaluation': '2023-05-29T21:24:16.626407818Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000593276,\n",
       "     'lastEvaluation': '2023-05-29T21:24:16.626046367Z'},\n",
       "    {'name': 'os-image-override.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-controller-1d6310c2-a79a-4d62-af61-01ed3ea396e2.yaml',\n",
       "     'rules': [{'name': 'os_image_url_override:sum',\n",
       "       'query': 'sum(os_image_url_override)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000337525,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.978323606Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000351611,\n",
       "     'lastEvaluation': '2023-05-29T21:24:37.978313246Z'},\n",
       "    {'name': 'extremely-high-individual-control-plane-memory',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-96972e89-6cb3-4032-a582-9db5724c6a09.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ExtremelyHighIndividualControlPlaneMemory',\n",
       "       'query': '(1 - sum by (instance) (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes and on (instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\")) / sum by (instance) (node_memory_MemTotal_bytes and on (instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\"))) * 100 > 90',\n",
       "       'duration': 2700,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'The memory utilization per instance within control plane nodes influence the stability, and responsiveness of the cluster. This can lead to cluster instability and slow responses from kube-apiserver or failing requests specially on etcd. Moreover, OOM kill is expected which negatively influences the pod scheduling. If this happens on container level, the descheduler will not be able to detect it, as it works on the pod level. To fix this, increase memory of the affected node of control plane nodes.',\n",
       "        'summary': 'Extreme memory utilization per node within control plane nodes is extremely high, and could impact responsiveness and stability.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00082799,\n",
       "       'lastEvaluation': '2023-05-29T21:24:16.257004517Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000840132,\n",
       "     'lastEvaluation': '2023-05-29T21:24:16.256994789Z'},\n",
       "    {'name': 'high-overall-control-plane-memory',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-96972e89-6cb3-4032-a582-9db5724c6a09.yaml',\n",
       "     'rules': [{'state': 'firing',\n",
       "       'name': 'HighOverallControlPlaneMemory',\n",
       "       'query': '(1 - sum(node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes and on (instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\")) / sum(node_memory_MemTotal_bytes and on (instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\"))) * 100 > 60',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Given three control plane nodes, the overall memory utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the kube-apiserver and etcd my be slow to respond. To fix this, increase memory of the control plane nodes.',\n",
       "        'summary': 'Memory utilization across all control plane nodes is high, and could impact responsiveness and stability.'},\n",
       "       'alerts': [{'labels': {'alertname': 'HighOverallControlPlaneMemory',\n",
       "          'severity': 'warning'},\n",
       "         'annotations': {'description': 'Given three control plane nodes, the overall memory utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the kube-apiserver and etcd my be slow to respond. To fix this, increase memory of the control plane nodes.',\n",
       "          'summary': 'Memory utilization across all control plane nodes is high, and could impact responsiveness and stability.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2023-05-27T18:20:14.419380858Z',\n",
       "         'value': '6.291935872976238e+01'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000901678,\n",
       "       'lastEvaluation': '2023-05-29T21:24:14.420918072Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000911797,\n",
       "     'lastEvaluation': '2023-05-29T21:24:14.420910688Z'},\n",
       "    {'name': 'mcd-drain-error',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-96972e89-6cb3-4032-a582-9db5724c6a09.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MCDDrainError',\n",
       "       'query': 'mcd_drain_err > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'message': 'Drain failed on {{ $labels.node }} , updates may be blocked. For more details check MachineConfigController pod logs: oc logs -f -n {{ $labels.namespace }} machine-config-controller-xxxxx -c machine-config-controller'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00031908,\n",
       "       'lastEvaluation': '2023-05-29T21:24:33.78655061Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000329941,\n",
       "     'lastEvaluation': '2023-05-29T21:24:33.786542765Z'},\n",
       "    {'name': 'mcd-kubelet-health-state-error',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-96972e89-6cb3-4032-a582-9db5724c6a09.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeletHealthState',\n",
       "       'query': 'mcd_kubelet_state > 2',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'message': 'Kubelet health failure threshold reached'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000168127,\n",
       "       'lastEvaluation': '2023-05-29T21:24:36.011108019Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000173577,\n",
       "     'lastEvaluation': '2023-05-29T21:24:36.011104864Z'},\n",
       "    {'name': 'mcd-pivot-error',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-96972e89-6cb3-4032-a582-9db5724c6a09.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MCDPivotError',\n",
       "       'query': 'mcd_pivot_err > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'message': 'Error detected in pivot logs on {{ $labels.node }} '},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000183026,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.591400225Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000188015,\n",
       "     'lastEvaluation': '2023-05-29T21:24:34.59139752Z'},\n",
       "    {'name': 'mcd-reboot-error',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-96972e89-6cb3-4032-a582-9db5724c6a09.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MCDRebootError',\n",
       "       'query': 'mcd_reboot_err > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'message': 'Reboot failed on {{ $labels.node }} , update may be blocked'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000278805,\n",
       "       'lastEvaluation': '2023-05-29T21:24:31.122349251Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000294644,\n",
       "     'lastEvaluation': '2023-05-29T21:24:31.122337018Z'},\n",
       "    {'name': 'system-memory-exceeds-reservation',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-96972e89-6cb3-4032-a582-9db5724c6a09.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'SystemMemoryExceedsReservation',\n",
       "       'query': 'sum by (node) (container_memory_rss{id=\"/system.slice\"}) > ((sum by (node) (kube_node_status_capacity{resource=\"memory\"} - kube_node_status_allocatable{resource=\"memory\"})) * 0.95)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'message': 'System memory usage of {{ $value | humanize }} on {{ $labels.node }} exceeds 95% of the reservation. Reserved memory ensures system processes can function even when the node is fully allocated and protects against workload out of memory events impacting the proper functioning of the node. The default reservation is expected to be sufficient for most configurations and should be increased (https://docs.openshift.com/container-platform/latest/nodes/nodes/nodes-nodes-managing.html) when running nodes with high numbers of pods (either due to rate of change or at steady state).'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000601773,\n",
       "       'lastEvaluation': '2023-05-29T21:24:14.950445475Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000613525,\n",
       "     'lastEvaluation': '2023-05-29T21:24:14.950436248Z'},\n",
       "    {'name': 'operator.marketplace.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-marketplace-marketplace-alert-rules-5e49cbc3-a9d1-462a-8d83-cec88f54f06f.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'OperatorHubSourceError',\n",
       "       'query': 'catalogsource_ready{exported_namespace=\"openshift-marketplace\"} == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Operators shipped via the {{ $labels.name }} source are not available for installation until the issue is fixed. Operators already installed from this source will not receive updates until issue is fixed. Inspect the status of the pod owned by {{ $labels.name }} source in the openshift-marketplace namespace (oc -n openshift-marketplace get pods -l olm.catalogSource={{ $labels.name }}) to diagnose and repair.',\n",
       "        'summary': 'The {{ $labels.name }} source is in non-ready state for more than 10 minutes.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000305254,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.931523764Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000314732,\n",
       "     'lastEvaluation': '2023-05-29T21:24:40.93151663Z'},\n",
       "    {'name': 'alertmanager.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-alertmanager-main-rules-98fc1b1c-a7ac-4ded-b1be-cc651081316b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'AlertmanagerFailedReload',\n",
       "       'query': 'max_over_time(alertmanager_config_last_reload_successful{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]) == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedReload.md',\n",
       "        'summary': 'Reloading an Alertmanager configuration has failed.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000455198,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.71045197Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'AlertmanagerMembersInconsistent',\n",
       "       'query': 'max_over_time(alertmanager_cluster_members{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]) < on (namespace, service) group_left () count by (namespace, service) (max_over_time(alertmanager_cluster_members{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]))',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only found {{ $value }} members of the {{$labels.job}} cluster.',\n",
       "        'summary': 'A member of an Alertmanager cluster has not found all other cluster members.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000322536,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.710909302Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'AlertmanagerFailedToSendAlerts',\n",
       "       'query': '(rate(alertmanager_notifications_failed_total{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]) / rate(alertmanager_notifications_total{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m])) > 0.01',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration }}.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedToSendAlerts.md',\n",
       "        'summary': 'An Alertmanager instance failed to send notifications.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000401176,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.71123288Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'AlertmanagerClusterFailedToSendAlerts',\n",
       "       'query': 'min by (namespace, service, integration) (rate(alertmanager_notifications_failed_total{integration=~\".*\",job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]) / rate(alertmanager_notifications_total{integration=~\".*\",job=~\"alertmanager-main|alertmanager-user-workload\"}[5m])) > 0.01',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerClusterFailedToSendAlerts.md',\n",
       "        'summary': 'All Alertmanager instances in a cluster failed to send notifications to a critical integration.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000470867,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.711635528Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'AlertmanagerConfigInconsistent',\n",
       "       'query': 'count by (namespace, service) (count_values by (namespace, service) (\"config_hash\", alertmanager_config_hash{job=~\"alertmanager-main|alertmanager-user-workload\"})) != 1',\n",
       "       'duration': 1200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Alertmanager instances within the {{$labels.job}} cluster have different configurations.',\n",
       "        'summary': 'Alertmanager instances within the same cluster have different configurations.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000189937,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.712107818Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'AlertmanagerClusterDown',\n",
       "       'query': '(count by (namespace, service) (avg_over_time(up{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]) < 0.5) / count by (namespace, service) (up{job=~\"alertmanager-main|alertmanager-user-workload\"})) >= 0.5',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have been up for less than half of the last 5m.',\n",
       "        'summary': 'Half or more of the Alertmanager instances within the same cluster are down.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000272713,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.712299258Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.002130481,\n",
       "     'lastEvaluation': '2023-05-29T21:24:39.710443494Z'},\n",
       "    {'name': 'general.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-edf3bdb6-aa4b-459b-912d-d6e52eb63b6c.yaml',\n",
       "     'rules': [{'state': 'firing',\n",
       "       'name': 'Watchdog',\n",
       "       'query': 'vector(1)',\n",
       "       'duration': 0,\n",
       "       'labels': {'namespace': 'openshift-monitoring', 'severity': 'none'},\n",
       "       'annotations': {'description': 'This is an alert meant to ensure that the entire alerting pipeline is functional.\\nThis alert is always firing, therefore it should always be firing in Alertmanager\\nand always fire against a receiver. There are integrations with various notification\\nmechanisms that send a notification when this alert is not firing. For example the\\n\"DeadMansSnitch\" integration in PagerDuty.\\n',\n",
       "        'summary': 'An alert that should always be firing to certify that Alertmanager is working properly.'},\n",
       "       'alerts': [{'labels': {'alertname': 'Watchdog',\n",
       "          'namespace': 'openshift-monitoring',\n",
       "          'severity': 'none'},\n",
       "         'annotations': {'description': 'This is an alert meant to ensure that the entire alerting pipeline is functional.\\nThis alert is always firing, therefore it should always be firing in Alertmanager\\nand always fire against a receiver. There are integrations with various notification\\nmechanisms that send a notification when this alert is not firing. For example the\\n\"DeadMansSnitch\" integration in PagerDuty.\\n',\n",
       "          'summary': 'An alert that should always be firing to certify that Alertmanager is working properly.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2023-05-25T14:36:54.212358567Z',\n",
       "         'value': '1e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000492597,\n",
       "       'lastEvaluation': '2023-05-29T21:24:24.213779765Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000511723,\n",
       "     'lastEvaluation': '2023-05-29T21:24:24.213763415Z'},\n",
       "    {'name': 'kube-prometheus-general.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-edf3bdb6-aa4b-459b-912d-d6e52eb63b6c.yaml',\n",
       "     'rules': [{'name': 'count:up1',\n",
       "       'query': 'count without (instance, pod, node) (up == 1)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001542934,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.944348828Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'count:up0',\n",
       "       'query': 'count without (instance, pod, node) (up == 0)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000748279,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.945893836Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.00230579,\n",
       "     'lastEvaluation': '2023-05-29T21:24:32.94433907Z'},\n",
       "    {'name': 'kube-prometheus-node-recording.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-edf3bdb6-aa4b-459b-912d-d6e52eb63b6c.yaml',\n",
       "     'rules': [{'name': 'instance:node_cpu:rate:sum',\n",
       "       'query': 'sum by (instance) (rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[3m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001259622,\n",
       "       'lastEvaluation': '2023-05-29T21:24:38.034493775Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_network_receive_bytes:rate:sum',\n",
       "       'query': 'sum by (instance) (rate(node_network_receive_bytes_total[3m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.0004144,\n",
       "       'lastEvaluation': '2023-05-29T21:24:38.035755661Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_network_transmit_bytes:rate:sum',\n",
       "       'query': 'sum by (instance) (rate(node_network_transmit_bytes_total[3m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00036672,\n",
       "       'lastEvaluation': '2023-05-29T21:24:38.036171273Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:node_cpu:sum_rate5m',\n",
       "       'query': 'sum(rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000898041,\n",
       "       'lastEvaluation': '2023-05-29T21:24:38.036539016Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:node_cpu:ratio',\n",
       "       'query': 'cluster:node_cpu:sum_rate5m / count(sum by (instance, cpu) (node_cpu_seconds_total))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001262065,\n",
       "       'lastEvaluation': '2023-05-29T21:24:38.037438189Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.00421773,\n",
       "     'lastEvaluation': '2023-05-29T21:24:38.03448542Z'},\n",
       "    {'name': 'kubernetes-recurring.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-edf3bdb6-aa4b-459b-912d-d6e52eb63b6c.yaml',\n",
       "     'rules': [{'name': 'cluster:usage:workload:capacity_physical_cpu_core_seconds',\n",
       "       'query': 'sum_over_time(workload:capacity_physical_cpu_cores:sum[30s:1s]) + ((cluster:usage:workload:capacity_physical_cpu_core_seconds offset 25s) or (absent(cluster:usage:workload:capacity_physical_cpu_core_seconds offset 25s) * 0))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000526761,\n",
       "       'lastEvaluation': '2023-05-29T21:24:42.260651662Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000539375,\n",
       "     'lastEvaluation': '2023-05-29T21:24:42.260642534Z'},\n",
       "    {'name': 'node-network',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-edf3bdb6-aa4b-459b-912d-d6e52eb63b6c.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'NodeNetworkInterfaceFlapping',\n",
       "       'query': 'changes(node_network_up{device!~\"veth.+|tunbr\",job=\"node-exporter\"}[2m]) > 2',\n",
       "       'duration': 120,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Network interface \"{{ $labels.device }}\" changing its up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}',\n",
       "        'summary': 'Network interface is often changing its status'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000474482,\n",
       "       'lastEvaluation': '2023-05-29T21:24:33.637944579Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000499249,\n",
       "     'lastEvaluation': '2023-05-29T21:24:33.637922608Z'},\n",
       "    {'name': 'openshift-build.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-edf3bdb6-aa4b-459b-912d-d6e52eb63b6c.yaml',\n",
       "     'rules': [{'name': 'openshift:build_by_strategy:sum',\n",
       "       'query': 'sum by (strategy) (openshift_build_status_phase_total)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000750533,\n",
       "       'lastEvaluation': '2023-05-29T21:24:24.636791053Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000762125,\n",
       "     'lastEvaluation': '2023-05-29T21:24:24.636782757Z'},\n",
       "    {'name': 'openshift-etcd-telemetry.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-edf3bdb6-aa4b-459b-912d-d6e52eb63b6c.yaml',\n",
       "     'rules': [{'name': 'instance:etcd_mvcc_db_total_size_in_bytes:sum',\n",
       "       'query': 'sum by (instance) (etcd_mvcc_db_total_size_in_bytes{job=\"etcd\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000373233,\n",
       "       'lastEvaluation': '2023-05-29T21:24:42.634263144Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:etcd_disk_wal_fsync_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=\"etcd\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000542051,\n",
       "       'lastEvaluation': '2023-05-29T21:24:42.63463838Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:etcd_network_peer_round_trip_time_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum by (instance, le) (rate(etcd_network_peer_round_trip_time_seconds_bucket{job=\"etcd\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001060457,\n",
       "       'lastEvaluation': '2023-05-29T21:24:42.635182364Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:etcd_mvcc_db_total_size_in_use_in_bytes:sum',\n",
       "       'query': 'sum by (instance) (etcd_mvcc_db_total_size_in_use_in_bytes{job=\"etcd\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000204344,\n",
       "       'lastEvaluation': '2023-05-29T21:24:42.636244644Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_backend_commit_duration_seconds_bucket{job=\"etcd\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00054734,\n",
       "       'lastEvaluation': '2023-05-29T21:24:42.636451363Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.002745588,\n",
       "     'lastEvaluation': '2023-05-29T21:24:42.63425573Z'},\n",
       "    {'name': 'openshift-general.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-edf3bdb6-aa4b-459b-912d-d6e52eb63b6c.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'TargetDown',\n",
       "       'query': '100 * ((1 - sum by (job, namespace, service) (up and on (namespace, pod) kube_pod_info) / count by (job, namespace, service) (up and on (namespace, pod) kube_pod_info)) or (count by (job, namespace, service) (up == 0) / count by (job, namespace, service) (up))) > 10',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace have been unreachable for more than 15 minutes. This may be a symptom of network connectivity issues, down nodes, or failures within these components. Assess the health of the infrastructure and nodes running these targets and then contact support.',\n",
       "        'summary': 'Some targets were not reachable from the monitoring server for an extended period of time.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.008922256,\n",
       "       'lastEvaluation': '2023-05-29T21:24:15.49212755Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.008939117,\n",
       "     'lastEvaluation': '2023-05-29T21:24:15.492116789Z'},\n",
       "    {'name': 'openshift-ingress.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-edf3bdb6-aa4b-459b-912d-d6e52eb63b6c.yaml',\n",
       "     'rules': [{'name': 'code:cluster:ingress_http_request_count:rate5m:sum',\n",
       "       'query': 'sum by (code) (rate(haproxy_server_http_responses_total[5m]) > 0)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002110141,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.894719506Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:ingress_frontend_bytes_in:rate5m:sum',\n",
       "       'query': 'sum(rate(haproxy_frontend_bytes_in_total[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000186181,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.896831381Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:ingress_frontend_bytes_out:rate5m:sum',\n",
       "       'query': 'sum(rate(haproxy_frontend_bytes_out_total[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00015371,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.897018984Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:ingress_frontend_connections:sum',\n",
       "       'query': 'sum(haproxy_frontend_current_sessions)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000132079,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.897174056Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:workload:ingress_request_error:fraction5m',\n",
       "       'query': 'sum(max without (service, endpoint, container, pod, job, namespace) (increase(haproxy_server_http_responses_total{code!~\"2xx|1xx|4xx|3xx\",exported_namespace!~\"openshift-.*\"}[5m]) > 0)) / sum(max without (service, endpoint, container, pod, job, namespace) (increase(haproxy_server_http_responses_total{exported_namespace!~\"openshift-.*\"}[5m]))) or absent(__does_not_exist__) * 0',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001118496,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.897307477Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:workload:ingress_request_total:irate5m',\n",
       "       'query': 'sum(max without (service, endpoint, container, pod, job, namespace) (irate(haproxy_server_http_responses_total{exported_namespace!~\"openshift-.*\"}[5m]))) or absent(__does_not_exist__) * 0',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000732529,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.898427245Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:openshift:ingress_request_error:fraction5m',\n",
       "       'query': 'sum(max without (service, endpoint, container, pod, job, namespace) (increase(haproxy_server_http_responses_total{code!~\"2xx|1xx|4xx|3xx\",exported_namespace=~\"openshift-.*\"}[5m]) > 0)) / sum(max without (service, endpoint, container, pod, job, namespace) (increase(haproxy_server_http_responses_total{exported_namespace=~\"openshift-.*\"}[5m]))) or absent(__does_not_exist__) * 0',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002344334,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.899161147Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:openshift:ingress_request_total:irate5m',\n",
       "       'query': 'sum(max without (service, endpoint, container, pod, job, namespace) (irate(haproxy_server_http_responses_total{exported_namespace=~\"openshift-.*\"}[5m]))) or absent(__does_not_exist__) * 0',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001783447,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.901506863Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:ingress_controller_aws_nlb_active:sum',\n",
       "       'query': 'sum(ingress_controller_aws_nlb_active) or vector(0)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.3517e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:41.903291983Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.008678597,\n",
       "     'lastEvaluation': '2023-05-29T21:24:41.894710349Z'},\n",
       "    {'name': 'openshift-kubernetes.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-edf3bdb6-aa4b-459b-912d-d6e52eb63b6c.yaml',\n",
       "     'rules': [{'name': 'pod:container_cpu_usage:sum',\n",
       "       'query': 'sum by (pod, namespace) (rate(container_cpu_usage_seconds_total{container=\"\",pod!=\"\"}[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.004284595,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.274974246Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'pod:container_fs_usage_bytes:sum',\n",
       "       'query': 'sum by (pod, namespace) (container_fs_usage_bytes{pod!=\"\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003874324,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.279262939Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace:container_memory_usage_bytes:sum',\n",
       "       'query': 'sum by (namespace) (container_memory_usage_bytes{container!=\"\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003176519,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.283139648Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace:container_cpu_usage:sum',\n",
       "       'query': 'sum by (namespace) (rate(container_cpu_usage_seconds_total{container!=\"\",container!=\"POD\"}[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003592513,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.286318371Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:memory_usage:ratio',\n",
       "       'query': 'sum by (cluster) (container_memory_usage_bytes{container=\"\",pod!=\"\"}) / sum by (cluster) (machine_memory_bytes)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002688381,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.289912968Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:container_spec_cpu_shares:ratio',\n",
       "       'query': 'sum(container_spec_cpu_shares{container=\"\",pod!=\"\"}) / 1000 / sum(machine_cpu_cores)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003167642,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.292602722Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:container_cpu_usage:ratio',\n",
       "       'query': 'sum(rate(container_cpu_usage_seconds_total{container=\"\",pod!=\"\"}[5m])) / sum(machine_cpu_cores)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.004375447,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.295773089Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:master_nodes',\n",
       "       'query': 'max without (endpoint, instance, job, pod, service) (kube_node_labels and on (node) kube_node_role{role=\"master\"})',\n",
       "       'labels': {'label_node_role_kubernetes_io': 'master',\n",
       "        'label_node_role_kubernetes_io_master': 'true'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000227989,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.300150941Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:infra_nodes',\n",
       "       'query': 'max without (endpoint, instance, job, pod, service) (kube_node_labels and on (node) kube_node_role{role=\"infra\"})',\n",
       "       'labels': {'label_node_role_kubernetes_io_infra': 'true'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00013232,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.300380182Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:master_infra_nodes',\n",
       "       'query': 'max without (endpoint, instance, job, pod, service) (cluster:master_nodes and on (node) cluster:infra_nodes)',\n",
       "       'labels': {'label_node_role_kubernetes_io_infra': 'true',\n",
       "        'label_node_role_kubernetes_io_master': 'true'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00010598,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.300513513Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:nodes_roles',\n",
       "       'query': 'cluster:master_infra_nodes or on (node) cluster:master_nodes or on (node) cluster:infra_nodes or on (node) max without (endpoint, instance, job, pod, service) (kube_node_labels)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000263105,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.30062304Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:hyperthread_enabled_nodes',\n",
       "       'query': 'kube_node_labels and on (node) (sum by (node, package, core) (label_replace(node_cpu_info, \"node\", \"$1\", \"instance\", \"(.*)\")) == 2)',\n",
       "       'labels': {'label_node_hyperthread_enabled': 'true'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000628714,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.300887658Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:virt_platform_nodes:sum',\n",
       "       'query': 'count by (type, system_manufacturer, system_product_name, baseboard_manufacturer, baseboard_product_name) (sum by (instance, type, system_manufacturer, system_product_name, baseboard_manufacturer, baseboard_product_name) (virt_platform))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000246944,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.301517594Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:capacity_cpu_cores:sum',\n",
       "       'query': 'sum by (label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id) ((cluster:master_nodes * on (node) group_left () max by (node) (kube_node_status_capacity{resource=\"cpu\",unit=\"core\"})) or on (node) (max without (endpoint, instance, job, pod, service) (kube_node_labels) * on (node) group_left () max by (node) (kube_node_status_capacity{resource=\"cpu\",unit=\"core\"})))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000473382,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.301766191Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:cpu_core_hyperthreading',\n",
       "       'query': 'clamp_max(label_replace(sum by (instance, package, core) (node_cpu_info{core!=\"\",package!=\"\"} or label_replace(label_join(node_cpu_info{core=\"\",package=\"\"}, \"core\", \"\", \"cpu\"), \"package\", \"0\", \"package\", \"\")) > 1, \"label_node_hyperthread_enabled\", \"true\", \"instance\", \"(.*)\") or on (instance, package) label_replace(sum by (instance, package, core) (label_replace(node_cpu_info{core!=\"\",package!=\"\"} or label_join(node_cpu_info{core=\"\",package=\"\"}, \"core\", \"\", \"cpu\"), \"package\", \"0\", \"package\", \"\")) <= 1, \"label_node_hyperthread_enabled\", \"false\", \"instance\", \"(.*)\"), 1)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001265803,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.302241106Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:cpu_core_node_labels',\n",
       "       'query': 'topk by (node) (1, cluster:nodes_roles) * on (node) group_right (label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_node_openshift_io_os_id, label_kubernetes_io_arch, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) label_replace(cluster:cpu_core_hyperthreading, \"node\", \"$1\", \"instance\", \"(.*)\")',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000682194,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.303508582Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:capacity_cpu_cores_hyperthread_enabled:sum',\n",
       "       'query': 'count by (label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled) (cluster:cpu_core_node_labels)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000197742,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.304193361Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:capacity_memory_bytes:sum',\n",
       "       'query': 'sum by (label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io) ((cluster:master_nodes * on (node) group_left () max by (node) (kube_node_status_capacity{resource=\"memory\",unit=\"byte\"})) or on (node) (max without (endpoint, instance, job, pod, service) (kube_node_labels) * on (node) group_left () max by (node) (kube_node_status_capacity{resource=\"memory\",unit=\"byte\"})))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000450479,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.304392435Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:cpu_usage_cores:sum',\n",
       "       'query': 'sum(1 - rate(node_cpu_seconds_total{mode=\"idle\"}[2m]) * on (namespace, pod) group_left (node) node_namespace_pod:kube_pod_info:{pod=~\"node-exporter.+\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000709104,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.304844327Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:memory_usage_bytes:sum',\n",
       "       'query': 'sum(node_memory_MemTotal_bytes{job=\"node-exporter\"} - node_memory_MemAvailable_bytes{job=\"node-exporter\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000391277,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.305555164Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'workload:cpu_usage_cores:sum',\n",
       "       'query': 'sum(rate(container_cpu_usage_seconds_total{container=\"\",namespace!~\"openshift-.+\",pod!=\"\"}[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002119119,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.305948505Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'openshift:cpu_usage_cores:sum',\n",
       "       'query': 'cluster:cpu_usage_cores:sum - workload:cpu_usage_cores:sum',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.6903e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.308069067Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'workload:memory_usage_bytes:sum',\n",
       "       'query': 'sum(container_memory_working_set_bytes{container=\"\",namespace!~\"openshift-.+\",pod!=\"\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001545088,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.308166821Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'openshift:memory_usage_bytes:sum',\n",
       "       'query': 'cluster:memory_usage_bytes:sum - workload:memory_usage_bytes:sum',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.1715e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.309713132Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:node_instance_type_count:sum',\n",
       "       'query': 'sum by (label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id) (cluster:master_nodes or on (node) kube_node_labels)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000134603,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.309785819Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum',\n",
       "       'query': 'sum by (provisioner) (topk by (namespace, persistentvolumeclaim) (1, kube_persistentvolumeclaim_resource_requests_storage_bytes) * on (namespace, persistentvolumeclaim) group_right () topk by (namespace, persistentvolumeclaim) (1, kube_persistentvolumeclaim_info * on (storageclass) group_left (provisioner) topk by (storageclass) (1, max by (storageclass, provisioner) (kube_storageclass_info))))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000172174,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.309921464Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'workload:capacity_physical_cpu_cores:sum',\n",
       "       'query': '(sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_infra=\"\",label_node_role_kubernetes_io_master=\"\"} or absent(__does_not_exist__) * 0)) + ((sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master=\"true\"} or absent(__does_not_exist__) * 0) * ((max(cluster_master_schedulable == 1) * 0 + 1) or (absent(cluster_master_schedulable == 1) * 0))))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000334891,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.31009457Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:workload:capacity_physical_cpu_cores:min:5m',\n",
       "       'query': 'min_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.3918e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.310430473Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:workload:capacity_physical_cpu_cores:max:5m',\n",
       "       'query': 'max_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 6.342e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.310505163Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:kubelet_volume_stats_used_bytes:provisioner:sum',\n",
       "       'query': 'sum by (provisioner) (topk by (namespace, persistentvolumeclaim) (1, kubelet_volume_stats_used_bytes) * on (namespace, persistentvolumeclaim) group_right () topk by (namespace, persistentvolumeclaim) (1, kube_persistentvolumeclaim_info * on (storageclass) group_left (provisioner) topk by (storageclass) (1, max by (storageclass, provisioner) (kube_storageclass_info))))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000153639,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.310569484Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:etcd_object_counts:sum',\n",
       "       'query': 'sum by (instance) (apiserver_storage_objects)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002784822,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.310723995Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:resources:sum',\n",
       "       'query': 'topk(500, max by (resource) (apiserver_storage_objects))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003549932,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.31351038Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:pods:terminal:workload:sum',\n",
       "       'query': 'count(count by (namespace, pod) (kube_pod_restart_policy{namespace!~\"openshift-.+\",type!=\"Always\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000231837,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.31706434Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:containers:sum',\n",
       "       'query': 'sum(max by (instance) (kubelet_containers_per_pod_count_sum))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000173526,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.317297229Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_role_os_version_machine:cpu_capacity_cores:sum',\n",
       "       'query': 'count by (label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) (cluster:cpu_core_node_labels)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.0002441,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.317472899Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:capacity_cpu_sockets_hyperthread_enabled:sum',\n",
       "       'query': 'count by (label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled, label_node_role_kubernetes_io) (max by (node, package, label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled, label_node_role_kubernetes_io) (cluster:cpu_core_node_labels))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00020698,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.317718091Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_role_os_version_machine:cpu_capacity_sockets:sum',\n",
       "       'query': 'count by (label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) (max by (node, package, label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) (cluster:cpu_core_node_labels))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000231867,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.317926163Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:alertmanager_integrations:max',\n",
       "       'query': 'max(alertmanager_integrations{namespace=\"openshift-monitoring\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.0061e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.318159052Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:kube_persistentvolume_plugin_type_counts:sum',\n",
       "       'query': 'sum by (plugin_name, volume_mode) (pv_collector_total_pv_count)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 3.6859e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.318230065Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:vsphere_vcenter_info:sum',\n",
       "       'query': 'sum by (version) (vsphere_vcenter_info)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 3.2472e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.318267695Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:vsphere_esxi_version_total:sum',\n",
       "       'query': 'sum by (version) (vsphere_esxi_version_total)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 2.8895e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.318300848Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:vsphere_node_hw_version_total:sum',\n",
       "       'query': 'sum by (hw_version) (vsphere_node_hw_version_total)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 2.7702e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.318330494Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:control_plane:all_nodes_ready',\n",
       "       'query': 'sum(min by (node) (kube_node_status_condition{condition=\"Ready\",status=\"true\"}) and max by (node) (kube_node_role{role=\"master\"})) == bool sum(kube_node_role{role=\"master\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000197513,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.318358917Z',\n",
       "       'type': 'recording'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ClusterMonitoringOperatorReconciliationErrors',\n",
       "       'query': 'max_over_time(cluster_monitoring_operator_last_reconciliation_successful[5m]) == 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Errors are occurring during reconciliation cycles. Inspect the cluster-monitoring-operator log for potential root causes.',\n",
       "        'summary': 'Cluster Monitoring Operator is experiencing unexpected reconciliation errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.9248e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.318557431Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'firing',\n",
       "       'name': 'AlertmanagerReceiversNotConfigured',\n",
       "       'query': 'cluster:alertmanager_integrations:max == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'namespace': 'openshift-monitoring', 'severity': 'warning'},\n",
       "       'annotations': {'description': 'Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager.',\n",
       "        'summary': 'Receivers (notification integrations) are not configured on Alertmanager'},\n",
       "       'alerts': [{'labels': {'alertname': 'AlertmanagerReceiversNotConfigured',\n",
       "          'namespace': 'openshift-monitoring',\n",
       "          'severity': 'warning'},\n",
       "         'annotations': {'description': 'Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager.',\n",
       "          'summary': 'Receivers (notification integrations) are not configured on Alertmanager'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2023-05-25T14:37:21.265757059Z',\n",
       "         'value': '0e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000253588,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.318637822Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeDeploymentReplicasMismatch',\n",
       "       'query': '(((kube_deployment_spec_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > kube_deployment_status_replicas_available{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) and (changes(kube_deployment_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[5m]) == 0)) * on () group_left () cluster:control_plane:all_nodes_ready) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes. This indicates that cluster infrastructure is unable to start or restart the necessary components. This most often occurs when one or more nodes are down or partioned from the cluster, or a fault occurs on the node that prevents the workload from starting. In rare cases this may indicate a new version of a cluster component cannot start due to a bug or configuration error. Assess the pods for this deployment to verify they are running on healthy nodes and then contact support.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeDeploymentReplicasMismatch.md',\n",
       "        'summary': 'Deployment has not matched the expected number of replicas'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00217306,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.318892642Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'MultipleContainersOOMKilled',\n",
       "       'query': 'sum(max by (namespace, container, pod) (increase(kube_pod_container_status_restarts_total[12m])) and max by (namespace, container, pod) (kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}) == 1) > 5',\n",
       "       'duration': 900,\n",
       "       'labels': {'namespace': 'kube-system', 'severity': 'info'},\n",
       "       'annotations': {'description': 'Multiple containers were out of memory killed within the past 15 minutes. There are many potential causes of OOM errors, however issues on a specific node or containers breaching their limits is common.',\n",
       "        'summary': 'Containers are being killed due to OOM'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003529535,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.321066714Z',\n",
       "       'type': 'alerting'},\n",
       "      {'name': 'cluster:usage:kube_schedulable_node_ready_reachable:avg5m',\n",
       "       'query': 'avg_over_time((((count((max by (node) (up{job=\"kubelet\",metrics_path=\"/metrics\"} == 1) and max by (node) (kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 1) and min by (node) (kube_node_spec_unschedulable == 0))) / scalar(count(min by (node) (kube_node_spec_unschedulable == 0))))))[5m:1s])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.011317916,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.324597401Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:kube_node_ready:avg5m',\n",
       "       'query': 'avg_over_time((count(max by (node) (kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 1)) / scalar(count(max by (node) (kube_node_status_condition{condition=\"Ready\",status=\"true\"}))))[5m:1s])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.005123204,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.335918062Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'kube_running_pod_ready',\n",
       "       'query': '(max without (condition, container, endpoint, instance, job, service) (((kube_pod_status_ready{condition=\"false\"} == 1) * 0 or (kube_pod_status_ready{condition=\"true\"} == 1)) * on (pod, namespace) group_left () group by (pod, namespace) (kube_pod_status_phase{phase=~\"Running|Unknown|Pending\"} == 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.010829177,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.34104331Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:openshift:kube_running_pod_ready:avg',\n",
       "       'query': 'avg(kube_running_pod_ready{namespace=~\"openshift-.*\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001426686,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.351876364Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:workload:kube_running_pod_ready:avg',\n",
       "       'query': 'avg(kube_running_pod_ready{namespace!~\"openshift-.*\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000386677,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.353304613Z',\n",
       "       'type': 'recording'},\n",
       "      {'state': 'firing',\n",
       "       'name': 'KubePodNotScheduled',\n",
       "       'query': 'last_over_time(kube_pod_status_unschedulable{namespace=~\"(openshift-.*|kube-.*|default)\"}[5m]) == 1',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Pod {{ $labels.namespace }}/{{ $labels.pod }} cannot be scheduled for more than 30 minutes.\\nCheck the details of the pod with the following command:\\noc describe -n {{ $labels.namespace }} pod {{ $labels.pod }}',\n",
       "        'summary': 'Pod cannot be scheduled.'},\n",
       "       'alerts': [{'labels': {'alertname': 'KubePodNotScheduled',\n",
       "          'container': 'kube-rbac-proxy-main',\n",
       "          'endpoint': 'https-main',\n",
       "          'job': 'kube-state-metrics',\n",
       "          'namespace': 'openshift-gitops',\n",
       "          'pod': 'openshift-gitops-application-controller-0',\n",
       "          'service': 'kube-state-metrics',\n",
       "          'severity': 'warning',\n",
       "          'uid': '15ac73e0-a53d-4a12-9bdf-10d4aa14dddc'},\n",
       "         'annotations': {'description': 'Pod openshift-gitops/openshift-gitops-application-controller-0 cannot be scheduled for more than 30 minutes.\\nCheck the details of the pod with the following command:\\noc describe -n openshift-gitops pod openshift-gitops-application-controller-0',\n",
       "          'summary': 'Pod cannot be scheduled.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2023-05-27T18:21:21.265757059Z',\n",
       "         'value': '1e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000493068,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.353692793Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.079223197,\n",
       "     'lastEvaluation': '2023-05-29T21:24:21.27496571Z'},\n",
       "    {'name': 'openshift-monitoring.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-edf3bdb6-aa4b-459b-912d-d6e52eb63b6c.yaml',\n",
       "     'rules': [{'name': 'openshift:prometheus_tsdb_head_series:sum',\n",
       "       'query': 'sum by (job, namespace) (max without (instance) (prometheus_tsdb_head_series{namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000373864,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.618768583Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'openshift:prometheus_tsdb_head_samples_appended_total:sum',\n",
       "       'query': 'sum by (job, namespace) (max without (instance) (rate(prometheus_tsdb_head_samples_appended_total{namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[2m])))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000276481,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.619145272Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'monitoring:container_memory_working_set_bytes:sum',\n",
       "       'query': 'sum by (namespace) (max without (instance) (container_memory_working_set_bytes{container=\"\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000701211,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.619424588Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_job:scrape_series_added:topk3_sum1h',\n",
       "       'query': 'topk(3, sum by (namespace, job) (sum_over_time(scrape_series_added[1h])))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002365673,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.620127081Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_job:scrape_samples_post_metric_relabeling:topk3',\n",
       "       'query': 'topk(3, max by (namespace, job) (topk by (namespace, job) (1, scrape_samples_post_metric_relabeling)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00105694,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.622495048Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'monitoring:haproxy_server_http_responses_total:sum',\n",
       "       'query': 'sum by (exported_service) (rate(haproxy_server_http_responses_total{exported_namespace=\"openshift-monitoring\",exported_service=~\"alertmanager-main|prometheus-k8s\"}[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000755643,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.623553751Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_workload_pod:kube_pod_owner:relabel',\n",
       "       'query': 'max by (cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"ReplicationController\"}, \"replicationcontroller\", \"$1\", \"owner_name\", \"(.*)\") * on (replicationcontroller, namespace) group_left (owner_name) topk by (replicationcontroller, namespace) (1, max by (replicationcontroller, namespace, owner_name) (kube_replicationcontroller_owner{job=\"kube-state-metrics\"})), \"workload\", \"$1\", \"owner_name\", \"(.*)\"))',\n",
       "       'labels': {'workload_type': 'deploymentconfig'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000263526,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.624310887Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.005815598,\n",
       "     'lastEvaluation': '2023-05-29T21:24:22.618761059Z'},\n",
       "    {'name': 'openshift-sre.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-edf3bdb6-aa4b-459b-912d-d6e52eb63b6c.yaml',\n",
       "     'rules': [{'name': 'code:apiserver_request_total:rate:sum',\n",
       "       'query': 'sum by (code) (rate(apiserver_request_total{job=\"apiserver\"}[10m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.02143963,\n",
       "       'lastEvaluation': '2023-05-29T21:24:42.289810453Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.02145565,\n",
       "     'lastEvaluation': '2023-05-29T21:24:42.289800314Z'},\n",
       "    {'name': 'kube-state-metrics',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kube-state-metrics-rules-01356bd5-358d-4387-9c30-f9877ee5fca0.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeStateMetricsListErrors',\n",
       "       'query': '(sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\"}[5m]))) > 0.01',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.',\n",
       "        'summary': 'kube-state-metrics is experiencing errors in list operations.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000743639,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.589713481Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeStateMetricsWatchErrors',\n",
       "       'query': '(sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\"}[5m]))) > 0.01',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.',\n",
       "        'summary': 'kube-state-metrics is experiencing errors in watch operations.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000432414,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.590459324Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001189949,\n",
       "     'lastEvaluation': '2023-05-29T21:24:34.589704284Z'},\n",
       "    {'name': 'k8s.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-b1bc5cc0-502b-46cc-a6cf-0e0e05097b28.yaml',\n",
       "     'rules': [{'name': 'node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate',\n",
       "       'query': 'sum by (cluster, namespace, pod, container) (irate(container_cpu_usage_seconds_total{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"}[5m])) * on (cluster, namespace, pod) group_left (node) topk by (cluster, namespace, pod) (1, max by (cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.008577427,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.46342829Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_namespace_pod_container:container_memory_working_set_bytes',\n",
       "       'query': 'container_memory_working_set_bytes{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on (namespace, pod) group_left (node) topk by (namespace, pod) (1, max by (namespace, pod, node) (kube_pod_info{node!=\"\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.008243268,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.472009935Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_namespace_pod_container:container_memory_rss',\n",
       "       'query': 'container_memory_rss{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on (namespace, pod) group_left (node) topk by (namespace, pod) (1, max by (namespace, pod, node) (kube_pod_info{node!=\"\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.010360794,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.48025712Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_namespace_pod_container:container_memory_cache',\n",
       "       'query': 'container_memory_cache{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on (namespace, pod) group_left (node) topk by (namespace, pod) (1, max by (namespace, pod, node) (kube_pod_info{node!=\"\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.010289239,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.490625609Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_namespace_pod_container:container_memory_swap',\n",
       "       'query': 'container_memory_swap{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on (namespace, pod) group_left (node) topk by (namespace, pod) (1, max by (namespace, pod, node) (kube_pod_info{node!=\"\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.011290795,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.500922042Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:namespace:pod_memory:active:kube_pod_container_resource_requests',\n",
       "       'query': 'kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"memory\"} * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.009810338,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.51221995Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_memory:kube_pod_container_resource_requests:sum',\n",
       "       'query': 'sum by (namespace, cluster) (sum by (namespace, pod, cluster) (max by (namespace, pod, container, cluster) (kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"memory\"}) * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.01132516,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.522036229Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests',\n",
       "       'query': 'kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"cpu\"} * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.013039928,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.533365777Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_cpu:kube_pod_container_resource_requests:sum',\n",
       "       'query': 'sum by (namespace, cluster) (sum by (namespace, pod, cluster) (max by (namespace, pod, container, cluster) (kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"cpu\"}) * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00877034,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.546416335Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:namespace:pod_memory:active:kube_pod_container_resource_limits',\n",
       "       'query': 'kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"memory\"} * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.005081546,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.555191193Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_memory:kube_pod_container_resource_limits:sum',\n",
       "       'query': 'sum by (namespace, cluster) (sum by (namespace, pod, cluster) (max by (namespace, pod, container, cluster) (kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"memory\"}) * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.004342365,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.560275815Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits',\n",
       "       'query': 'kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"cpu\"} * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.004116769,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.564620214Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_cpu:kube_pod_container_resource_limits:sum',\n",
       "       'query': 'sum by (namespace, cluster) (sum by (namespace, pod, cluster) (max by (namespace, pod, container, cluster) (kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"cpu\"}) * on (namespace, pod, cluster) group_left () max by (namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.004421042,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.56874024Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_workload_pod:kube_pod_owner:relabel',\n",
       "       'query': 'max by (cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"ReplicaSet\"}, \"replicaset\", \"$1\", \"owner_name\", \"(.*)\") * on (replicaset, namespace) group_left (owner_name) topk by (replicaset, namespace) (1, max by (replicaset, namespace, owner_name) (kube_replicaset_owner{job=\"kube-state-metrics\"})), \"workload\", \"$1\", \"owner_name\", \"(.*)\"))',\n",
       "       'labels': {'workload_type': 'deployment'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.004140635,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.573164388Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_workload_pod:kube_pod_owner:relabel',\n",
       "       'query': 'max by (cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"DaemonSet\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))',\n",
       "       'labels': {'workload_type': 'daemonset'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001059654,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.577307197Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_workload_pod:kube_pod_owner:relabel',\n",
       "       'query': 'max by (cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"StatefulSet\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))',\n",
       "       'labels': {'workload_type': 'statefulset'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00029735,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.578368264Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_workload_pod:kube_pod_owner:relabel',\n",
       "       'query': 'max by (cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"Job\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))',\n",
       "       'labels': {'workload_type': 'job'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000394944,\n",
       "       'lastEvaluation': '2023-05-29T21:24:21.578704226Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.115681189,\n",
       "     'lastEvaluation': '2023-05-29T21:24:21.463420325Z'},\n",
       "    {'name': 'kube-scheduler.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-b1bc5cc0-502b-46cc-a6cf-0e0e05097b28.yaml',\n",
       "     'rules': [{'name': 'cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum without (instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000488259,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.537317068Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum without (instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000928598,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.537808423Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum without (instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000165261,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.538739907Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.9, sum without (instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.9'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.3657e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.538906049Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.9, sum without (instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.9'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000428216,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.539000507Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.9, sum without (instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.9'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000187983,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.539430186Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.5, sum without (instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.5'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000194617,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.539620824Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.5, sum without (instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.5'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000542852,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.539816523Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.5, sum without (instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.5'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000112883,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.540360467Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.003169887,\n",
       "     'lastEvaluation': '2023-05-29T21:24:28.53730741Z'},\n",
       "    {'name': 'kubelet.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-b1bc5cc0-502b-46cc-a6cf-0e0e05097b28.yaml',\n",
       "     'rules': [{'name': 'node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum by (cluster, instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on (cluster, instance) group_left (node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001075164,\n",
       "       'lastEvaluation': '2023-05-29T21:24:27.263216178Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.9, sum by (cluster, instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on (cluster, instance) group_left (node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})',\n",
       "       'labels': {'quantile': '0.9'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000793153,\n",
       "       'lastEvaluation': '2023-05-29T21:24:27.264293235Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.5, sum by (cluster, instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on (cluster, instance) group_left (node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})',\n",
       "       'labels': {'quantile': '0.5'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000742368,\n",
       "       'lastEvaluation': '2023-05-29T21:24:27.265087641Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.002623298,\n",
       "     'lastEvaluation': '2023-05-29T21:24:27.263208674Z'},\n",
       "    {'name': 'kubernetes-apps',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-b1bc5cc0-502b-46cc-a6cf-0e0e05097b28.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubePodCrashLooping',\n",
       "       'query': 'max_over_time(kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",reason=\"CrashLoopBackOff\"}[5m]) >= 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: \"CrashLoopBackOff\").',\n",
       "        'summary': 'Pod is crash looping.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000417566,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.260890331Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubePodNotReady',\n",
       "       'query': 'sum by (namespace, pod, cluster) (max by (namespace, pod, cluster) (kube_pod_status_phase{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",phase=~\"Pending|Unknown\"} unless ignoring (phase) (kube_pod_status_unschedulable{job=\"kube-state-metrics\"} == 1)) * on (namespace, pod, cluster) group_left (owner_kind) topk by (namespace, pod, cluster) (1, max by (namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!=\"Job\"}))) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePodNotReady.md',\n",
       "        'summary': 'Pod has been in a non-ready state for more than 15 minutes.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.006415407,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.26130962Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeDeploymentGenerationMismatch',\n",
       "       'query': 'kube_deployment_status_observed_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_deployment_metadata_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.',\n",
       "        'summary': 'Deployment generation mismatch due to possible roll-back'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001334302,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.26772698Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'firing',\n",
       "       'name': 'KubeStatefulSetReplicasMismatch',\n",
       "       'query': '(kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_statefulset_status_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) and (changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[10m]) == 0)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.',\n",
       "        'summary': 'Deployment has not matched the expected number of replicas.'},\n",
       "       'alerts': [{'labels': {'alertname': 'KubeStatefulSetReplicasMismatch',\n",
       "          'container': 'kube-rbac-proxy-main',\n",
       "          'endpoint': 'https-main',\n",
       "          'job': 'kube-state-metrics',\n",
       "          'namespace': 'openshift-gitops',\n",
       "          'service': 'kube-state-metrics',\n",
       "          'severity': 'warning',\n",
       "          'statefulset': 'openshift-gitops-application-controller'},\n",
       "         'annotations': {'description': 'StatefulSet openshift-gitops/openshift-gitops-application-controller has not matched the expected number of replicas for longer than 15 minutes.',\n",
       "          'summary': 'Deployment has not matched the expected number of replicas.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2023-05-27T18:21:37.260184685Z',\n",
       "         'value': '0e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000854098,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.269062494Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeStatefulSetGenerationMismatch',\n",
       "       'query': 'kube_statefulset_status_observed_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_statefulset_metadata_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.',\n",
       "        'summary': 'StatefulSet generation mismatch due to possible roll-back'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000439398,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.269918305Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeStatefulSetUpdateNotRolledOut',\n",
       "       'query': '(max without (revision) (kube_statefulset_status_current_revision{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} unless kube_statefulset_status_update_revision{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) * (kube_statefulset_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"})) and (changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[5m]) == 0)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.',\n",
       "        'summary': 'StatefulSet update has not been rolled out.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001356964,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.270359366Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeDaemonSetRolloutStuck',\n",
       "       'query': '((kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) or (kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != 0) or (kube_daemonset_status_updated_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) or (kube_daemonset_status_number_available{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"})) and (changes(kube_daemonset_status_updated_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[5m]) == 0)',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 30 minutes.',\n",
       "        'summary': 'DaemonSet rollout is stuck.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002529462,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.271718254Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeContainerWaiting',\n",
       "       'query': 'sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour.',\n",
       "        'summary': 'Pod container waiting longer than 1 hour'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000311366,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.274249389Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeDaemonSetNotScheduled',\n",
       "       'query': 'kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} - kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.',\n",
       "        'summary': 'DaemonSet pods are not scheduled.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000695801,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.274562187Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeDaemonSetMisScheduled',\n",
       "       'query': 'kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.',\n",
       "        'summary': 'DaemonSet pods are misscheduled.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000279356,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.27525914Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeJobNotCompleted',\n",
       "       'query': 'time() - max by (namespace, job_name, cluster) (kube_job_status_start_time{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} and kube_job_status_active{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0) > 43200',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than {{ \"43200\" | humanizeDuration }} to complete.',\n",
       "        'summary': 'Job did not complete in time'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000780638,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.275539488Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'firing',\n",
       "       'name': 'KubeJobFailed',\n",
       "       'query': 'kube_job_failed{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeJobFailed.md',\n",
       "        'summary': 'Job failed to complete.'},\n",
       "       'alerts': [{'labels': {'alertname': 'KubeJobFailed',\n",
       "          'condition': 'true',\n",
       "          'container': 'kube-rbac-proxy-main',\n",
       "          'endpoint': 'https-main',\n",
       "          'job': 'kube-state-metrics',\n",
       "          'job_name': '8f12d1cd92648f537fdd45e5bb968f1e43e951c643d36aac0dfe5c579cd70a6',\n",
       "          'namespace': 'openshift-marketplace',\n",
       "          'service': 'kube-state-metrics',\n",
       "          'severity': 'warning'},\n",
       "         'annotations': {'description': 'Job openshift-marketplace/8f12d1cd92648f537fdd45e5bb968f1e43e951c643d36aac0dfe5c579cd70a6 failed to complete. Removing failed job after investigation should clear this alert.',\n",
       "          'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeJobFailed.md',\n",
       "          'summary': 'Job failed to complete.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2023-05-25T14:37:37.260184685Z',\n",
       "         'value': '1e+00'},\n",
       "        {'labels': {'alertname': 'KubeJobFailed',\n",
       "          'condition': 'true',\n",
       "          'container': 'kube-rbac-proxy-main',\n",
       "          'endpoint': 'https-main',\n",
       "          'job': 'kube-state-metrics',\n",
       "          'job_name': '97a4a032566a2db7a443ce49481ae24144238b46c0454e9b4c0d3794b781247',\n",
       "          'namespace': 'openshift-marketplace',\n",
       "          'service': 'kube-state-metrics',\n",
       "          'severity': 'warning'},\n",
       "         'annotations': {'description': 'Job openshift-marketplace/97a4a032566a2db7a443ce49481ae24144238b46c0454e9b4c0d3794b781247 failed to complete. Removing failed job after investigation should clear this alert.',\n",
       "          'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeJobFailed.md',\n",
       "          'summary': 'Job failed to complete.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2023-05-25T14:37:37.260184685Z',\n",
       "         'value': '1e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000765921,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.276321189Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeHpaReplicasMismatch',\n",
       "       'query': '(kube_horizontalpodautoscaler_status_desired_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) and (kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > kube_horizontalpodautoscaler_spec_min_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) and (kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} < kube_horizontalpodautoscaler_spec_max_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) and changes(kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[15m]) == 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes.',\n",
       "        'summary': 'HPA has not matched desired number of replicas.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001223433,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.277089385Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeHpaMaxedOut',\n",
       "       'query': 'kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} == kube_horizontalpodautoscaler_spec_max_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes.',\n",
       "        'summary': 'HPA is running at max replicas'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00033934,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.27831424Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.017772526,\n",
       "     'lastEvaluation': '2023-05-29T21:24:37.260882967Z'},\n",
       "    {'name': 'kubernetes-resources',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-b1bc5cc0-502b-46cc-a6cf-0e0e05097b28.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeCPUOvercommit',\n",
       "       'query': 'sum(namespace_cpu:kube_pod_container_resource_requests:sum) - (sum(kube_node_status_allocatable{resource=\"cpu\"}) - max(kube_node_status_allocatable{resource=\"cpu\"})) > 0 and (sum(kube_node_status_allocatable{resource=\"cpu\"}) - max(kube_node_status_allocatable{resource=\"cpu\"})) > 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'namespace': 'kube-system', 'severity': 'warning'},\n",
       "       'annotations': {'description': 'Cluster has overcommitted CPU resource requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.',\n",
       "        'summary': 'Cluster has overcommitted CPU resource requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000769479,\n",
       "       'lastEvaluation': '2023-05-29T21:24:16.205070792Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeMemoryOvercommit',\n",
       "       'query': 'sum(namespace_memory:kube_pod_container_resource_requests:sum) - (sum(kube_node_status_allocatable{resource=\"memory\"}) - max(kube_node_status_allocatable{resource=\"memory\"})) > 0 and (sum(kube_node_status_allocatable{resource=\"memory\"}) - max(kube_node_status_allocatable{resource=\"memory\"})) > 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'namespace': 'kube-system', 'severity': 'warning'},\n",
       "       'annotations': {'description': 'Cluster has overcommitted memory resource requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node failure.',\n",
       "        'summary': 'Cluster has overcommitted memory resource requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000791971,\n",
       "       'lastEvaluation': '2023-05-29T21:24:16.205841784Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeCPUQuotaOvercommit',\n",
       "       'query': 'sum(min without (resource) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",resource=~\"(cpu|requests.cpu)\",type=\"hard\"})) / sum(kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"cpu\"}) > 1.5',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Cluster has overcommitted CPU resource requests for Namespaces.',\n",
       "        'summary': 'Cluster has overcommitted CPU resource requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000691802,\n",
       "       'lastEvaluation': '2023-05-29T21:24:16.206635348Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeMemoryQuotaOvercommit',\n",
       "       'query': 'sum(min without (resource) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",resource=~\"(memory|requests.memory)\",type=\"hard\"})) / sum(kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"memory\"}) > 1.5',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Cluster has overcommitted memory resource requests for Namespaces.',\n",
       "        'summary': 'Cluster has overcommitted memory resource requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000505212,\n",
       "       'lastEvaluation': '2023-05-29T21:24:16.207328422Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeQuotaAlmostFull',\n",
       "       'query': 'kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",type=\"used\"} / ignoring (instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",type=\"hard\"} > 0) > 0.9 < 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.',\n",
       "        'summary': 'Namespace quota is going to be full.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000518336,\n",
       "       'lastEvaluation': '2023-05-29T21:24:16.207834996Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeQuotaFullyUsed',\n",
       "       'query': 'kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",type=\"used\"} / ignoring (instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",type=\"hard\"} > 0) == 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.',\n",
       "        'summary': 'Namespace quota is fully used.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000488651,\n",
       "       'lastEvaluation': '2023-05-29T21:24:16.208354604Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeQuotaExceeded',\n",
       "       'query': 'kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",type=\"used\"} / ignoring (instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",type=\"hard\"} > 0) > 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.',\n",
       "        'summary': 'Namespace quota has exceeded the limits.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000508286,\n",
       "       'lastEvaluation': '2023-05-29T21:24:16.208844637Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.004292981,\n",
       "     'lastEvaluation': '2023-05-29T21:24:16.205062607Z'},\n",
       "    {'name': 'kubernetes-storage',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-b1bc5cc0-502b-46cc-a6cf-0e0e05097b28.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubePersistentVolumeFillingUp',\n",
       "       'query': '(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) < 0.03 and kubelet_volume_stats_used_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_alerts_k8s_io_kube_persistent_volume_filling_up=\"disabled\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1',\n",
       "       'duration': 60,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md',\n",
       "        'summary': 'PersistentVolume is filling up.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001133583,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.854836494Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubePersistentVolumeFillingUp',\n",
       "       'query': '(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) < 0.15 and kubelet_volume_stats_used_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0 and predict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[6h], 4 * 24 * 3600) < 0 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_alerts_k8s_io_kube_persistent_volume_filling_up=\"disabled\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md',\n",
       "        'summary': 'PersistentVolume is filling up.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.004397318,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.855972542Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubePersistentVolumeInodesFillingUp',\n",
       "       'query': '(kubelet_volume_stats_inodes_free{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} / kubelet_volume_stats_inodes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) < 0.03 and kubelet_volume_stats_inodes_used{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_alerts_k8s_io_kube_persistent_volume_filling_up=\"disabled\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1',\n",
       "       'duration': 60,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} only has {{ $value | humanizePercentage }} free inodes.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md',\n",
       "        'summary': 'PersistentVolumeInodes are filling up.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000666775,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.860371503Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubePersistentVolumeInodesFillingUp',\n",
       "       'query': '(kubelet_volume_stats_inodes_free{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} / kubelet_volume_stats_inodes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) < 0.15 and kubelet_volume_stats_inodes_used{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0 and predict_linear(kubelet_volume_stats_inodes_free{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[6h], 4 * 24 * 3600) < 0 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1 unless on (namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_alerts_k8s_io_kube_persistent_volume_filling_up=\"disabled\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to run out of inodes within four days. Currently {{ $value | humanizePercentage }} of its inodes are free.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md',\n",
       "        'summary': 'PersistentVolumeInodes are filling up.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003185667,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.861039871Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubePersistentVolumeErrors',\n",
       "       'query': 'kube_persistentvolume_status_phase{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",phase=~\"Failed|Pending\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.',\n",
       "        'summary': 'PersistentVolume is having issues with provisioning.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000172466,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.86422676Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.009572791,\n",
       "     'lastEvaluation': '2023-05-29T21:24:34.854828288Z'},\n",
       "    {'name': 'kubernetes-system',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-b1bc5cc0-502b-46cc-a6cf-0e0e05097b28.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeClientErrors',\n",
       "       'query': '(sum by (cluster, instance, job, namespace) (rate(rest_client_requests_total{code=~\"5..\"}[5m])) / sum by (cluster, instance, job, namespace) (rate(rest_client_requests_total[5m]))) > 0.01',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'\",\n",
       "        'summary': 'Kubernetes API server client is experiencing errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002641522,\n",
       "       'lastEvaluation': '2023-05-29T21:24:20.417166697Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.002662262,\n",
       "     'lastEvaluation': '2023-05-29T21:24:20.417151979Z'},\n",
       "    {'name': 'kubernetes-system-apiserver',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-b1bc5cc0-502b-46cc-a6cf-0e0e05097b28.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeAggregatedAPIErrors',\n",
       "       'query': 'sum by (name, namespace, cluster) (increase(aggregator_unavailable_apiservice_total[10m])) > 4',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. It has appeared unavailable {{ $value | humanize }} times averaged over the past 10m.',\n",
       "        'summary': 'Kubernetes aggregated API has reported errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000392158,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.734781865Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeAggregatedAPIDown',\n",
       "       'query': '(1 - max by (name, namespace, cluster) (avg_over_time(aggregator_unavailable_apiservice[10m]))) * 100 < 85',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only {{ $value | humanize }}% available over the last 10m.',\n",
       "        'summary': 'Kubernetes aggregated API is down.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002592079,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.735176237Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeAPIDown',\n",
       "       'query': 'absent(up{job=\"apiserver\"} == 1)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'KubeAPI has disappeared from Prometheus target discovery.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeAPIDown.md',\n",
       "        'summary': 'Target disappeared from Prometheus target discovery.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000392399,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.737769308Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeAPITerminatedRequests',\n",
       "       'query': 'sum(rate(apiserver_request_terminations_total{job=\"apiserver\"}[10m])) / (sum(rate(apiserver_request_total{job=\"apiserver\"}[10m])) + sum(rate(apiserver_request_terminations_total{job=\"apiserver\"}[10m]))) > 0.2',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.',\n",
       "        'summary': 'The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.029561209,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.738164632Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.032965056,\n",
       "     'lastEvaluation': '2023-05-29T21:24:28.734769211Z'},\n",
       "    {'name': 'kubernetes-system-kubelet',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-b1bc5cc0-502b-46cc-a6cf-0e0e05097b28.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeNodeNotReady',\n",
       "       'query': 'kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",status=\"true\"} == 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $labels.node }} has been unready for more than 15 minutes.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeNodeNotReady.md',\n",
       "        'summary': 'Node is not ready.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000895866,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.933449569Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeNodeUnreachable',\n",
       "       'query': '(kube_node_spec_taint{effect=\"NoSchedule\",job=\"kube-state-metrics\",key=\"node.kubernetes.io/unreachable\"} unless ignoring (key, value) kube_node_spec_taint{job=\"kube-state-metrics\",key=~\"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn\"}) == 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $labels.node }} is unreachable and some workloads may be rescheduled.',\n",
       "        'summary': 'Node is unreachable.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000554293,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.934348572Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeletTooManyPods',\n",
       "       'query': 'count by (cluster, node) ((kube_pod_status_phase{job=\"kube-state-metrics\",phase=\"Running\"} == 1) * on (instance, pod, namespace, cluster) group_left (node) topk by (instance, pod, namespace, cluster) (1, kube_pod_info{job=\"kube-state-metrics\"})) / max by (cluster, node) (kube_node_status_capacity{job=\"kube-state-metrics\",resource=\"pods\"} != 1) > 0.95',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': \"Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity.\",\n",
       "        'summary': 'Kubelet is running at capacity.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.010388576,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.934904758Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeNodeReadinessFlapping',\n",
       "       'query': 'sum by (cluster, node) (changes(kube_node_status_condition{condition=\"Ready\",status=\"true\"}[15m])) > 2',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes.',\n",
       "        'summary': 'Node readiness status is flapping.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000361329,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.945296841Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeletPlegDurationHigh',\n",
       "       'query': 'node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile=\"0.99\"} >= 10',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}.',\n",
       "        'summary': 'Kubelet Pod Lifecycle Event Generator is taking too long to relist.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000208904,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.945659734Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeletPodStartUpLatencyHigh',\n",
       "       'query': 'histogram_quantile(0.99, sum by (cluster, instance, le) (rate(kubelet_pod_worker_duration_seconds_bucket{job=\"kubelet\",metrics_path=\"/metrics\"}[5m]))) * on (cluster, instance) group_left (node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"} > 60',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}.',\n",
       "        'summary': 'Kubelet Pod startup latency is too high.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.006033187,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.945870381Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeletClientCertificateRenewalErrors',\n",
       "       'query': 'increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $value | humanize }} errors in the last 5 minutes).',\n",
       "        'summary': 'Kubelet has failed to renew its client certificate.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000309443,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.951907235Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeletServerCertificateRenewalErrors',\n",
       "       'query': 'increase(kubelet_server_expiration_renew_errors[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $value | humanize }} errors in the last 5 minutes).',\n",
       "        'summary': 'Kubelet has failed to renew its server certificate.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000188114,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.952218331Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeletDown',\n",
       "       'query': 'absent(up{job=\"kubelet\",metrics_path=\"/metrics\"} == 1)',\n",
       "       'duration': 900,\n",
       "       'labels': {'namespace': 'kube-system', 'severity': 'critical'},\n",
       "       'annotations': {'description': 'Kubelet has disappeared from Prometheus target discovery.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeletDown.md',\n",
       "        'summary': 'Target disappeared from Prometheus target discovery.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000247867,\n",
       "       'lastEvaluation': '2023-05-29T21:24:17.952411965Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.019228878,\n",
       "     'lastEvaluation': '2023-05-29T21:24:17.933433809Z'},\n",
       "    {'name': 'node.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-b1bc5cc0-502b-46cc-a6cf-0e0e05097b28.yaml',\n",
       "     'rules': [{'name': 'node_namespace_pod:kube_pod_info:',\n",
       "       'query': 'topk by (cluster, namespace, pod) (1, max by (cluster, node, namespace, pod) (label_replace(kube_pod_info{job=\"kube-state-metrics\",node!=\"\"}, \"pod\", \"$1\", \"pod\", \"(.*)\")))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.004681482,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.462188308Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': ':node_memory_MemAvailable_bytes:sum',\n",
       "       'query': 'sum by (cluster) (node_memory_MemAvailable_bytes{job=\"node-exporter\"} or (node_memory_Buffers_bytes{job=\"node-exporter\"} + node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_MemFree_bytes{job=\"node-exporter\"} + node_memory_Slab_bytes{job=\"node-exporter\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000549795,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.466874449Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:node_cpu:ratio_rate5m',\n",
       "       'query': 'sum(rate(node_cpu_seconds_total{job=\"node-exporter\",mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[5m])) / count(sum by (cluster, instance, cpu) (node_cpu_seconds_total{job=\"node-exporter\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002728626,\n",
       "       'lastEvaluation': '2023-05-29T21:24:18.467425927Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.007980743,\n",
       "     'lastEvaluation': '2023-05-29T21:24:18.462177006Z'},\n",
       "    {'name': 'node-exporter',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-node-exporter-rules-58251d2c-8830-4347-83a2-da6cbad92a09.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'NodeFilesystemSpaceFillingUp',\n",
       "       'query': '(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 15 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md',\n",
       "        'summary': 'Filesystem is predicted to run out of space within the next 24 hours.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.013669803,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.75928679Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemSpaceFillingUp',\n",
       "       'query': '(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 10 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md',\n",
       "        'summary': 'Filesystem is predicted to run out of space within the next 4 hours.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.018994647,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.772959278Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemAlmostOutOfSpace',\n",
       "       'query': '(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 5 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md',\n",
       "        'summary': 'Filesystem has less than 5% space left.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002872798,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.791957111Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemAlmostOutOfSpace',\n",
       "       'query': '(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 3 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md',\n",
       "        'summary': 'Filesystem has less than 3% space left.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003040604,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.794832414Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemFilesFillingUp',\n",
       "       'query': '(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 40 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md',\n",
       "        'summary': 'Filesystem is predicted to run out of inodes within the next 24 hours.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.014271256,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.797875132Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemFilesFillingUp',\n",
       "       'query': '(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 20 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md',\n",
       "        'summary': 'Filesystem is predicted to run out of inodes within the next 4 hours.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.013885389,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.812148622Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemAlmostOutOfFiles',\n",
       "       'query': '(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 5 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md',\n",
       "        'summary': 'Filesystem has less than 5% inodes left.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002181387,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.826036846Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemAlmostOutOfFiles',\n",
       "       'query': '(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} * 100 < 3 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\",mountpoint!=\"\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md',\n",
       "        'summary': 'Filesystem has less than 3% inodes left.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001959568,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.828219866Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeNetworkReceiveErrs',\n",
       "       'query': 'rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.',\n",
       "        'summary': 'Network interface is reporting many receive errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000821065,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.830181368Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeNetworkTransmitErrs',\n",
       "       'query': 'rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.',\n",
       "        'summary': 'Network interface is reporting many transmit errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.0007488,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.831003525Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeHighNumberConntrackEntriesUsed',\n",
       "       'query': '(node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $value | humanizePercentage }} of conntrack entries are used.',\n",
       "        'summary': 'Number of conntrack are getting close to the limit.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000253989,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.831753417Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeTextFileCollectorScrapeError',\n",
       "       'query': 'node_textfile_scrape_error{job=\"node-exporter\"} == 1',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Node Exporter text file collector failed to scrape.',\n",
       "        'summary': 'Node Exporter text file collector failed to scrape.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000119034,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.832008418Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeClockSkewDetected',\n",
       "       'query': '(node_timex_offset_seconds{job=\"node-exporter\"} > 0.05 and deriv(node_timex_offset_seconds{job=\"node-exporter\"}[5m]) >= 0) or (node_timex_offset_seconds{job=\"node-exporter\"} < -0.05 and deriv(node_timex_offset_seconds{job=\"node-exporter\"}[5m]) <= 0)',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host.',\n",
       "        'summary': 'Clock skew detected.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000495584,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.832128263Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeClockNotSynchronising',\n",
       "       'query': 'min_over_time(node_timex_sync_status{job=\"node-exporter\"}[5m]) == 0 and node_timex_maxerror_seconds{job=\"node-exporter\"} >= 16',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeClockNotSynchronising.md',\n",
       "        'summary': 'Clock not synchronising.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000263686,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.832625209Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeRAIDDegraded',\n",
       "       'query': 'node_md_disks_required{device=~\"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\",job=\"node-exporter\"} - ignoring (state) (node_md_disks{device=~\"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\",job=\"node-exporter\",state=\"active\"}) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': \"RAID array '{{ $labels.device }}' on {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\",\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeRAIDDegraded.md',\n",
       "        'summary': 'RAID Array is degraded'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000263766,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.832890018Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeRAIDDiskFailure',\n",
       "       'query': 'node_md_disks{device=~\"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\",job=\"node-exporter\",state=\"failed\"} > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"At least one device in RAID array on {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.\",\n",
       "        'summary': 'Failed device in RAID array'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000148099,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.833154956Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFileDescriptorLimit',\n",
       "       'query': '(node_filefd_allocated{job=\"node-exporter\"} * 100 / node_filefd_maximum{job=\"node-exporter\"} > 70)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'File descriptors limit at {{ $labels.instance }} is currently at {{ printf \"%.2f\" $value }}%.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md',\n",
       "        'summary': 'Kernel is predicted to exhaust file descriptors limit soon.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000236445,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.833304387Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFileDescriptorLimit',\n",
       "       'query': '(node_filefd_allocated{job=\"node-exporter\"} * 100 / node_filefd_maximum{job=\"node-exporter\"} > 90)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'File descriptors limit at {{ $labels.instance }} is currently at {{ printf \"%.2f\" $value }}%.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md',\n",
       "        'summary': 'Kernel is predicted to exhaust file descriptors limit soon.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00022909,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.833541795Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.074494626,\n",
       "     'lastEvaluation': '2023-05-29T21:24:32.759278073Z'},\n",
       "    {'name': 'node-exporter.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-node-exporter-rules-58251d2c-8830-4347-83a2-da6cbad92a09.yaml',\n",
       "     'rules': [{'name': 'instance:node_num_cpu:sum',\n",
       "       'query': 'count without (cpu, mode) (node_cpu_seconds_total{job=\"node-exporter\",mode=\"idle\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000898462,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.772551883Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_cpu_utilisation:rate1m',\n",
       "       'query': '1 - avg without (cpu) (sum without (mode) (rate(node_cpu_seconds_total{job=\"node-exporter\",mode=~\"idle|iowait|steal\"}[1m])))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000963343,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.773456045Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_load1_per_cpu:ratio',\n",
       "       'query': '(node_load1{job=\"node-exporter\"} / instance:node_num_cpu:sum{job=\"node-exporter\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00019684,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.774420801Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_memory_utilisation:ratio',\n",
       "       'query': '1 - ((node_memory_MemAvailable_bytes{job=\"node-exporter\"} or (node_memory_Buffers_bytes{job=\"node-exporter\"} + node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_MemFree_bytes{job=\"node-exporter\"} + node_memory_Slab_bytes{job=\"node-exporter\"})) / node_memory_MemTotal_bytes{job=\"node-exporter\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000529617,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.774618553Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_vmstat_pgmajfault:rate1m',\n",
       "       'query': 'rate(node_vmstat_pgmajfault{job=\"node-exporter\"}[1m])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000120728,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.775149222Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance_device:node_disk_io_time_seconds:rate1m',\n",
       "       'query': 'rate(node_disk_io_time_seconds_total{device=~\"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\",job=\"node-exporter\"}[1m])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000182895,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.775270831Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance_device:node_disk_io_time_weighted_seconds:rate1m',\n",
       "       'query': 'rate(node_disk_io_time_weighted_seconds_total{device=~\"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\",job=\"node-exporter\"}[1m])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000161103,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.775454748Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_network_receive_bytes_excluding_lo:rate1m',\n",
       "       'query': 'sum without (device) (rate(node_network_receive_bytes_total{device!=\"lo\",job=\"node-exporter\"}[1m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000383061,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.775616913Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_network_transmit_bytes_excluding_lo:rate1m',\n",
       "       'query': 'sum without (device) (rate(node_network_transmit_bytes_total{device!=\"lo\",job=\"node-exporter\"}[1m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000328829,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.776001657Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_network_receive_drop_excluding_lo:rate1m',\n",
       "       'query': 'sum without (device) (rate(node_network_receive_drop_total{device!=\"lo\",job=\"node-exporter\"}[1m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000280508,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.776331578Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_network_transmit_drop_excluding_lo:rate1m',\n",
       "       'query': 'sum without (device) (rate(node_network_transmit_drop_total{device!=\"lo\",job=\"node-exporter\"}[1m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000364787,\n",
       "       'lastEvaluation': '2023-05-29T21:24:34.776612968Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.00443576,\n",
       "     'lastEvaluation': '2023-05-29T21:24:34.772544279Z'},\n",
       "    {'name': 'prometheus',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-prometheus-rules-5abe4ef1-5f0a-4999-8bcd-592fdd8df5ae.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'PrometheusBadConfig',\n",
       "       'query': 'max_over_time(prometheus_config_last_reload_successful{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.',\n",
       "        'summary': 'Failed Prometheus configuration reload.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000311336,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.475900916Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusNotificationQueueRunningFull',\n",
       "       'query': '(predict_linear(prometheus_notifications_queue_length{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m], 60 * 30) > min_over_time(prometheus_notifications_queue_capacity{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]))',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.',\n",
       "        'summary': 'Prometheus alert notification queue predicted to run full in less than 30m.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000220775,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.476213686Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusErrorSendingAlertsToSomeAlertmanagers',\n",
       "       'query': '(rate(prometheus_notifications_errors_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) / rate(prometheus_notifications_sent_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])) * 100 > 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ printf \"%.1f\" $value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.',\n",
       "        'summary': 'Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000221076,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.476435453Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusNotConnectedToAlertmanagers',\n",
       "       'query': 'max_over_time(prometheus_notifications_alertmanagers_discovered{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) < 1',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.',\n",
       "        'summary': 'Prometheus is not connected to any Alertmanagers.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000114105,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.476657441Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusTSDBReloadsFailing',\n",
       "       'query': 'increase(prometheus_tsdb_reloads_failures_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[3h]) > 0',\n",
       "       'duration': 14400,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.',\n",
       "        'summary': 'Prometheus has issues reloading blocks from disk.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000202981,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.476772428Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusTSDBCompactionsFailing',\n",
       "       'query': 'increase(prometheus_tsdb_compactions_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[3h]) > 0',\n",
       "       'duration': 14400,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.',\n",
       "        'summary': 'Prometheus has issues compacting blocks.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000170501,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.476976661Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusNotIngestingSamples',\n",
       "       'query': '(rate(prometheus_tsdb_head_samples_appended_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) <= 0 and (sum without (scrape_job) (prometheus_target_metadata_cache_entries{job=~\"prometheus-k8s|prometheus-user-workload\"}) > 0 or sum without (rule_group) (prometheus_rule_group_rules{job=~\"prometheus-k8s|prometheus-user-workload\"}) > 0))',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.',\n",
       "        'summary': 'Prometheus is not ingesting samples.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001426375,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.477148175Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusDuplicateTimestamps',\n",
       "       'query': 'rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with different values but duplicated timestamp.',\n",
       "        'summary': 'Prometheus is dropping samples with duplicate timestamps.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000114746,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.478575611Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOutOfOrderTimestamps',\n",
       "       'query': 'rate(prometheus_target_scrapes_sample_out_of_order_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with timestamps arriving out of order.',\n",
       "        'summary': 'Prometheus drops samples with out-of-order timestamps.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 8.991e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.478691279Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusRemoteStorageFailures',\n",
       "       'query': '((rate(prometheus_remote_storage_failed_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])) / ((rate(prometheus_remote_storage_failed_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])) + (rate(prometheus_remote_storage_succeeded_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) or rate(prometheus_remote_storage_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])))) * 100 > 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf \"%.1f\" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}',\n",
       "        'summary': 'Prometheus fails to send samples to remote storage.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000433365,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.47878201Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusRemoteWriteBehind',\n",
       "       'query': '(max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) - ignoring (remote_name, url) group_right () max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])) > 120',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf \"%.1f\" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.',\n",
       "        'summary': 'Prometheus remote write is behind.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00018517,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.479216608Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusRemoteWriteDesiredShards',\n",
       "       'query': '(max_over_time(prometheus_remote_storage_shards_desired{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > max_over_time(prometheus_remote_storage_shards_max{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]))',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance=\"%s\",job=~\"prometheus-k8s|prometheus-user-workload\"}` $labels.instance | query | first | value }}.',\n",
       "        'summary': 'Prometheus remote write desired shards calculation wants to run more than configured max shards.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000158799,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.479402709Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusRuleFailures',\n",
       "       'query': 'increase(prometheus_rule_evaluation_failures_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf \"%.0f\" $value }} rules in the last 5m.',\n",
       "        'summary': 'Prometheus is failing rule evaluations.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001027575,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.47956259Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusMissingRuleEvaluations',\n",
       "       'query': 'increase(prometheus_rule_group_iterations_missed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf \"%.0f\" $value }} rule group evaluations in the last 5m.',\n",
       "        'summary': 'Prometheus is missing rule evaluations due to slow rule group evaluation.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000829852,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.480591888Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusTargetLimitHit',\n",
       "       'query': 'increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf \"%.0f\" $value }} targets because the number of targets exceeded the configured target_limit.',\n",
       "        'summary': 'Prometheus has dropped targets because some scrape configs have exceeded the targets limit.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000136517,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.481422862Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusLabelLimitHit',\n",
       "       'query': 'increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf \"%.0f\" $value }} targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.',\n",
       "        'summary': 'Prometheus has dropped targets because some scrape configs have exceeded the labels limit.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000101141,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.481560431Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusScrapeBodySizeLimitHit',\n",
       "       'query': 'increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf \"%.0f\" $value }} scrapes in the last 5m because some targets exceeded the configured body_size_limit.',\n",
       "        'summary': 'Prometheus has dropped some targets that exceeded body size limit.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000154971,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.481662734Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusScrapeSampleLimitHit',\n",
       "       'query': 'increase(prometheus_target_scrapes_exceeded_sample_limit_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf \"%.0f\" $value }} scrapes in the last 5m because some targets exceeded the configured sample_limit.',\n",
       "        'summary': 'Prometheus has failed scrapes that have exceeded the configured sample limit.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.0972e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.481818717Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusTargetSyncFailure',\n",
       "       'query': 'increase(prometheus_target_sync_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[30m]) > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': '{{ printf \"%.0f\" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}} have failed to sync because invalid configuration was supplied.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusTargetSyncFailure.md',\n",
       "        'summary': 'Prometheus has failed to sync targets.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001465118,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.481910811Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusHighQueryLoad',\n",
       "       'query': 'avg_over_time(prometheus_engine_queries{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) / max_over_time(prometheus_engine_queries_concurrent_max{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0.8',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} query API has less than 20% available capacity in its query engine for the last 15 minutes.',\n",
       "        'summary': 'Prometheus is reaching its maximum capacity serving concurrent requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000319361,\n",
       "       'lastEvaluation': '2023-05-29T21:24:37.483377121Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.007808079,\n",
       "     'lastEvaluation': '2023-05-29T21:24:37.475891419Z'},\n",
       "    {'name': 'thanos-sidecar',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-thanos-sidecar-rules-9c02c23a-3226-406a-b083-d94d7d8e8ae1.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ThanosSidecarBucketOperationsFailed',\n",
       "       'query': 'sum by (namespace, job, instance) (rate(thanos_objstore_bucket_operation_failures_total{job=~\"prometheus-(k8s|user-workload)-thanos-sidecar\"}[5m])) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}} bucket operations are failing',\n",
       "        'summary': 'Thanos Sidecar bucket operations are failing'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000494351,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.295376205Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ThanosSidecarNoConnectionToStartedPrometheus',\n",
       "       'query': 'thanos_sidecar_prometheus_up{job=~\"prometheus-(k8s|user-workload)-thanos-sidecar\"} == 0 and on (namespace, pod) prometheus_tsdb_data_replay_duration_seconds != 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}} is unhealthy.',\n",
       "        'summary': 'Thanos Sidecar cannot access Prometheus, even though Prometheus seems healthy and has reloaded WAL.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000225544,\n",
       "       'lastEvaluation': '2023-05-29T21:24:28.29587248Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000751754,\n",
       "     'lastEvaluation': '2023-05-29T21:24:28.295349736Z'},\n",
       "    {'name': 'config-reloaders',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-operator-rules-07b76246-d0ec-48c9-bf58-502274a18658.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ConfigReloaderSidecarErrors',\n",
       "       'query': 'max_over_time(reloader_last_reload_successful{namespace=~\".+\"}[5m]) == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.\\nAs a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.',\n",
       "        'summary': 'config-reloader sidecar has not had a successful reload for 10m'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000390926,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.432199177Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000403219,\n",
       "     'lastEvaluation': '2023-05-29T21:24:22.43218984Z'},\n",
       "    {'name': 'prometheus-operator',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-operator-rules-07b76246-d0ec-48c9-bf58-502274a18658.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorListErrors',\n",
       "       'query': '(sum by (controller, namespace) (rate(prometheus_operator_list_operations_failed_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[10m])) / sum by (controller, namespace) (rate(prometheus_operator_list_operations_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[10m]))) > 0.4',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.',\n",
       "        'summary': 'Errors while performing list operations in controller.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000668668,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.612576848Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorWatchErrors',\n",
       "       'query': '(sum by (controller, namespace) (rate(prometheus_operator_watch_operations_failed_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m])) / sum by (controller, namespace) (rate(prometheus_operator_watch_operations_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]))) > 0.4',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Errors while performing watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.',\n",
       "        'summary': 'Errors while performing watch operations in controller.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000335401,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.613247791Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorSyncFailed',\n",
       "       'query': 'min_over_time(prometheus_operator_syncs{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\",status=\"failed\"}[5m]) > 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $value }} objects.',\n",
       "        'summary': 'Last controller reconciliation failed'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000141736,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.613584774Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorReconcileErrors',\n",
       "       'query': '(sum by (controller, namespace) (rate(prometheus_operator_reconcile_errors_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]))) / (sum by (controller, namespace) (rate(prometheus_operator_reconcile_operations_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]))) > 0.1',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $value | humanizePercentage }} of reconciling operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.',\n",
       "        'summary': 'Errors while reconciling controller.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000256543,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.613727413Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorNodeLookupErrors',\n",
       "       'query': 'rate(prometheus_operator_node_address_lookup_errors_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]) > 0.1',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.',\n",
       "        'summary': 'Errors while reconciling Prometheus.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000110738,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.613984968Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorNotReady',\n",
       "       'query': 'min by (controller, namespace) (max_over_time(prometheus_operator_ready{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]) == 0)',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"Prometheus operator in {{ $labels.namespace }} namespace isn't ready to reconcile {{ $labels.controller }} resources.\",\n",
       "        'summary': 'Prometheus operator not ready'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000128703,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.614097159Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorRejectedResources',\n",
       "       'query': 'min_over_time(prometheus_operator_managed_resources{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\",state=\"rejected\"}[5m]) > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf \"%0.0f\" $value }} {{ $labels.controller }}/{{ $labels.resource }} resources.',\n",
       "        'summary': 'Resources rejected by Prometheus operator'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000120046,\n",
       "       'lastEvaluation': '2023-05-29T21:24:22.614226743Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001796481,\n",
       "     'lastEvaluation': '2023-05-29T21:24:22.612552432Z'},\n",
       "    {'name': 'telemeter.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-telemetry-eb4786fb-eb99-47e4-a3e6-25a11b8bdc63.yaml',\n",
       "     'rules': [{'name': 'cluster:telemetry_selected_series:count',\n",
       "       'query': 'max(federate_samples - federate_filtered_samples)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000264668,\n",
       "       'lastEvaluation': '2023-05-29T21:24:35.695093193Z',\n",
       "       'type': 'recording'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'TelemeterClientFailures',\n",
       "       'query': 'sum by (namespace) (rate(federate_requests_failed_total{job=\"telemeter-client\"}[15m])) / sum by (namespace) (rate(federate_requests_total{job=\"telemeter-client\"}[15m])) > 0.2',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The telemeter client in namespace {{ $labels.namespace }} fails {{ $value | humanize }} of the requests to the telemeter service.\\nCheck the logs of the telemeter-client pod with the following command:\\noc logs -n openshift-monitoring deployment.apps/telemeter-client -c telemeter-client\\nIf the telemeter client fails to authenticate with the telemeter service, make sure that the global pull secret is up to date, see https://docs.openshift.com/container-platform/latest/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets for more details.',\n",
       "        'summary': 'Telemeter client fails to send metrics'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000199446,\n",
       "       'lastEvaluation': '2023-05-29T21:24:35.695359785Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.00047786,\n",
       "     'lastEvaluation': '2023-05-29T21:24:35.695083305Z'},\n",
       "    {'name': 'thanos-query',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-thanos-querier-cd8f7625-cfe8-42ca-a92d-a08dced8ed41.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ThanosQueryHttpRequestQueryErrorRateHigh',\n",
       "       'query': '(sum by (namespace, job) (rate(http_requests_total{code=~\"5..\",handler=\"query\",job=\"thanos-querier\"}[5m])) / sum by (namespace, job) (rate(http_requests_total{handler=\"query\",job=\"thanos-querier\"}[5m]))) * 100 > 5',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to handle {{$value | humanize}}% of \"query\" requests.',\n",
       "        'summary': 'Thanos Query is failing to handle requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000352873,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.947518065Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ThanosQueryHttpRequestQueryRangeErrorRateHigh',\n",
       "       'query': '(sum by (namespace, job) (rate(http_requests_total{code=~\"5..\",handler=\"query_range\",job=\"thanos-querier\"}[5m])) / sum by (namespace, job) (rate(http_requests_total{handler=\"query_range\",job=\"thanos-querier\"}[5m]))) * 100 > 5',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to handle {{$value | humanize}}% of \"query_range\" requests.',\n",
       "        'summary': 'Thanos Query is failing to handle requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000229562,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.947872211Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ThanosQueryGrpcServerErrorRate',\n",
       "       'query': '(sum by (namespace, job) (rate(grpc_server_handled_total{grpc_code=~\"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\",job=\"thanos-querier\"}[5m])) / sum by (namespace, job) (rate(grpc_server_started_total{job=\"thanos-querier\"}[5m])) * 100 > 5)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to handle {{$value | humanize}}% of requests.',\n",
       "        'summary': 'Thanos Query is failing to handle requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001107695,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.948102775Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ThanosQueryGrpcClientErrorRate',\n",
       "       'query': '(sum by (namespace, job) (rate(grpc_client_handled_total{grpc_code!=\"OK\",job=\"thanos-querier\"}[5m])) / sum by (namespace, job) (rate(grpc_client_started_total{job=\"thanos-querier\"}[5m]))) * 100 > 5',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to send {{$value | humanize}}% of requests.',\n",
       "        'summary': 'Thanos Query is failing to send requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000346753,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.949211562Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ThanosQueryHighDNSFailures',\n",
       "       'query': '(sum by (namespace, job) (rate(thanos_query_store_apis_dns_failures_total{job=\"thanos-querier\"}[5m])) / sum by (namespace, job) (rate(thanos_query_store_apis_dns_lookups_total{job=\"thanos-querier\"}[5m]))) * 100 > 1',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Query {{$labels.job}} in {{$labels.namespace}} have {{$value | humanize}}% of failing DNS queries for store endpoints.',\n",
       "        'summary': 'Thanos Query is having high number of DNS failures.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000251784,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.949559507Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ThanosQueryOverload',\n",
       "       'query': '(max_over_time(thanos_query_concurrent_gate_queries_max[5m]) - avg_over_time(thanos_query_concurrent_gate_queries_in_flight[5m]) < 1)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Query {{$labels.job}} in {{$labels.namespace}} has been overloaded for more than 15 minutes. This may be a symptom of excessive simultanous complex requests, low performance of the Prometheus API, or failures within these components. Assess the health of the Thanos query instances, the connnected Prometheus instances, look for potential senders of these requests and then contact support.',\n",
       "        'summary': 'Thanos query reaches its maximum capacity serving concurrent requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000187012,\n",
       "       'lastEvaluation': '2023-05-29T21:24:32.949812373Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.002486912,\n",
       "     'lastEvaluation': '2023-05-29T21:24:32.947514787Z'},\n",
       "    {'name': 'multus-admission-controller-monitor-service.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-multus-prometheus-k8s-rules-93dc998a-9e10-40d6-be47-61a36ca5ad04.yaml',\n",
       "     'rules': [{'name': 'cluster:network_attachment_definition_enabled_instance_up:max',\n",
       "       'query': 'max by (networks) (network_attachment_definition_enabled_instance_up)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000376688,\n",
       "       'lastEvaluation': '2023-05-29T21:24:25.714963118Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:network_attachment_definition_instances:max',\n",
       "       'query': 'max by (networks) (network_attachment_definition_instances)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000228761,\n",
       "       'lastEvaluation': '2023-05-29T21:24:25.715342361Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000618525,\n",
       "     'lastEvaluation': '2023-05-29T21:24:25.714955703Z'},\n",
       "    {'name': 'olm.csv_abnormal.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-operator-lifecycle-manager-olm-alert-rules-33fa3a63-8eab-4b23-aafa-2d26582efed2.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'CsvAbnormalFailedOver2Min',\n",
       "       'query': 'csv_abnormal{phase=~\"^Failed$\"}',\n",
       "       'duration': 120,\n",
       "       'labels': {'namespace': '{{ $labels.namespace }}',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'Fires whenever a CSV has been in the failed phase for more than 2 minutes.',\n",
       "        'message': 'Failed to install Operator {{ $labels.name }} version {{ $labels.version }}. Reason-{{ $labels.reason }}',\n",
       "        'summary': 'CSV failed for over 2 minutes'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00020679,\n",
       "       'lastEvaluation': '2023-05-29T21:24:43.295264556Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'firing',\n",
       "       'name': 'CsvAbnormalOver30Min',\n",
       "       'query': 'csv_abnormal{phase=~\"(^Replacing$|^Pending$|^Deleting$|^Unknown$)\"}',\n",
       "       'duration': 1800,\n",
       "       'labels': {'namespace': '{{ $labels.namespace }}',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'Fires whenever a CSV is in the Replacing, Pending, Deleting, or Unkown phase for more than 30 minutes.',\n",
       "        'message': 'Failed to install Operator {{ $labels.name }} version {{ $labels.version }}. Phase-{{ $labels.phase }} Reason-{{ $labels.reason }}',\n",
       "        'summary': 'CSV abnormal for over 30 minutes'},\n",
       "       'alerts': [{'labels': {'alertname': 'CsvAbnormalOver30Min',\n",
       "          'container': 'olm-operator',\n",
       "          'endpoint': 'https-metrics',\n",
       "          'exported_namespace': 'sealed-secrets',\n",
       "          'instance': '10.129.1.254:8443',\n",
       "          'job': 'olm-operator-metrics',\n",
       "          'name': 'sealed-secrets-operator-helm.v0.0.2',\n",
       "          'namespace': 'openshift-operator-lifecycle-manager',\n",
       "          'phase': 'Pending',\n",
       "          'pod': 'olm-operator-866df97d4b-2477w',\n",
       "          'reason': 'RequirementsNotMet',\n",
       "          'service': 'olm-operator-metrics',\n",
       "          'severity': 'warning',\n",
       "          'version': '0.0.2'},\n",
       "         'annotations': {'description': 'Fires whenever a CSV is in the Replacing, Pending, Deleting, or Unkown phase for more than 30 minutes.',\n",
       "          'message': 'Failed to install Operator sealed-secrets-operator-helm.v0.0.2 version 0.0.2. Phase-Pending Reason-RequirementsNotMet',\n",
       "          'summary': 'CSV abnormal for over 30 minutes'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2023-05-27T19:07:43.294200587Z',\n",
       "         'value': '1e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000448393,\n",
       "       'lastEvaluation': '2023-05-29T21:24:43.295472759Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000678668,\n",
       "     'lastEvaluation': '2023-05-29T21:24:43.295246552Z'},\n",
       "    {'name': 'olm.installplan.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-operator-lifecycle-manager-olm-alert-rules-33fa3a63-8eab-4b23-aafa-2d26582efed2.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'InstallPlanStepAppliedWithWarnings',\n",
       "       'query': 'sum(increase(installplan_warnings_total[5m])) > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Fires whenever the API server returns a warning when attempting to modify an operator.',\n",
       "        'message': 'The API server returned a warning during installation or upgrade of an operator. An Event with reason \"AppliedWithWarnings\" has been created with complete details, including a reference to the InstallPlan step that generated the warning.',\n",
       "        'summary': 'API returned a warning when modifying an operator'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000311336,\n",
       "       'lastEvaluation': '2023-05-29T21:24:36.007868662Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000322878,\n",
       "     'lastEvaluation': '2023-05-29T21:24:36.007860306Z'},\n",
       "    {'name': 'cluster-network-operator-master.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-ovn-kubernetes-master-rules-dc1fe25f-78da-4cc0-b52e-a83b70be4bb3.yaml',\n",
       "     'rules': [{'name': 'cluster:ovnkube_master_egress_routing_via_host:max',\n",
       "       'query': 'max(ovnkube_master_egress_routing_via_host)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000344039,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.99898922Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:ovn_db_nbdb_not_cluster_member:abs',\n",
       "       'query': 'abs(count(ovn_db_cluster_server_status{db_name=\"OVN_Northbound\",server_status=\"cluster member\"}) - 3)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000223772,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.999335863Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:ovn_db_sbdb_not_cluster_member:abs',\n",
       "       'query': 'abs(count(ovn_db_cluster_server_status{db_name=\"OVN_Southbound\",server_status=\"cluster member\"}) - 3)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000160372,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.999560817Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:ovn_db_nbdb_missing_inbound_connections:abs',\n",
       "       'query': 'abs(sum(ovn_db_cluster_inbound_connections_total{db_name=\"OVN_Northbound\"}) - (3 * (3 - 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000242176,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.999722181Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:ovn_db_sbdb_missing_inbound_connections:abs',\n",
       "       'query': 'abs(sum(ovn_db_cluster_inbound_connections_total{db_name=\"OVN_Southbound\"}) - (3 * (3 - 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000189817,\n",
       "       'lastEvaluation': '2023-05-29T21:24:39.999967032Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:ovn_db_nbdb_missing_outbound_connections:abs',\n",
       "       'query': 'abs(sum(ovn_db_cluster_outbound_connections_total{db_name=\"OVN_Northbound\"}) - (3 * (3 - 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000164008,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.000157831Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:ovn_db_sbdb_missing_outbound_connections:abs',\n",
       "       'query': 'abs(sum(ovn_db_cluster_outbound_connections_total{db_name=\"OVN_Southbound\"}) - (3 * (3 - 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000174499,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.000323322Z',\n",
       "       'type': 'recording'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NoRunningOvnMaster',\n",
       "       'query': 'absent(up{job=\"ovnkube-master\",namespace=\"openshift-ovn-kubernetes\"} == 1)',\n",
       "       'duration': 300,\n",
       "       'labels': {'namespace': 'openshift-ovn-kubernetes',\n",
       "        'severity': 'critical'},\n",
       "       'annotations': {'description': 'Networking control plane is degraded. Networking configuration updates applied to the cluster will not be\\nimplemented while there are no OVN Kubernetes pods.\\n',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NoRunningOvnMaster.md',\n",
       "        'summary': 'There is no running ovn-kubernetes master.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000117552,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.000499624Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NoOvnMasterLeader',\n",
       "       'query': 'max by (namespace) (max_over_time(ovnkube_master_leader[5m])) == 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Networking control plane is degraded. Networking configuration updates applied to the cluster will not be\\nimplemented while there is no OVN Kubernetes leader. Existing workloads should continue to have connectivity.\\nOVN-Kubernetes control plane is not functional.\\n',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NoOvnMasterLeader.md',\n",
       "        'summary': 'There is no ovn-kubernetes master leader.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000187563,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.000618238Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'V4SubnetAllocationThresholdExceeded',\n",
       "       'query': 'ovnkube_master_allocated_v4_host_subnets / ovnkube_master_num_v4_host_subnets > 0.8',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'More than 80% of IPv4 subnets are used. Insufficient IPv4 subnets could degrade provisioning of workloads.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/V4SubnetAllocationThresholdExceeded.md',\n",
       "        'summary': 'More than 80% of v4 subnets available to assign to the nodes are allocated. Current v4 subnet allocation percentage is {{ $value | humanizePercentage }}.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000155302,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.000807624Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'V6SubnetAllocationThresholdExceeded',\n",
       "       'query': 'ovnkube_master_allocated_v6_host_subnets / ovnkube_master_num_v6_host_subnets > 0.8',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'More than 80% of IPv6 subnets are used. Insufficient IPv6 subnets could degrade provisioning of workloads.',\n",
       "        'summary': 'More than 80% of the v6 subnets available to assign to the nodes are allocated. Current v6 subnet allocation percentage is {{ $value | humanizePercentage }}.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000157427,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.000964148Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NorthboundStale',\n",
       "       'query': 'time() - max_over_time(ovnkube_master_nb_e2e_timestamp[5m]) > 120',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Networking control plane is degraded. Networking configuration updates applied to the cluster will not be\\nimplemented. Existing workloads should continue to have connectivity. OVN-Kubernetes control plane and/or\\nOVN northbound database may not be functional.\\n',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NorthboundStaleAlert.md',\n",
       "        'summary': 'ovn-kubernetes has not written anything to the northbound database for too long.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000139883,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.001123208Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SouthboundStale',\n",
       "       'query': 'max_over_time(ovnkube_master_nb_e2e_timestamp[5m]) - max_over_time(ovnkube_master_sb_e2e_timestamp[5m]) > 120',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"Networking control plane is degraded. Networking configuration updates may not be applied to the cluster or\\ntaking a long time to apply. This usually means there is a large load on OVN component 'northd' or it is not\\nfunctioning.\\n\",\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/SouthboundStaleAlert.md',\n",
       "        'summary': 'ovn-northd has not successfully synced any changes to the southbound DB for too long.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.0001641,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.001264353Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNorthboundDatabaseClusterIDError',\n",
       "       'query': 'count by (namespace) (count by (cluster_id, namespace) (min_over_time(ovn_db_cluster_id{db_name=\"OVN_Northbound\"}[5m]))) > 1',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'More than one OVN northbound database cluster ID indicates degraded OVN database high availability and possible database split brain.',\n",
       "        'summary': 'Multiple OVN northbound database cluster IDs exist.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000167445,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.001429745Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesSouthboundDatabaseClusterIDError',\n",
       "       'query': 'count by (namespace) (count by (cluster_id, namespace) (min_over_time(ovn_db_cluster_id{db_name=\"OVN_Southbound\"}[5m]))) > 1',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'More than one OVN southbound database cluster ID indicates degraded OVN database high availability and possible database split brain.',\n",
       "        'summary': 'Multiple OVN southbound database cluster IDs exist.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000190458,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.001598613Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNorthboundDatabaseTermLag',\n",
       "       'query': 'max by (namespace) (max_over_time(ovn_db_cluster_term{db_name=\"OVN_Northbound\"}[5m])) - min by (namespace) (max_over_time(ovn_db_cluster_term{db_name=\"OVN_Northbound\"}[5m])) > 0',\n",
       "       'duration': 1500,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN northbound database(s) RAFT term have not been equal which may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN northbound databases RAFT term have not been equal for a period of time.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000274216,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.001790354Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesSouthboundDatabaseTermLag',\n",
       "       'query': 'max by (namespace) (max_over_time(ovn_db_cluster_term{db_name=\"OVN_Southbound\"}[5m])) - min by (namespace) (max_over_time(ovn_db_cluster_term{db_name=\"OVN_Southbound\"}[5m])) > 0',\n",
       "       'duration': 1500,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN southbound database(s) RAFT term have not been equal which may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN southbound databases RAFT term have not been equal for a period of time.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000156826,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.002065832Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNorthboundDatabaseLeaderError',\n",
       "       'query': 'count by (namespace) (max_over_time(ovn_db_cluster_server_role{db_name=\"OVN_Northbound\",server_role=\"leader\"}[5m])) == 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'OVN northbound database(s) have no RAFT leader. Networking control plane is degraded.',\n",
       "        'summary': 'OVN northbound database(s) have no RAFT leader'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000129493,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.0022235Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesSouthboundDatabaseLeaderError',\n",
       "       'query': 'count by (namespace) (max_over_time(ovn_db_cluster_server_role{db_name=\"OVN_Southbound\",server_role=\"leader\"}[5m])) == 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'OVN southbound database(s) have no leader. Networking control plane is degraded.',\n",
       "        'summary': 'OVN southbound database(s) have no RAFT leader'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000155653,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.002354456Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNorthboundDatabaseMultipleLeadersError',\n",
       "       'query': 'count by (leader, namespace) (min_over_time(ovn_db_cluster_server_role{db_name=\"OVN_Northbound\",server_role=\"leader\"}[1m])) > 1',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'OVN northbound database(s) have multiple RAFT leaders which may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN northbound database(s) have multiple RAFT leaders'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.2794e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.002511291Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesSouthboundDatabaseMultipleLeadersError',\n",
       "       'query': 'count by (leader, namespace) (min_over_time(ovn_db_cluster_server_role{db_name=\"OVN_Southbound\",server_role=\"leader\"}[1m])) > 1',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'OVN southbound database(s) have multiple RAFT leaders which may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN southbound database(s) have multiple RAFT leaders'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000141758,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.002604887Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNorthboundDatabaseClusterMemberError',\n",
       "       'query': 'min_over_time(cluster:ovn_db_nbdb_not_cluster_member:abs[5m]) != 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'namespace': 'openshift-ovn-kubernetes',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN northbound database server(s) has not been a RAFT cluster member for a period of time which may indicate degraded OVN database high availability cluster.',\n",
       "        'summary': 'OVN northbound database server(s) has not been a member of the databases high availability for a period of time.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.0259e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.002747917Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesSouthboundDatabaseClusterMemberError',\n",
       "       'query': 'min_over_time(cluster:ovn_db_sbdb_not_cluster_member:abs[5m]) != 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'namespace': 'openshift-ovn-kubernetes',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN southbound database server(s) has not been a RAFT cluster member for a period of time which may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN southbound database server(s) has not been a member of the databases high availability for a period of time.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 8.0072e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.002839338Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNorthboundDatabaseInboundConnectionError',\n",
       "       'query': 'min_over_time(ovn_db_cluster_inbound_connections_error_total{db_name=\"OVN_Northbound\"}[5m]) > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN northbound database server(s) is experiencing inbound RAFT connectivity errors which may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN northbound database server(s) is experiencing inbound RAFT connectivity errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000124424,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.002920471Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesSouthboundDatabaseInboundConnectionError',\n",
       "       'query': 'min_over_time(ovn_db_cluster_inbound_connections_error_total{db_name=\"OVN_Southbound\"}[5m]) > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN southbound database server(s) is experiencing inbound RAFT connectivity errors which may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN southbound database server(s) is experiencing inbound RAFT connectivity errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.5258e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.003046389Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNorthboundDatabaseOutboundConnectionError',\n",
       "       'query': 'min_over_time(ovn_db_cluster_outbound_connections_error_total{db_name=\"OVN_Northbound\"}[5m]) > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN northbound database server(s) outbound RAFT connectivity errors may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN northbound database server(s) is experiencing outbound RAFT connectivity errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 8.1263e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.003142499Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesSouthboundDatabaseOutboundConnectionError',\n",
       "       'query': 'min_over_time(ovn_db_cluster_outbound_connections_error_total{db_name=\"OVN_Southbound\"}[5m]) > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN southbound database server(s) outbound RAFT connectivity errors which may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN southbound database server(s) is experiencing outbound RAFT connectivity errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000112281,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.003224544Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNorthboundDatabaseInboundConnectionMissing',\n",
       "       'query': 'min_over_time(cluster:ovn_db_nbdb_missing_inbound_connections:abs[5m]) != 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'namespace': 'openshift-ovn-kubernetes',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN northbound database server(s) do not have expected number of inbound connections for a RAFT cluster which may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN northbound database server(s) do not have expected number of inbound RAFT connections.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000115177,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.003337927Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesSouthboundDatabaseInboundConnectionMissing',\n",
       "       'query': 'min_over_time(cluster:ovn_db_sbdb_missing_inbound_connections:abs[5m]) != 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'namespace': 'openshift-ovn-kubernetes',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN southbound database server(s) do not have expected number of inbound connections for a RAFT cluster which may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN southbound database server(s) do not have expected number of inbound RAFT connections.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000101992,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.003454156Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNorthboundDatabaseOutboundConnectionMissing',\n",
       "       'query': 'min_over_time(cluster:ovn_db_nbdb_missing_outbound_connections:abs[5m]) != 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'namespace': 'openshift-ovn-kubernetes',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN northbound database server(s) do not have expected number of outbound connections for a RAFT cluster which may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN northbound database server(s) do not have expected number of outbound RAFT connections.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.9637e-05,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.003557561Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesSouthboundDatabaseOutboundConnectionMissing',\n",
       "       'query': 'min_over_time(cluster:ovn_db_sbdb_missing_outbound_connections:abs[5m]) != 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'namespace': 'openshift-ovn-kubernetes',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN southbound database server(s) do not have expected number of outbound connections for a RAFT cluster which may indicate degraded OVN database high availability.',\n",
       "        'summary': 'OVN southbound database server(s) do not have expected number of outbound RAFT connections.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000124394,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.00365836Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNorthboundDatabaseCPUUsageHigh',\n",
       "       'query': '(sum by (instance, name, namespace) (rate(container_cpu_usage_seconds_total{container=\"nbdb\"}[5m]))) > 0.8',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'High OVN northbound CPU usage indicates high load on the networking control plane.',\n",
       "        'summary': 'OVN northbound database {{ $labels.instance }} is greater than {{ $value | humanizePercentage }} percent CPU usage for a period of time.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000196891,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.003784177Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesSouthboundDatabaseCPUUsageHigh',\n",
       "       'query': '(sum by (instance, name, namespace) (rate(container_cpu_usage_seconds_total{container=\"sbdb\"}[5m]))) > 0.8',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'High OVN southbound CPU usage indicates high load on the networking control plane.',\n",
       "        'summary': 'OVN southbound database {{ $labels.instance }} is greater than {{ $value | humanizePercentage }} percent CPU usage for a period of time.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000175059,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.00398206Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNorthdInactive',\n",
       "       'query': 'count by (namespace) (ovn_northd_status == 1) != 1',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Exactly one OVN northd must have an active status within the high availability set. Networking control plane is degraded.',\n",
       "        'summary': 'Exactly one OVN northd instance must have an active status.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000184979,\n",
       "       'lastEvaluation': '2023-05-29T21:24:40.004158241Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.005365,\n",
       "     'lastEvaluation': '2023-05-29T21:24:39.998980204Z'},\n",
       "    {'name': 'cluster-network-operator-ovn.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-ovn-kubernetes-networking-rules-0faa166f-a2e3-426b-bb43-002284681413.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'NodeWithoutOVNKubeNodePodRunning',\n",
       "       'query': '(kube_node_info unless on (node) (kube_pod_info{namespace=\"openshift-ovn-kubernetes\",pod=~\"ovnkube-node.*\"} or kube_node_labels{label_kubernetes_io_os=\"windows\"})) > 0',\n",
       "       'duration': 1200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Networking is degraded on nodes that do not have a functioning ovnkube-node pod. Existing workloads on the\\nnode may continue to have connectivity but any changes to the networking control plane will not be implemented.\\n',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NodeWithoutOVNKubeNodePodRunning.md',\n",
       "        'summary': 'All Linux nodes should be running an ovnkube-node pod, {{ $labels.node }} is not.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000509077,\n",
       "       'lastEvaluation': '2023-05-29T21:24:26.050395298Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesControllerDisconnectedSouthboundDatabase',\n",
       "       'query': 'max_over_time(ovn_controller_southbound_database_connected[5m]) == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Networking is degraded on nodes when OVN controller is not connected to OVN southbound database connection. No networking control plane updates will be applied to the node.\\n',\n",
       "        'summary': 'Networking control plane is degraded on node {{ $labels.node }} because OVN controller is not connected to OVN southbound database.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.0001432,\n",
       "       'lastEvaluation': '2023-05-29T21:24:26.050906039Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNodePodAddError',\n",
       "       'query': '(sum by (instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command=\"ADD\",err=\"true\"}[5m])) / sum by (instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command=\"ADD\"}[5m]))) > 0.1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN Kubernetes experiences pod creation errors at an elevated rate. The pods will be retried.',\n",
       "        'summary': 'OVN Kubernetes is experiencing pod creation errors at an elevated rate.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000163418,\n",
       "       'lastEvaluation': '2023-05-29T21:24:26.05105008Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'OVNKubernetesNodePodDeleteError',\n",
       "       'query': '(sum by (instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command=\"DEL\",err=\"true\"}[5m])) / sum by (instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command=\"DEL\"}[5m]))) > 0.1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'OVN Kubernetes experiences pod deletion errors at an elevated rate. The pods will be retried.',\n",
       "        'summary': 'OVN Kubernetes experiencing pod deletion errors at an elevated rate.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000147247,\n",
       "       'lastEvaluation': '2023-05-29T21:24:26.051214389Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000975997,\n",
       "     'lastEvaluation': '2023-05-29T21:24:26.050387804Z'}]}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "r = requests.get(\n",
    "    f\"https://{prometheus_host}/v1/rules\",\n",
    "    verify=False,\n",
    "    headers=kubeConfig.api_key,\n",
    ")\n",
    "r.status_code, r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " {'status': 'success',\n",
       "  'data': ['From',\n",
       "   'Local',\n",
       "   'Remote',\n",
       "   'To',\n",
       "   '__name__',\n",
       "   'access_mode',\n",
       "   'action',\n",
       "   'addr',\n",
       "   'address',\n",
       "   'alertmanager',\n",
       "   'alertname',\n",
       "   'alertstate',\n",
       "   'apiserver',\n",
       "   'apiservice',\n",
       "   'approval',\n",
       "   'attempts',\n",
       "   'backend',\n",
       "   'bios_date',\n",
       "   'bios_vendor',\n",
       "   'bios_version',\n",
       "   'boot_id',\n",
       "   'bound',\n",
       "   'branch',\n",
       "   'bridge',\n",
       "   'broadcast',\n",
       "   'build',\n",
       "   'buildDate',\n",
       "   'build_date',\n",
       "   'build_phase',\n",
       "   'build_user',\n",
       "   'buildconfig',\n",
       "   'cachesize',\n",
       "   'call',\n",
       "   'cause',\n",
       "   'channel',\n",
       "   'chassis_vendor',\n",
       "   'chassis_version',\n",
       "   'check',\n",
       "   'checkName',\n",
       "   'cidr',\n",
       "   'client_api_version',\n",
       "   'clocksource',\n",
       "   'cloud_type',\n",
       "   'cluster_id',\n",
       "   'cluster_ip',\n",
       "   'cluster_version',\n",
       "   'code',\n",
       "   'code_path',\n",
       "   'collector',\n",
       "   'command',\n",
       "   'compiler',\n",
       "   'completion_mode',\n",
       "   'component',\n",
       "   'concurrency_policy',\n",
       "   'condition',\n",
       "   'config',\n",
       "   'configmap',\n",
       "   'connection_method',\n",
       "   'container',\n",
       "   'container_id',\n",
       "   'container_runtime_version',\n",
       "   'container_state',\n",
       "   'container_type',\n",
       "   'controller',\n",
       "   'core',\n",
       "   'cpu',\n",
       "   'crd',\n",
       "   'crd_name',\n",
       "   'created_by_kind',\n",
       "   'created_by_name',\n",
       "   'cronjob',\n",
       "   'daemonset',\n",
       "   'datapath',\n",
       "   'db_name',\n",
       "   'decision',\n",
       "   'deployment',\n",
       "   'deploymentconfig',\n",
       "   'deprecated_version',\n",
       "   'device',\n",
       "   'dialer_name',\n",
       "   'dnsResolve',\n",
       "   'domainname',\n",
       "   'driver',\n",
       "   'dry_run',\n",
       "   'duplex',\n",
       "   'effect',\n",
       "   'enabled',\n",
       "   'endpoint',\n",
       "   'err',\n",
       "   'error',\n",
       "   'event',\n",
       "   'eventName',\n",
       "   'execute',\n",
       "   'exported_endpoint',\n",
       "   'exported_namespace',\n",
       "   'exported_pod',\n",
       "   'exported_service',\n",
       "   'extension_point',\n",
       "   'external_labels',\n",
       "   'external_name',\n",
       "   'failed',\n",
       "   'family',\n",
       "   'field_validation',\n",
       "   'file',\n",
       "   'filter',\n",
       "   'flow_schema',\n",
       "   'from_version',\n",
       "   'frontend',\n",
       "   'fstype',\n",
       "   'gitCommit',\n",
       "   'gitTreeState',\n",
       "   'gitVersion',\n",
       "   'git_commit',\n",
       "   'git_tree_state',\n",
       "   'git_version',\n",
       "   'go_version',\n",
       "   'goversion',\n",
       "   'group',\n",
       "   'grpc_code',\n",
       "   'grpc_method',\n",
       "   'grpc_service',\n",
       "   'grpc_type',\n",
       "   'handler',\n",
       "   'horizontalpodautoscaler',\n",
       "   'host',\n",
       "   'host_ip',\n",
       "   'host_network',\n",
       "   'id',\n",
       "   'id_like',\n",
       "   'image',\n",
       "   'image_id',\n",
       "   'image_spec',\n",
       "   'index',\n",
       "   'initiator',\n",
       "   'installed',\n",
       "   'instance',\n",
       "   'integration',\n",
       "   'interface',\n",
       "   'internal_ip',\n",
       "   'interval',\n",
       "   'invoker',\n",
       "   'ip',\n",
       "   'ip_family',\n",
       "   'ipaddress',\n",
       "   'job',\n",
       "   'job_name',\n",
       "   'kernelVersion',\n",
       "   'kernel_version',\n",
       "   'key',\n",
       "   'kind',\n",
       "   'kubelet_version',\n",
       "   'kubeproxy_version',\n",
       "   'label_alertmanager',\n",
       "   'label_apiserver',\n",
       "   'label_app',\n",
       "   'label_app_kubernetes_io_component',\n",
       "   'label_app_kubernetes_io_created_by',\n",
       "   'label_app_kubernetes_io_instance',\n",
       "   'label_app_kubernetes_io_managed_by',\n",
       "   'label_app_kubernetes_io_name',\n",
       "   'label_app_kubernetes_io_part_of',\n",
       "   'label_app_kubernetes_io_version',\n",
       "   'label_app_openshift_io_runtime',\n",
       "   'label_beta_kubernetes_io_arch',\n",
       "   'label_beta_kubernetes_io_os',\n",
       "   'label_buildconfig',\n",
       "   'label_catalogsource_operators_coreos_com_update',\n",
       "   'label_clusterName',\n",
       "   'label_cluster_name',\n",
       "   'label_com_company',\n",
       "   'label_component',\n",
       "   'label_control_plane',\n",
       "   'label_controller_manager',\n",
       "   'label_controller_revision_hash',\n",
       "   'label_controller_tools_k8s_io',\n",
       "   'label_controller_uid',\n",
       "   'label_deployment',\n",
       "   'label_deploymentconfig',\n",
       "   'label_dex_config_changed',\n",
       "   'label_dns_operator_openshift_io_daemonset_dns',\n",
       "   'label_docker_registry',\n",
       "   'label_e2e_az_north_south',\n",
       "   'label_etcd',\n",
       "   'label_helm_sh_chart',\n",
       "   'label_infinispan_cr',\n",
       "   'label_ingress_openshift_io_canary',\n",
       "   'label_ingresscanary_operator_openshift_io_daemonset_ingresscanary',\n",
       "   'label_ingresscontroller_operator_openshift_io_deployment_ingresscontroller',\n",
       "   'label_ingresscontroller_operator_openshift_io_hash',\n",
       "   'label_job_name',\n",
       "   'label_k8s_app',\n",
       "   'label_kube_controller_manager',\n",
       "   'label_kubernetes_io_arch',\n",
       "   'label_kubernetes_io_hostname',\n",
       "   'label_kubernetes_io_metadata_name',\n",
       "   'label_kubernetes_io_os',\n",
       "   'label_name',\n",
       "   'label_namespace',\n",
       "   'label_network_openshift_io_policy_group',\n",
       "   'label_node_hyperthread_enabled',\n",
       "   'label_node_openshift_io_os_id',\n",
       "   'label_node_role_kubernetes_io',\n",
       "   'label_node_role_kubernetes_io_master',\n",
       "   'label_oauth_apiserver_anti_affinity',\n",
       "   'label_oauth_openshift_anti_affinity',\n",
       "   'label_olm_catalog_source',\n",
       "   'label_olm_pod_spec_hash',\n",
       "   'label_openshift_apiserver_anti_affinity',\n",
       "   'label_openshift_app',\n",
       "   'label_openshift_io_build_config_name',\n",
       "   'label_openshift_io_build_name',\n",
       "   'label_openshift_io_build_start_policy',\n",
       "   'label_openshift_io_cluster_monitoring',\n",
       "   'label_openshift_io_component',\n",
       "   'label_openshift_io_deployer_pod_for_name',\n",
       "   'label_openshift_io_run_level',\n",
       "   'label_openshift_pipelines_tekton_dev_namespace_reconcile_version',\n",
       "   'label_openshift_route_controller_manager_anti_affinity',\n",
       "   'label_operator_prometheus_io_name',\n",
       "   'label_operator_prometheus_io_shard',\n",
       "   'label_operator_tekton_dev_disable_proxy',\n",
       "   'label_operator_tekton_dev_operand_name',\n",
       "   'label_operator_tekton_dev_target_namespace',\n",
       "   'label_ovn_db_pod',\n",
       "   'label_pipeline_tekton_dev_release',\n",
       "   'label_pipelines_as_code_route',\n",
       "   'label_pod_security_kubernetes_io_audit',\n",
       "   'label_pod_security_kubernetes_io_audit_version',\n",
       "   'label_pod_security_kubernetes_io_enforce',\n",
       "   'label_pod_security_kubernetes_io_enforce_version',\n",
       "   'label_pod_security_kubernetes_io_warn',\n",
       "   'label_pod_security_kubernetes_io_warn_version',\n",
       "   'label_pod_template_generation',\n",
       "   'label_pod_template_hash',\n",
       "   'label_prometheus',\n",
       "   'label_revision',\n",
       "   'label_rht_comp',\n",
       "   'label_rht_comp_ver',\n",
       "   'label_rht_prod_name',\n",
       "   'label_rht_prod_ver',\n",
       "   'label_rht_subcomp',\n",
       "   'label_rht_subcomp_t',\n",
       "   'label_route_controller_manager',\n",
       "   'label_scheduler',\n",
       "   'label_security_openshift_io_scc_pod_security_label_sync',\n",
       "   'label_service',\n",
       "   'label_service_ca',\n",
       "   'label_statefulset_kubernetes_io_pod_name',\n",
       "   'label_triggers_tekton_dev_release',\n",
       "   'label_type',\n",
       "   'label_version',\n",
       "   'le',\n",
       "   'lease',\n",
       "   'level',\n",
       "   'listener_name',\n",
       "   'location',\n",
       "   'long_running',\n",
       "   'machine',\n",
       "   'machine_id',\n",
       "   'major',\n",
       "   'mapping',\n",
       "   'mark',\n",
       "   'mediatype',\n",
       "   'member',\n",
       "   'method',\n",
       "   'metric',\n",
       "   'metric_name',\n",
       "   'metric_source',\n",
       "   'metric_target_type',\n",
       "   'metrics_path',\n",
       "   'microcode',\n",
       "   'migrated',\n",
       "   'minor',\n",
       "   'mode',\n",
       "   'model',\n",
       "   'model_name',\n",
       "   'mountpoint',\n",
       "   'msg_type',\n",
       "   'mutatingwebhookconfiguration',\n",
       "   'name',\n",
       "   'namespace',\n",
       "   'nb_schema_version',\n",
       "   'network_name',\n",
       "   'networkpolicy',\n",
       "   'networks',\n",
       "   'nfs_path',\n",
       "   'nfs_server',\n",
       "   'node',\n",
       "   'nodename',\n",
       "   'op',\n",
       "   'operation',\n",
       "   'operation_name',\n",
       "   'operation_type',\n",
       "   'operator',\n",
       "   'operstate',\n",
       "   'os',\n",
       "   'osVersion',\n",
       "   'os_image',\n",
       "   'outcome',\n",
       "   'ovs_lib_version',\n",
       "   'owner_is_controller',\n",
       "   'owner_kind',\n",
       "   'owner_name',\n",
       "   'package',\n",
       "   'path',\n",
       "   'peer',\n",
       "   'persistentvolume',\n",
       "   'persistentvolumeclaim',\n",
       "   'phase',\n",
       "   'platform',\n",
       "   'plugin',\n",
       "   'plugin_name',\n",
       "   'pod',\n",
       "   'pod_ip',\n",
       "   'pod_uid',\n",
       "   'poddisruptionbudget',\n",
       "   'policy_level',\n",
       "   'policy_version',\n",
       "   'pool',\n",
       "   'port_name',\n",
       "   'port_number',\n",
       "   'port_protocol',\n",
       "   'pretty_name',\n",
       "   'primary_model',\n",
       "   'priority',\n",
       "   'priority_class',\n",
       "   'priority_level',\n",
       "   'probe_type',\n",
       "   'product_family',\n",
       "   'product_name',\n",
       "   'product_version',\n",
       "   'profile',\n",
       "   'proto',\n",
       "   'provisioner',\n",
       "   'quantile',\n",
       "   'queue',\n",
       "   'queue_name',\n",
       "   'rcode',\n",
       "   'ready',\n",
       "   'reason',\n",
       "   'reclaim_policy',\n",
       "   'reconciler',\n",
       "   'registry',\n",
       "   'rejected',\n",
       "   'release',\n",
       "   'removed_release',\n",
       "   'replicaset',\n",
       "   'replicationcontroller',\n",
       "   'request_kind',\n",
       "   'request_operation',\n",
       "   'resource',\n",
       "   'resource_prefix',\n",
       "   'resource_type',\n",
       "   'resourcequota',\n",
       "   'result',\n",
       "   'revision',\n",
       "   'role',\n",
       "   'route',\n",
       "   'router_name',\n",
       "   'rule_group',\n",
       "   'sb_schema_version',\n",
       "   'scaletargetref_api_version',\n",
       "   'scaletargetref_kind',\n",
       "   'scaletargetref_name',\n",
       "   'schedule',\n",
       "   'scheduled',\n",
       "   'scheduler',\n",
       "   'scope',\n",
       "   'scrape_job',\n",
       "   'secret',\n",
       "   'serial',\n",
       "   'server',\n",
       "   'server_go_version',\n",
       "   'server_id',\n",
       "   'server_role',\n",
       "   'server_status',\n",
       "   'server_type',\n",
       "   'server_version',\n",
       "   'server_vote',\n",
       "   'service',\n",
       "   'service_kind',\n",
       "   'severity',\n",
       "   'shard_name',\n",
       "   'shard_ordinal',\n",
       "   'size',\n",
       "   'slice',\n",
       "   'source',\n",
       "   'stability_level',\n",
       "   'state',\n",
       "   'statefulset',\n",
       "   'status',\n",
       "   'status_code',\n",
       "   'stepping',\n",
       "   'storage',\n",
       "   'storage_class',\n",
       "   'storageclass',\n",
       "   'store_type',\n",
       "   'strategy',\n",
       "   'subresource',\n",
       "   'succeeded',\n",
       "   'success',\n",
       "   'sysname',\n",
       "   'system_client',\n",
       "   'system_uuid',\n",
       "   'system_vendor',\n",
       "   'targetEndpoint',\n",
       "   'tcpConnect',\n",
       "   'time_zone',\n",
       "   'tls_termination',\n",
       "   'to',\n",
       "   'to_kind',\n",
       "   'to_name',\n",
       "   'to_version',\n",
       "   'to_weight',\n",
       "   'toleration_seconds',\n",
       "   'topology',\n",
       "   'triggered_by',\n",
       "   'type',\n",
       "   'uid',\n",
       "   'ulimit',\n",
       "   'unit',\n",
       "   'upstream',\n",
       "   'url',\n",
       "   'usage',\n",
       "   'username',\n",
       "   'uuid',\n",
       "   'validatingwebhookconfiguration',\n",
       "   'vendor',\n",
       "   'verb',\n",
       "   'version',\n",
       "   'version_id',\n",
       "   'volume',\n",
       "   'volume_binding_mode',\n",
       "   'volume_mode',\n",
       "   'volume_plugin',\n",
       "   'webhook',\n",
       "   'work',\n",
       "   'workload',\n",
       "   'workload_type',\n",
       "   'zone',\n",
       "   'zones']})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "r = requests.get(\n",
    "    f\"https://{prometheus_host}/v1/labels\",\n",
    "    verify=False,\n",
    "    headers=kubeConfig.api_key,\n",
    ")\n",
    "r.status_code, r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': ['47c12e10-c5d8-4688-a5f9-c89574e71465',\n",
       "  '4db6cf3c-4598-435c-88dd-715887bc8b30',\n",
       "  '6738e507-5fa3-4ed8-b7c2-b33097d5828c',\n",
       "  '8d6d2ff7-7011-427a-bde9-4129b2ee1c3f',\n",
       "  '9c9d50ad-f1d4-4597-b06f-876ab6747c76',\n",
       "  'aab78d4a-1d24-4a9e-9c44-694ea6882ba5']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\n",
    "    f\"https://{prometheus_host}/v1/label/system_uuid/values\",\n",
    "    verify=False,\n",
    "    headers=kubeConfig.api_key,\n",
    ")\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'__name__': 'machine_cpu_cores',\n",
       "     'boot_id': '162a40c1-36a1-48a2-8546-bddd8c2b4dab',\n",
       "     'endpoint': 'https-metrics',\n",
       "     'instance': '10.0.94.152:10250',\n",
       "     'job': 'kubelet',\n",
       "     'machine_id': '47c12e10c5d84688a5f9c89574e71465',\n",
       "     'metrics_path': '/metrics/cadvisor',\n",
       "     'namespace': 'kube-system',\n",
       "     'node': 'master-0.sharedocp4upi412ovn.lab.upshift.rdu2.redhat.com',\n",
       "     'service': 'kubelet',\n",
       "     'system_uuid': '47c12e10-c5d8-4688-a5f9-c89574e71465'},\n",
       "    'value': [1685395595.153, '4']},\n",
       "   {'metric': {'__name__': 'machine_cpu_cores',\n",
       "     'boot_id': '1c55857c-a00e-4762-883a-3d829a7fb71f',\n",
       "     'endpoint': 'https-metrics',\n",
       "     'instance': '10.0.92.95:10250',\n",
       "     'job': 'kubelet',\n",
       "     'machine_id': '4db6cf3c4598435c88dd715887bc8b30',\n",
       "     'metrics_path': '/metrics/cadvisor',\n",
       "     'namespace': 'kube-system',\n",
       "     'node': 'worker-0.sharedocp4upi412ovn.lab.upshift.rdu2.redhat.com',\n",
       "     'service': 'kubelet',\n",
       "     'system_uuid': '4db6cf3c-4598-435c-88dd-715887bc8b30'},\n",
       "    'value': [1685395595.153, '4']},\n",
       "   {'metric': {'__name__': 'machine_cpu_cores',\n",
       "     'boot_id': '2cf05b2a-8d73-4c70-be92-32931e58b6bd',\n",
       "     'endpoint': 'https-metrics',\n",
       "     'instance': '10.0.91.184:10250',\n",
       "     'job': 'kubelet',\n",
       "     'machine_id': '9c9d50adf1d44597b06f876ab6747c76',\n",
       "     'metrics_path': '/metrics/cadvisor',\n",
       "     'namespace': 'kube-system',\n",
       "     'node': 'worker-2.sharedocp4upi412ovn.lab.upshift.rdu2.redhat.com',\n",
       "     'service': 'kubelet',\n",
       "     'system_uuid': '9c9d50ad-f1d4-4597-b06f-876ab6747c76'},\n",
       "    'value': [1685395595.153, '4']},\n",
       "   {'metric': {'__name__': 'machine_cpu_cores',\n",
       "     'boot_id': '38a72f6d-b1ca-4b00-a5f7-db8c09ad0906',\n",
       "     'endpoint': 'https-metrics',\n",
       "     'instance': '10.0.93.8:10250',\n",
       "     'job': 'kubelet',\n",
       "     'machine_id': '8d6d2ff77011427abde94129b2ee1c3f',\n",
       "     'metrics_path': '/metrics/cadvisor',\n",
       "     'namespace': 'kube-system',\n",
       "     'node': 'master-1.sharedocp4upi412ovn.lab.upshift.rdu2.redhat.com',\n",
       "     'service': 'kubelet',\n",
       "     'system_uuid': '8d6d2ff7-7011-427a-bde9-4129b2ee1c3f'},\n",
       "    'value': [1685395595.153, '4']},\n",
       "   {'metric': {'__name__': 'machine_cpu_cores',\n",
       "     'boot_id': '3927d6b0-8118-451a-8211-7d0f19891c1b',\n",
       "     'endpoint': 'https-metrics',\n",
       "     'instance': '10.0.93.146:10250',\n",
       "     'job': 'kubelet',\n",
       "     'machine_id': 'aab78d4a1d244a9e9c44694ea6882ba5',\n",
       "     'metrics_path': '/metrics/cadvisor',\n",
       "     'namespace': 'kube-system',\n",
       "     'node': 'master-2.sharedocp4upi412ovn.lab.upshift.rdu2.redhat.com',\n",
       "     'service': 'kubelet',\n",
       "     'system_uuid': 'aab78d4a-1d24-4a9e-9c44-694ea6882ba5'},\n",
       "    'value': [1685395595.153, '4']},\n",
       "   {'metric': {'__name__': 'machine_cpu_cores',\n",
       "     'boot_id': '9da67a93-f6d6-4344-90a7-81d575c44c63',\n",
       "     'endpoint': 'https-metrics',\n",
       "     'instance': '10.0.90.233:10250',\n",
       "     'job': 'kubelet',\n",
       "     'machine_id': '6738e5075fa34ed8b7c2b33097d5828c',\n",
       "     'metrics_path': '/metrics/cadvisor',\n",
       "     'namespace': 'kube-system',\n",
       "     'node': 'worker-1.sharedocp4upi412ovn.lab.upshift.rdu2.redhat.com',\n",
       "     'service': 'kubelet',\n",
       "     'system_uuid': '6738e507-5fa3-4ed8-b7c2-b33097d5828c'},\n",
       "    'value': [1685395595.153, '4']}]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\n",
    "    f\"https://{prometheus_host}/v1/query?query=machine_cpu_cores&query=machine_cpu_sockets\",\n",
    "    verify=False,\n",
    "    headers=kubeConfig.api_key,\n",
    ")\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>?</th>\n",
       "      <th>__name__</th>\n",
       "      <th>condition</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>instance</th>\n",
       "      <th>job</th>\n",
       "      <th>name</th>\n",
       "      <th>namespace</th>\n",
       "      <th>pod</th>\n",
       "      <th>reason</th>\n",
       "      <th>service</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>cluster_operator_conditions</td>\n",
       "      <td>Available</td>\n",
       "      <td>metrics</td>\n",
       "      <td>10.0.93.146:9099</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "      <td>authentication</td>\n",
       "      <td>openshift-cluster-version</td>\n",
       "      <td>cluster-version-operator-586dd8fdbf-tt8s2</td>\n",
       "      <td>AsExpected</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>cluster_operator_conditions</td>\n",
       "      <td>Available</td>\n",
       "      <td>metrics</td>\n",
       "      <td>10.0.93.146:9099</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "      <td>baremetal</td>\n",
       "      <td>openshift-cluster-version</td>\n",
       "      <td>cluster-version-operator-586dd8fdbf-tt8s2</td>\n",
       "      <td>WaitingForProvisioningCR</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>cluster_operator_conditions</td>\n",
       "      <td>Available</td>\n",
       "      <td>metrics</td>\n",
       "      <td>10.0.93.146:9099</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "      <td>cloud-controller-manager</td>\n",
       "      <td>openshift-cluster-version</td>\n",
       "      <td>cluster-version-operator-586dd8fdbf-tt8s2</td>\n",
       "      <td>AsExpected</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>cluster_operator_conditions</td>\n",
       "      <td>Available</td>\n",
       "      <td>metrics</td>\n",
       "      <td>10.0.93.146:9099</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "      <td>cloud-credential</td>\n",
       "      <td>openshift-cluster-version</td>\n",
       "      <td>cluster-version-operator-586dd8fdbf-tt8s2</td>\n",
       "      <td>AsExpected</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>cluster_operator_conditions</td>\n",
       "      <td>Available</td>\n",
       "      <td>metrics</td>\n",
       "      <td>10.0.93.146:9099</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "      <td>cluster-autoscaler</td>\n",
       "      <td>openshift-cluster-version</td>\n",
       "      <td>cluster-version-operator-586dd8fdbf-tt8s2</td>\n",
       "      <td>AsExpected</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1</td>\n",
       "      <td>cluster_operator_conditions</td>\n",
       "      <td>Upgradeable</td>\n",
       "      <td>metrics</td>\n",
       "      <td>10.0.93.146:9099</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "      <td>service-ca</td>\n",
       "      <td>openshift-cluster-version</td>\n",
       "      <td>cluster-version-operator-586dd8fdbf-tt8s2</td>\n",
       "      <td>AsExpected</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1</td>\n",
       "      <td>cluster_operator_conditions</td>\n",
       "      <td>Upgradeable</td>\n",
       "      <td>metrics</td>\n",
       "      <td>10.0.93.146:9099</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "      <td>storage</td>\n",
       "      <td>openshift-cluster-version</td>\n",
       "      <td>cluster-version-operator-586dd8fdbf-tt8s2</td>\n",
       "      <td>AsExpected</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>cluster_operator_conditions</td>\n",
       "      <td>Upgradeable</td>\n",
       "      <td>metrics</td>\n",
       "      <td>10.0.93.146:9099</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "      <td>version</td>\n",
       "      <td>openshift-cluster-version</td>\n",
       "      <td>cluster-version-operator-586dd8fdbf-tt8s2</td>\n",
       "      <td>MultipleReasons</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>cluster_operator_conditions</td>\n",
       "      <td>UpgradeableAdminAckRequired</td>\n",
       "      <td>metrics</td>\n",
       "      <td>10.0.93.146:9099</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "      <td>version</td>\n",
       "      <td>openshift-cluster-version</td>\n",
       "      <td>cluster-version-operator-586dd8fdbf-tt8s2</td>\n",
       "      <td>AdminAckRequired</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>cluster_operator_conditions</td>\n",
       "      <td>UpgradeableClusterOperators</td>\n",
       "      <td>metrics</td>\n",
       "      <td>10.0.93.146:9099</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "      <td>version</td>\n",
       "      <td>openshift-cluster-version</td>\n",
       "      <td>cluster-version-operator-586dd8fdbf-tt8s2</td>\n",
       "      <td>IncompatibleOperatorsInstalled</td>\n",
       "      <td>cluster-version-operator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>148 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ?                     __name__                    condition endpoint  \\\n",
       "0    1  cluster_operator_conditions                    Available  metrics   \n",
       "1    1  cluster_operator_conditions                    Available  metrics   \n",
       "2    1  cluster_operator_conditions                    Available  metrics   \n",
       "3    1  cluster_operator_conditions                    Available  metrics   \n",
       "4    1  cluster_operator_conditions                    Available  metrics   \n",
       "..  ..                          ...                          ...      ...   \n",
       "143  1  cluster_operator_conditions                  Upgradeable  metrics   \n",
       "144  1  cluster_operator_conditions                  Upgradeable  metrics   \n",
       "145  0  cluster_operator_conditions                  Upgradeable  metrics   \n",
       "146  0  cluster_operator_conditions  UpgradeableAdminAckRequired  metrics   \n",
       "147  0  cluster_operator_conditions  UpgradeableClusterOperators  metrics   \n",
       "\n",
       "             instance                       job                      name  \\\n",
       "0    10.0.93.146:9099  cluster-version-operator            authentication   \n",
       "1    10.0.93.146:9099  cluster-version-operator                 baremetal   \n",
       "2    10.0.93.146:9099  cluster-version-operator  cloud-controller-manager   \n",
       "3    10.0.93.146:9099  cluster-version-operator          cloud-credential   \n",
       "4    10.0.93.146:9099  cluster-version-operator        cluster-autoscaler   \n",
       "..                ...                       ...                       ...   \n",
       "143  10.0.93.146:9099  cluster-version-operator                service-ca   \n",
       "144  10.0.93.146:9099  cluster-version-operator                   storage   \n",
       "145  10.0.93.146:9099  cluster-version-operator                   version   \n",
       "146  10.0.93.146:9099  cluster-version-operator                   version   \n",
       "147  10.0.93.146:9099  cluster-version-operator                   version   \n",
       "\n",
       "                     namespace                                        pod  \\\n",
       "0    openshift-cluster-version  cluster-version-operator-586dd8fdbf-tt8s2   \n",
       "1    openshift-cluster-version  cluster-version-operator-586dd8fdbf-tt8s2   \n",
       "2    openshift-cluster-version  cluster-version-operator-586dd8fdbf-tt8s2   \n",
       "3    openshift-cluster-version  cluster-version-operator-586dd8fdbf-tt8s2   \n",
       "4    openshift-cluster-version  cluster-version-operator-586dd8fdbf-tt8s2   \n",
       "..                         ...                                        ...   \n",
       "143  openshift-cluster-version  cluster-version-operator-586dd8fdbf-tt8s2   \n",
       "144  openshift-cluster-version  cluster-version-operator-586dd8fdbf-tt8s2   \n",
       "145  openshift-cluster-version  cluster-version-operator-586dd8fdbf-tt8s2   \n",
       "146  openshift-cluster-version  cluster-version-operator-586dd8fdbf-tt8s2   \n",
       "147  openshift-cluster-version  cluster-version-operator-586dd8fdbf-tt8s2   \n",
       "\n",
       "                             reason                   service  \n",
       "0                        AsExpected  cluster-version-operator  \n",
       "1          WaitingForProvisioningCR  cluster-version-operator  \n",
       "2                        AsExpected  cluster-version-operator  \n",
       "3                        AsExpected  cluster-version-operator  \n",
       "4                        AsExpected  cluster-version-operator  \n",
       "..                              ...                       ...  \n",
       "143                      AsExpected  cluster-version-operator  \n",
       "144                      AsExpected  cluster-version-operator  \n",
       "145                 MultipleReasons  cluster-version-operator  \n",
       "146                AdminAckRequired  cluster-version-operator  \n",
       "147  IncompatibleOperatorsInstalled  cluster-version-operator  \n",
       "\n",
       "[148 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prometheus_json_to_df(\"cluster_operator_conditions\", \"?\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('openshift-poc-LA5XNyZa-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5137217e5745a6a9de879e2c6663edb7f019d88c9c3887719f710afebf6e60a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
