{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from kubernetes import client\n",
    "from openshift.dynamic import DynamicClient\n",
    "from openshift.helper.userpassauth import OCPLoginConfiguration\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "# disable pesky urllib3 ssl warnings.\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "OCP_APIHOST = os.environ[\"OCP_APIHOST\"]\n",
    "OCP_USERNAME = os.environ[\"OCP_USERNAME\"]\n",
    "OCP_PASSWORD = os.environ[\"OCP_PASSWORD\"]\n",
    "QPC_BASE_URL = os.environ[\"QPC_BASE_URL\"]\n",
    "QPC_USERNAME = os.environ[\"QPC_USERNAME\"]\n",
    "QPC_PASSWORD = os.environ[\"QPC_PASSWORD\"]\n",
    "QPC_REPORT_ID = os.environ[\"QPC_REPORT_ID\"]\n",
    "OUTPUT_DIR = os.environ[\"OUTPUT_DIR\"]\n",
    "\n",
    "kubeConfig = OCPLoginConfiguration(ocp_username=OCP_USERNAME, ocp_password=OCP_PASSWORD)\n",
    "kubeConfig.host = OCP_APIHOST\n",
    "kubeConfig.verify_ssl = False\n",
    "\n",
    "kubeConfig.get_token()\n",
    "\n",
    " \n",
    "k8s_client = client.ApiClient(kubeConfig)\n",
    " \n",
    "dyn_client = DynamicClient(k8s_client)\n",
    "# v1_projects = dyn_client.resources.get(api_version='project.openshift.io/v1', kind='Project')\n",
    "# project_list = v1_projects.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prometheus-k8s-openshift-monitoring.apps.sharedocp4upi411ovn.lab.upshift.rdu2.redhat.com/api'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routes_client = dyn_client.resources.get(api_version='route.openshift.io/v1', kind='Route')\n",
    "routes = routes_client.get(namespace=\"openshift-monitoring\", field_selector=\"metadata.name=prometheus-k8s\").items\n",
    "prometheus_host = routes[0][\"spec\"][\"host\"] + routes[0][\"spec\"].get(\"path\", \"/api\")\n",
    "prometheus_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://console-openshift-console.apps.sharedocp4upi411ovn.lab.upshift.rdu2.redhat.com\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sockets</th>\n",
       "      <th>instance</th>\n",
       "      <th>label_node_hyperthread_enabled</th>\n",
       "      <th>label_node_openshift_io_os_id</th>\n",
       "      <th>label_node_role_kubernetes_io_master</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>master-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>false</td>\n",
       "      <td>rhcos</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>master-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>false</td>\n",
       "      <td>rhcos</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>master-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>false</td>\n",
       "      <td>rhcos</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>false</td>\n",
       "      <td>rhcos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>false</td>\n",
       "      <td>rhcos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>false</td>\n",
       "      <td>rhcos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sockets                                           instance  \\\n",
       "0       4  master-0.sharedocp4upi411ovn.lab.upshift.rdu2....   \n",
       "1       4  master-1.sharedocp4upi411ovn.lab.upshift.rdu2....   \n",
       "2       4  master-2.sharedocp4upi411ovn.lab.upshift.rdu2....   \n",
       "3       4  worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....   \n",
       "4       4  worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....   \n",
       "5       4  worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....   \n",
       "\n",
       "  label_node_hyperthread_enabled label_node_openshift_io_os_id  \\\n",
       "0                          false                         rhcos   \n",
       "1                          false                         rhcos   \n",
       "2                          false                         rhcos   \n",
       "3                          false                         rhcos   \n",
       "4                          false                         rhcos   \n",
       "5                          false                         rhcos   \n",
       "\n",
       "  label_node_role_kubernetes_io_master  \n",
       "0                                 true  \n",
       "1                                 true  \n",
       "2                                 true  \n",
       "3                                  NaN  \n",
       "4                                  NaN  \n",
       "5                                  NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "console_client = dyn_client.resources.get(\n",
    "    api_version=\"config.openshift.io/v1\", kind=\"Console\"\n",
    ")\n",
    "console_url = console_client.get().items[0][\"status\"][\"consoleURL\"]\n",
    "print(console_url)\n",
    "\n",
    "# prometheus_query=\"node_role_os_version_machine:cpu_capacity_sockets:sum\"\n",
    "#prometheus_query=\"workload:capacity_physical_cpu_cores:sum\" # https://github.com/openshift/cluster-monitoring-operator/blob/1c595854ab4acbefd70446b54ae314016f400d8e/jsonnet/rules.libsonnet#L249-L251\n",
    "# prometheus_query=\"node_role_os_version_machine:cpu_capacity_cores:sum\" # https://github.com/openshift/cluster-monitoring-operator/blob/f365468055303edc8a4bb93e494bcbdedcf587bd/manifests/0000_50_cluster-monitoring-operator_04-config.yaml#L248-L253\n",
    "# slightly modified version of what cluster-monitoring-operator uses for getting cpu_capacity_sockets\n",
    "prometheus_query = '''\n",
    "count by(instance, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) \n",
    "(max \n",
    "    by(node, instance, package, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra)\n",
    "    (cluster:cpu_core_node_labels)\n",
    ")\n",
    "'''\n",
    "# other interesting metrics\n",
    "# - node_cpu_info\n",
    "# - machine_cpu_sockets\n",
    "# - machine_cpu_cores\n",
    "# - machine_cpu_physical_cores\n",
    "# - node_network_info (macaddresses)\n",
    "# - cluster_infrastructure_provider\n",
    "\n",
    "# r.status_code, r.json()\n",
    "\n",
    "def prometheus_json_to_df(query, value_key):\n",
    "    response = requests.get(\n",
    "        f\"https://{prometheus_host}/v1/query?query={query}\",\n",
    "        verify=False,\n",
    "        headers=kubeConfig.api_key,\n",
    "    )\n",
    "    assert response.ok\n",
    "    return pd.DataFrame(\n",
    "        {value_key:d[\"value\"][1], **d[\"metric\"]}\n",
    "        for d in response.json()[\"data\"][\"result\"]\n",
    "    )\n",
    "\n",
    "prometheus_json_to_df(prometheus_query, \"sockets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>?</th>\n",
       "      <th>__name__</th>\n",
       "      <th>cachesize</th>\n",
       "      <th>container</th>\n",
       "      <th>core</th>\n",
       "      <th>cpu</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>family</th>\n",
       "      <th>instance</th>\n",
       "      <th>job</th>\n",
       "      <th>microcode</th>\n",
       "      <th>model</th>\n",
       "      <th>model_name</th>\n",
       "      <th>namespace</th>\n",
       "      <th>package</th>\n",
       "      <th>pod</th>\n",
       "      <th>service</th>\n",
       "      <th>stepping</th>\n",
       "      <th>vendor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>node-exporter-sq5qm</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>node-exporter-sbmqf</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>node-exporter-2vh2b</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>node-exporter-tcq6s</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>node-exporter-k5d5x</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>0</td>\n",
       "      <td>node-exporter-bmnhv</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>1</td>\n",
       "      <td>node-exporter-sq5qm</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>1</td>\n",
       "      <td>node-exporter-sbmqf</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>1</td>\n",
       "      <td>node-exporter-2vh2b</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>1</td>\n",
       "      <td>node-exporter-tcq6s</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>1</td>\n",
       "      <td>node-exporter-k5d5x</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>1</td>\n",
       "      <td>node-exporter-bmnhv</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>2</td>\n",
       "      <td>node-exporter-sq5qm</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>2</td>\n",
       "      <td>node-exporter-sbmqf</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>2</td>\n",
       "      <td>node-exporter-2vh2b</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>2</td>\n",
       "      <td>node-exporter-tcq6s</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>2</td>\n",
       "      <td>node-exporter-k5d5x</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>2</td>\n",
       "      <td>node-exporter-bmnhv</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>3</td>\n",
       "      <td>node-exporter-sq5qm</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>3</td>\n",
       "      <td>node-exporter-sbmqf</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>master-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>3</td>\n",
       "      <td>node-exporter-2vh2b</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>3</td>\n",
       "      <td>node-exporter-tcq6s</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>3</td>\n",
       "      <td>node-exporter-k5d5x</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>node_cpu_info</td>\n",
       "      <td>512 KB</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>https</td>\n",
       "      <td>23</td>\n",
       "      <td>worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>0x1000065</td>\n",
       "      <td>1</td>\n",
       "      <td>AMD EPYC Processor (with IBPB)</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>3</td>\n",
       "      <td>node-exporter-bmnhv</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>2</td>\n",
       "      <td>AuthenticAMD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ?       __name__ cachesize        container core cpu endpoint family  \\\n",
       "0   1  node_cpu_info    512 KB  kube-rbac-proxy    0   0    https     23   \n",
       "1   1  node_cpu_info    512 KB  kube-rbac-proxy    0   0    https     23   \n",
       "2   1  node_cpu_info    512 KB  kube-rbac-proxy    0   0    https     23   \n",
       "3   1  node_cpu_info    512 KB  kube-rbac-proxy    0   0    https     23   \n",
       "4   1  node_cpu_info    512 KB  kube-rbac-proxy    0   0    https     23   \n",
       "5   1  node_cpu_info    512 KB  kube-rbac-proxy    0   0    https     23   \n",
       "6   1  node_cpu_info    512 KB  kube-rbac-proxy    0   1    https     23   \n",
       "7   1  node_cpu_info    512 KB  kube-rbac-proxy    0   1    https     23   \n",
       "8   1  node_cpu_info    512 KB  kube-rbac-proxy    0   1    https     23   \n",
       "9   1  node_cpu_info    512 KB  kube-rbac-proxy    0   1    https     23   \n",
       "10  1  node_cpu_info    512 KB  kube-rbac-proxy    0   1    https     23   \n",
       "11  1  node_cpu_info    512 KB  kube-rbac-proxy    0   1    https     23   \n",
       "12  1  node_cpu_info    512 KB  kube-rbac-proxy    0   2    https     23   \n",
       "13  1  node_cpu_info    512 KB  kube-rbac-proxy    0   2    https     23   \n",
       "14  1  node_cpu_info    512 KB  kube-rbac-proxy    0   2    https     23   \n",
       "15  1  node_cpu_info    512 KB  kube-rbac-proxy    0   2    https     23   \n",
       "16  1  node_cpu_info    512 KB  kube-rbac-proxy    0   2    https     23   \n",
       "17  1  node_cpu_info    512 KB  kube-rbac-proxy    0   2    https     23   \n",
       "18  1  node_cpu_info    512 KB  kube-rbac-proxy    0   3    https     23   \n",
       "19  1  node_cpu_info    512 KB  kube-rbac-proxy    0   3    https     23   \n",
       "20  1  node_cpu_info    512 KB  kube-rbac-proxy    0   3    https     23   \n",
       "21  1  node_cpu_info    512 KB  kube-rbac-proxy    0   3    https     23   \n",
       "22  1  node_cpu_info    512 KB  kube-rbac-proxy    0   3    https     23   \n",
       "23  1  node_cpu_info    512 KB  kube-rbac-proxy    0   3    https     23   \n",
       "\n",
       "                                             instance            job  \\\n",
       "0   master-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "1   master-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "2   master-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "3   worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "4   worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "5   worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "6   master-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "7   master-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "8   master-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "9   worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "10  worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "11  worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "12  master-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "13  master-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "14  master-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "15  worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "16  worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "17  worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "18  master-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "19  master-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "20  master-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "21  worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "22  worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "23  worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "\n",
       "    microcode model                      model_name             namespace  \\\n",
       "0   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "1   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "2   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "3   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "4   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "5   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "6   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "7   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "8   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "9   0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "10  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "11  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "12  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "13  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "14  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "15  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "16  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "17  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "18  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "19  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "20  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "21  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "22  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "23  0x1000065     1  AMD EPYC Processor (with IBPB)  openshift-monitoring   \n",
       "\n",
       "   package                  pod        service stepping        vendor  \n",
       "0        0  node-exporter-sq5qm  node-exporter        2  AuthenticAMD  \n",
       "1        0  node-exporter-sbmqf  node-exporter        2  AuthenticAMD  \n",
       "2        0  node-exporter-2vh2b  node-exporter        2  AuthenticAMD  \n",
       "3        0  node-exporter-tcq6s  node-exporter        2  AuthenticAMD  \n",
       "4        0  node-exporter-k5d5x  node-exporter        2  AuthenticAMD  \n",
       "5        0  node-exporter-bmnhv  node-exporter        2  AuthenticAMD  \n",
       "6        1  node-exporter-sq5qm  node-exporter        2  AuthenticAMD  \n",
       "7        1  node-exporter-sbmqf  node-exporter        2  AuthenticAMD  \n",
       "8        1  node-exporter-2vh2b  node-exporter        2  AuthenticAMD  \n",
       "9        1  node-exporter-tcq6s  node-exporter        2  AuthenticAMD  \n",
       "10       1  node-exporter-k5d5x  node-exporter        2  AuthenticAMD  \n",
       "11       1  node-exporter-bmnhv  node-exporter        2  AuthenticAMD  \n",
       "12       2  node-exporter-sq5qm  node-exporter        2  AuthenticAMD  \n",
       "13       2  node-exporter-sbmqf  node-exporter        2  AuthenticAMD  \n",
       "14       2  node-exporter-2vh2b  node-exporter        2  AuthenticAMD  \n",
       "15       2  node-exporter-tcq6s  node-exporter        2  AuthenticAMD  \n",
       "16       2  node-exporter-k5d5x  node-exporter        2  AuthenticAMD  \n",
       "17       2  node-exporter-bmnhv  node-exporter        2  AuthenticAMD  \n",
       "18       3  node-exporter-sq5qm  node-exporter        2  AuthenticAMD  \n",
       "19       3  node-exporter-sbmqf  node-exporter        2  AuthenticAMD  \n",
       "20       3  node-exporter-2vh2b  node-exporter        2  AuthenticAMD  \n",
       "21       3  node-exporter-tcq6s  node-exporter        2  AuthenticAMD  \n",
       "22       3  node-exporter-k5d5x  node-exporter        2  AuthenticAMD  \n",
       "23       3  node-exporter-bmnhv  node-exporter        2  AuthenticAMD  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prometheus_json_to_df(\"node_cpu_info\", \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>?</th>\n",
       "      <th>__name__</th>\n",
       "      <th>address</th>\n",
       "      <th>broadcast</th>\n",
       "      <th>container</th>\n",
       "      <th>device</th>\n",
       "      <th>endpoint</th>\n",
       "      <th>instance</th>\n",
       "      <th>job</th>\n",
       "      <th>namespace</th>\n",
       "      <th>operstate</th>\n",
       "      <th>pod</th>\n",
       "      <th>service</th>\n",
       "      <th>duplex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>lo</td>\n",
       "      <td>https</td>\n",
       "      <td>master-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-sq5qm</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>lo</td>\n",
       "      <td>https</td>\n",
       "      <td>master-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-sbmqf</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>lo</td>\n",
       "      <td>https</td>\n",
       "      <td>master-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-2vh2b</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>lo</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-tcq6s</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>lo</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-k5d5x</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>00:00:00:00:00:00</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>lo</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-bmnhv</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>0a:75:c2:8d:b0:e6</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>genev_sys_6081</td>\n",
       "      <td>https</td>\n",
       "      <td>master-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-2vh2b</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>0e:3e:1b:fb:5f:a4</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>genev_sys_6081</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-bmnhv</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>0e:51:58:e8:f8:84</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovn-k8s-mp0</td>\n",
       "      <td>https</td>\n",
       "      <td>master-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-2vh2b</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>12:83:97:59:aa:2a</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>br-int</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-bmnhv</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>1e:e3:57:d6:84:f6</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>br-int</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-k5d5x</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>36:f1:40:a9:b1:45</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovs-system</td>\n",
       "      <td>https</td>\n",
       "      <td>master-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-2vh2b</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>52:c3:59:4b:13:bc</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovn-k8s-mp0</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-bmnhv</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>5a:78:94:b0:35:d5</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>br-int</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-tcq6s</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>6e:86:a9:91:e0:5f</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovn-k8s-mp0</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-k5d5x</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>6e:fc:ca:5d:05:27</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>genev_sys_6081</td>\n",
       "      <td>https</td>\n",
       "      <td>master-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-sbmqf</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>72:86:c5:e6:99:a5</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovn-k8s-mp0</td>\n",
       "      <td>https</td>\n",
       "      <td>master-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-sq5qm</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>7a:b8:a8:a5:be:a1</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>genev_sys_6081</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-k5d5x</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>aa:31:14:0c:85:4a</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovs-system</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-k5d5x</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>ae:b4:c0:e5:2f:84</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovs-system</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-tcq6s</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>ba:f8:f3:82:b6:b4</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>genev_sys_6081</td>\n",
       "      <td>https</td>\n",
       "      <td>master-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-sq5qm</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>ca:48:36:87:7d:d4</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>br-int</td>\n",
       "      <td>https</td>\n",
       "      <td>master-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-2vh2b</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>ce:d3:60:ce:4c:f2</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovs-system</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-bmnhv</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>d2:80:63:e5:44:5a</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>genev_sys_6081</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-tcq6s</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>d2:fa:e7:ea:ef:d1</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>br-int</td>\n",
       "      <td>https</td>\n",
       "      <td>master-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-sq5qm</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>d6:f9:83:cb:5b:95</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>cni-podman0</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-tcq6s</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>da:99:37:c3:85:3a</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovs-system</td>\n",
       "      <td>https</td>\n",
       "      <td>master-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-sbmqf</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>e2:6a:45:fe:72:4d</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovs-system</td>\n",
       "      <td>https</td>\n",
       "      <td>master-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-sq5qm</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>e6:37:2f:98:da:ba</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovn-k8s-mp0</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-tcq6s</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>e6:e4:61:ff:92:2b</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ovn-k8s-mp0</td>\n",
       "      <td>https</td>\n",
       "      <td>master-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-sbmqf</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>ee:34:8b:9c:45:b5</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>br-int</td>\n",
       "      <td>https</td>\n",
       "      <td>master-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>down</td>\n",
       "      <td>node-exporter-sbmqf</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:49:d4:86</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>br-ex</td>\n",
       "      <td>https</td>\n",
       "      <td>master-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-2vh2b</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:49:d4:86</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ens3</td>\n",
       "      <td>https</td>\n",
       "      <td>master-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>up</td>\n",
       "      <td>node-exporter-2vh2b</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:8f:96:63</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>br-ex</td>\n",
       "      <td>https</td>\n",
       "      <td>master-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-sq5qm</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:8f:96:63</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ens3</td>\n",
       "      <td>https</td>\n",
       "      <td>master-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>up</td>\n",
       "      <td>node-exporter-sq5qm</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:a4:bc:9a</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>br-ex</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-bmnhv</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:a4:bc:9a</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ens3</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>up</td>\n",
       "      <td>node-exporter-bmnhv</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:b9:20:bc</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>br-ex</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-tcq6s</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:b9:20:bc</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ens3</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>up</td>\n",
       "      <td>node-exporter-tcq6s</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:c1:dc:68</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>br-ex</td>\n",
       "      <td>https</td>\n",
       "      <td>master-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-sbmqf</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:c1:dc:68</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ens3</td>\n",
       "      <td>https</td>\n",
       "      <td>master-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>up</td>\n",
       "      <td>node-exporter-sbmqf</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:cc:db:2f</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>br-ex</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>unknown</td>\n",
       "      <td>node-exporter-k5d5x</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>node_network_info</td>\n",
       "      <td>fa:16:3e:cc:db:2f</td>\n",
       "      <td>ff:ff:ff:ff:ff:ff</td>\n",
       "      <td>kube-rbac-proxy</td>\n",
       "      <td>ens3</td>\n",
       "      <td>https</td>\n",
       "      <td>worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>openshift-monitoring</td>\n",
       "      <td>up</td>\n",
       "      <td>node-exporter-k5d5x</td>\n",
       "      <td>node-exporter</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ?           __name__            address          broadcast  \\\n",
       "0   1  node_network_info  00:00:00:00:00:00  00:00:00:00:00:00   \n",
       "1   1  node_network_info  00:00:00:00:00:00  00:00:00:00:00:00   \n",
       "2   1  node_network_info  00:00:00:00:00:00  00:00:00:00:00:00   \n",
       "3   1  node_network_info  00:00:00:00:00:00  00:00:00:00:00:00   \n",
       "4   1  node_network_info  00:00:00:00:00:00  00:00:00:00:00:00   \n",
       "5   1  node_network_info  00:00:00:00:00:00  00:00:00:00:00:00   \n",
       "6   1  node_network_info  0a:75:c2:8d:b0:e6  ff:ff:ff:ff:ff:ff   \n",
       "7   1  node_network_info  0e:3e:1b:fb:5f:a4  ff:ff:ff:ff:ff:ff   \n",
       "8   1  node_network_info  0e:51:58:e8:f8:84  ff:ff:ff:ff:ff:ff   \n",
       "9   1  node_network_info  12:83:97:59:aa:2a  ff:ff:ff:ff:ff:ff   \n",
       "10  1  node_network_info  1e:e3:57:d6:84:f6  ff:ff:ff:ff:ff:ff   \n",
       "11  1  node_network_info  36:f1:40:a9:b1:45  ff:ff:ff:ff:ff:ff   \n",
       "12  1  node_network_info  52:c3:59:4b:13:bc  ff:ff:ff:ff:ff:ff   \n",
       "13  1  node_network_info  5a:78:94:b0:35:d5  ff:ff:ff:ff:ff:ff   \n",
       "14  1  node_network_info  6e:86:a9:91:e0:5f  ff:ff:ff:ff:ff:ff   \n",
       "15  1  node_network_info  6e:fc:ca:5d:05:27  ff:ff:ff:ff:ff:ff   \n",
       "16  1  node_network_info  72:86:c5:e6:99:a5  ff:ff:ff:ff:ff:ff   \n",
       "17  1  node_network_info  7a:b8:a8:a5:be:a1  ff:ff:ff:ff:ff:ff   \n",
       "18  1  node_network_info  aa:31:14:0c:85:4a  ff:ff:ff:ff:ff:ff   \n",
       "19  1  node_network_info  ae:b4:c0:e5:2f:84  ff:ff:ff:ff:ff:ff   \n",
       "20  1  node_network_info  ba:f8:f3:82:b6:b4  ff:ff:ff:ff:ff:ff   \n",
       "21  1  node_network_info  ca:48:36:87:7d:d4  ff:ff:ff:ff:ff:ff   \n",
       "22  1  node_network_info  ce:d3:60:ce:4c:f2  ff:ff:ff:ff:ff:ff   \n",
       "23  1  node_network_info  d2:80:63:e5:44:5a  ff:ff:ff:ff:ff:ff   \n",
       "24  1  node_network_info  d2:fa:e7:ea:ef:d1  ff:ff:ff:ff:ff:ff   \n",
       "25  1  node_network_info  d6:f9:83:cb:5b:95  ff:ff:ff:ff:ff:ff   \n",
       "26  1  node_network_info  da:99:37:c3:85:3a  ff:ff:ff:ff:ff:ff   \n",
       "27  1  node_network_info  e2:6a:45:fe:72:4d  ff:ff:ff:ff:ff:ff   \n",
       "28  1  node_network_info  e6:37:2f:98:da:ba  ff:ff:ff:ff:ff:ff   \n",
       "29  1  node_network_info  e6:e4:61:ff:92:2b  ff:ff:ff:ff:ff:ff   \n",
       "30  1  node_network_info  ee:34:8b:9c:45:b5  ff:ff:ff:ff:ff:ff   \n",
       "31  1  node_network_info  fa:16:3e:49:d4:86  ff:ff:ff:ff:ff:ff   \n",
       "32  1  node_network_info  fa:16:3e:49:d4:86  ff:ff:ff:ff:ff:ff   \n",
       "33  1  node_network_info  fa:16:3e:8f:96:63  ff:ff:ff:ff:ff:ff   \n",
       "34  1  node_network_info  fa:16:3e:8f:96:63  ff:ff:ff:ff:ff:ff   \n",
       "35  1  node_network_info  fa:16:3e:a4:bc:9a  ff:ff:ff:ff:ff:ff   \n",
       "36  1  node_network_info  fa:16:3e:a4:bc:9a  ff:ff:ff:ff:ff:ff   \n",
       "37  1  node_network_info  fa:16:3e:b9:20:bc  ff:ff:ff:ff:ff:ff   \n",
       "38  1  node_network_info  fa:16:3e:b9:20:bc  ff:ff:ff:ff:ff:ff   \n",
       "39  1  node_network_info  fa:16:3e:c1:dc:68  ff:ff:ff:ff:ff:ff   \n",
       "40  1  node_network_info  fa:16:3e:c1:dc:68  ff:ff:ff:ff:ff:ff   \n",
       "41  1  node_network_info  fa:16:3e:cc:db:2f  ff:ff:ff:ff:ff:ff   \n",
       "42  1  node_network_info  fa:16:3e:cc:db:2f  ff:ff:ff:ff:ff:ff   \n",
       "\n",
       "          container          device endpoint  \\\n",
       "0   kube-rbac-proxy              lo    https   \n",
       "1   kube-rbac-proxy              lo    https   \n",
       "2   kube-rbac-proxy              lo    https   \n",
       "3   kube-rbac-proxy              lo    https   \n",
       "4   kube-rbac-proxy              lo    https   \n",
       "5   kube-rbac-proxy              lo    https   \n",
       "6   kube-rbac-proxy  genev_sys_6081    https   \n",
       "7   kube-rbac-proxy  genev_sys_6081    https   \n",
       "8   kube-rbac-proxy     ovn-k8s-mp0    https   \n",
       "9   kube-rbac-proxy          br-int    https   \n",
       "10  kube-rbac-proxy          br-int    https   \n",
       "11  kube-rbac-proxy      ovs-system    https   \n",
       "12  kube-rbac-proxy     ovn-k8s-mp0    https   \n",
       "13  kube-rbac-proxy          br-int    https   \n",
       "14  kube-rbac-proxy     ovn-k8s-mp0    https   \n",
       "15  kube-rbac-proxy  genev_sys_6081    https   \n",
       "16  kube-rbac-proxy     ovn-k8s-mp0    https   \n",
       "17  kube-rbac-proxy  genev_sys_6081    https   \n",
       "18  kube-rbac-proxy      ovs-system    https   \n",
       "19  kube-rbac-proxy      ovs-system    https   \n",
       "20  kube-rbac-proxy  genev_sys_6081    https   \n",
       "21  kube-rbac-proxy          br-int    https   \n",
       "22  kube-rbac-proxy      ovs-system    https   \n",
       "23  kube-rbac-proxy  genev_sys_6081    https   \n",
       "24  kube-rbac-proxy          br-int    https   \n",
       "25  kube-rbac-proxy     cni-podman0    https   \n",
       "26  kube-rbac-proxy      ovs-system    https   \n",
       "27  kube-rbac-proxy      ovs-system    https   \n",
       "28  kube-rbac-proxy     ovn-k8s-mp0    https   \n",
       "29  kube-rbac-proxy     ovn-k8s-mp0    https   \n",
       "30  kube-rbac-proxy          br-int    https   \n",
       "31  kube-rbac-proxy           br-ex    https   \n",
       "32  kube-rbac-proxy            ens3    https   \n",
       "33  kube-rbac-proxy           br-ex    https   \n",
       "34  kube-rbac-proxy            ens3    https   \n",
       "35  kube-rbac-proxy           br-ex    https   \n",
       "36  kube-rbac-proxy            ens3    https   \n",
       "37  kube-rbac-proxy           br-ex    https   \n",
       "38  kube-rbac-proxy            ens3    https   \n",
       "39  kube-rbac-proxy           br-ex    https   \n",
       "40  kube-rbac-proxy            ens3    https   \n",
       "41  kube-rbac-proxy           br-ex    https   \n",
       "42  kube-rbac-proxy            ens3    https   \n",
       "\n",
       "                                             instance            job  \\\n",
       "0   master-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "1   master-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "2   master-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "3   worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "4   worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "5   worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "6   master-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "7   worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "8   master-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "9   worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "10  worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "11  master-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "12  worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "13  worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "14  worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "15  master-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "16  master-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "17  worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "18  worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "19  worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "20  master-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "21  master-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "22  worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "23  worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "24  master-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "25  worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "26  master-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "27  master-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "28  worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "29  master-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "30  master-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "31  master-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "32  master-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "33  master-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "34  master-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "35  worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "36  worker-2.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "37  worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "38  worker-0.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "39  master-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "40  master-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "41  worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "42  worker-1.sharedocp4upi411ovn.lab.upshift.rdu2....  node-exporter   \n",
       "\n",
       "               namespace operstate                  pod        service  \\\n",
       "0   openshift-monitoring   unknown  node-exporter-sq5qm  node-exporter   \n",
       "1   openshift-monitoring   unknown  node-exporter-sbmqf  node-exporter   \n",
       "2   openshift-monitoring   unknown  node-exporter-2vh2b  node-exporter   \n",
       "3   openshift-monitoring   unknown  node-exporter-tcq6s  node-exporter   \n",
       "4   openshift-monitoring   unknown  node-exporter-k5d5x  node-exporter   \n",
       "5   openshift-monitoring   unknown  node-exporter-bmnhv  node-exporter   \n",
       "6   openshift-monitoring   unknown  node-exporter-2vh2b  node-exporter   \n",
       "7   openshift-monitoring   unknown  node-exporter-bmnhv  node-exporter   \n",
       "8   openshift-monitoring   unknown  node-exporter-2vh2b  node-exporter   \n",
       "9   openshift-monitoring      down  node-exporter-bmnhv  node-exporter   \n",
       "10  openshift-monitoring      down  node-exporter-k5d5x  node-exporter   \n",
       "11  openshift-monitoring      down  node-exporter-2vh2b  node-exporter   \n",
       "12  openshift-monitoring   unknown  node-exporter-bmnhv  node-exporter   \n",
       "13  openshift-monitoring      down  node-exporter-tcq6s  node-exporter   \n",
       "14  openshift-monitoring   unknown  node-exporter-k5d5x  node-exporter   \n",
       "15  openshift-monitoring   unknown  node-exporter-sbmqf  node-exporter   \n",
       "16  openshift-monitoring   unknown  node-exporter-sq5qm  node-exporter   \n",
       "17  openshift-monitoring   unknown  node-exporter-k5d5x  node-exporter   \n",
       "18  openshift-monitoring      down  node-exporter-k5d5x  node-exporter   \n",
       "19  openshift-monitoring      down  node-exporter-tcq6s  node-exporter   \n",
       "20  openshift-monitoring   unknown  node-exporter-sq5qm  node-exporter   \n",
       "21  openshift-monitoring      down  node-exporter-2vh2b  node-exporter   \n",
       "22  openshift-monitoring      down  node-exporter-bmnhv  node-exporter   \n",
       "23  openshift-monitoring   unknown  node-exporter-tcq6s  node-exporter   \n",
       "24  openshift-monitoring      down  node-exporter-sq5qm  node-exporter   \n",
       "25  openshift-monitoring      down  node-exporter-tcq6s  node-exporter   \n",
       "26  openshift-monitoring      down  node-exporter-sbmqf  node-exporter   \n",
       "27  openshift-monitoring      down  node-exporter-sq5qm  node-exporter   \n",
       "28  openshift-monitoring   unknown  node-exporter-tcq6s  node-exporter   \n",
       "29  openshift-monitoring   unknown  node-exporter-sbmqf  node-exporter   \n",
       "30  openshift-monitoring      down  node-exporter-sbmqf  node-exporter   \n",
       "31  openshift-monitoring   unknown  node-exporter-2vh2b  node-exporter   \n",
       "32  openshift-monitoring        up  node-exporter-2vh2b  node-exporter   \n",
       "33  openshift-monitoring   unknown  node-exporter-sq5qm  node-exporter   \n",
       "34  openshift-monitoring        up  node-exporter-sq5qm  node-exporter   \n",
       "35  openshift-monitoring   unknown  node-exporter-bmnhv  node-exporter   \n",
       "36  openshift-monitoring        up  node-exporter-bmnhv  node-exporter   \n",
       "37  openshift-monitoring   unknown  node-exporter-tcq6s  node-exporter   \n",
       "38  openshift-monitoring        up  node-exporter-tcq6s  node-exporter   \n",
       "39  openshift-monitoring   unknown  node-exporter-sbmqf  node-exporter   \n",
       "40  openshift-monitoring        up  node-exporter-sbmqf  node-exporter   \n",
       "41  openshift-monitoring   unknown  node-exporter-k5d5x  node-exporter   \n",
       "42  openshift-monitoring        up  node-exporter-k5d5x  node-exporter   \n",
       "\n",
       "     duplex  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "5       NaN  \n",
       "6       NaN  \n",
       "7       NaN  \n",
       "8       NaN  \n",
       "9       NaN  \n",
       "10      NaN  \n",
       "11      NaN  \n",
       "12      NaN  \n",
       "13      NaN  \n",
       "14      NaN  \n",
       "15      NaN  \n",
       "16      NaN  \n",
       "17      NaN  \n",
       "18      NaN  \n",
       "19      NaN  \n",
       "20      NaN  \n",
       "21      NaN  \n",
       "22      NaN  \n",
       "23      NaN  \n",
       "24      NaN  \n",
       "25  unknown  \n",
       "26      NaN  \n",
       "27      NaN  \n",
       "28      NaN  \n",
       "29      NaN  \n",
       "30      NaN  \n",
       "31      NaN  \n",
       "32  unknown  \n",
       "33      NaN  \n",
       "34  unknown  \n",
       "35      NaN  \n",
       "36  unknown  \n",
       "37      NaN  \n",
       "38  unknown  \n",
       "39      NaN  \n",
       "40  unknown  \n",
       "41      NaN  \n",
       "42  unknown  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prometheus_json_to_df(\"node_network_info\", \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " {'status': 'success',\n",
       "  'data': {'aggregator_openapi_v2_regeneration_count': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of OpenAPI v2 spec regeneration count broken down by causing APIService name and reason.',\n",
       "     'unit': ''}],\n",
       "   'aggregator_openapi_v2_regeneration_duration': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Gauge of OpenAPI v2 spec regeneration duration in seconds.',\n",
       "     'unit': ''}],\n",
       "   'aggregator_unavailable_apiservice': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Gauge of APIServices which are marked as unavailable broken down by APIService name.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_alerts': [{'type': 'gauge',\n",
       "     'help': 'How many alerts by state.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_alerts_invalid_total': [{'type': 'counter',\n",
       "     'help': 'The total number of received alerts that were invalid.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_alerts_received_total': [{'type': 'counter',\n",
       "     'help': 'The total number of received alerts.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_build_info': [{'type': 'gauge',\n",
       "     'help': \"A metric with a constant '1' value labeled by version, revision, branch, and goversion from which alertmanager was built.\",\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_alive_messages_total': [{'type': 'counter',\n",
       "     'help': 'Total number of received alive messages.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_enabled': [{'type': 'gauge',\n",
       "     'help': 'Indicates whether the clustering is enabled or not.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_failed_peers': [{'type': 'gauge',\n",
       "     'help': 'Number indicating the current number of failed peers in the cluster.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_health_score': [{'type': 'gauge',\n",
       "     'help': \"Health score of the cluster. Lower values are better and zero means 'totally healthy'.\",\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_members': [{'type': 'gauge',\n",
       "     'help': 'Number indicating current number of members in cluster.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_messages_pruned_total': [{'type': 'counter',\n",
       "     'help': 'Total number of cluster messages pruned.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_messages_queued': [{'type': 'gauge',\n",
       "     'help': 'Number of cluster messages which are queued.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_messages_received_size_total': [{'type': 'counter',\n",
       "     'help': 'Total size of cluster messages received.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_messages_received_total': [{'type': 'counter',\n",
       "     'help': 'Total number of cluster messages received.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_messages_sent_size_total': [{'type': 'counter',\n",
       "     'help': 'Total size of cluster messages sent.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_messages_sent_total': [{'type': 'counter',\n",
       "     'help': 'Total number of cluster messages sent.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_peer_info': [{'type': 'gauge',\n",
       "     'help': \"A metric with a constant '1' value labeled by peer name.\",\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_peers_joined_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of peers that have joined.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_peers_left_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of peers that have left.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_peers_update_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of peers that have updated metadata.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_pings_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of latencies for ping messages.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_reconnections_failed_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of failed cluster peer reconnection attempts.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_reconnections_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of cluster peer reconnections.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_refresh_join_failed_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of failed cluster peer joined attempts via refresh.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_cluster_refresh_join_total': [{'type': 'counter',\n",
       "     'help': 'A counter of the number of cluster peer joined via refresh.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_config_hash': [{'type': 'gauge',\n",
       "     'help': 'Hash of the currently loaded alertmanager configuration.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_config_last_reload_success_timestamp_seconds': [{'type': 'gauge',\n",
       "     'help': 'Timestamp of the last successful configuration reload.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_config_last_reload_successful': [{'type': 'gauge',\n",
       "     'help': 'Whether the last configuration reload attempt was successful.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_dispatcher_aggregation_groups': [{'type': 'gauge',\n",
       "     'help': 'Number of active aggregation groups',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_dispatcher_alert_processing_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Summary of latencies for the processing of alerts.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_http_concurrency_limit_exceeded_total': [{'type': 'counter',\n",
       "     'help': 'Total number of times an HTTP request failed because the concurrency limit was reached.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_http_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of latencies for HTTP requests.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_http_requests_in_flight': [{'type': 'gauge',\n",
       "     'help': 'Current number of HTTP requests being processed.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_http_response_size_bytes': [{'type': 'histogram',\n",
       "     'help': 'Histogram of response size for HTTP requests.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_integrations': [{'type': 'gauge',\n",
       "     'help': 'Number of configured integrations.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_gc_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Duration of the last notification log garbage collection cycle.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_gossip_messages_propagated_total': [{'type': 'counter',\n",
       "     'help': 'Number of received gossip messages that have been further gossiped.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_queries_total': [{'type': 'counter',\n",
       "     'help': 'Number of notification log queries were received.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_query_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Duration of notification log query evaluation.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_query_errors_total': [{'type': 'counter',\n",
       "     'help': 'Number notification log received queries that failed.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_snapshot_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Duration of the last notification log snapshot.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_nflog_snapshot_size_bytes': [{'type': 'gauge',\n",
       "     'help': 'Size of the last notification log snapshot in bytes.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_notification_latency_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency of notifications in seconds.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_notification_requests_failed_total': [{'type': 'counter',\n",
       "     'help': 'The total number of failed notification requests.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_notification_requests_total': [{'type': 'counter',\n",
       "     'help': 'The total number of attempted notification requests.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_notifications_failed_total': [{'type': 'counter',\n",
       "     'help': 'The total number of failed notifications.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_notifications_total': [{'type': 'counter',\n",
       "     'help': 'The total number of attempted notifications.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_oversize_gossip_message_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Duration of oversized gossip message requests.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_oversized_gossip_message_dropped_total': [{'type': 'counter',\n",
       "     'help': 'Number of oversized gossip messages that were dropped due to a full message queue.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_oversized_gossip_message_failure_total': [{'type': 'counter',\n",
       "     'help': 'Number of oversized gossip message sends that failed.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_oversized_gossip_message_sent_total': [{'type': 'counter',\n",
       "     'help': 'Number of oversized gossip message sent.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_peer_position': [{'type': 'gauge',\n",
       "     'help': \"Position the Alertmanager instance believes it's in. The position determines a peer's behavior in the cluster.\",\n",
       "     'unit': ''}],\n",
       "   'alertmanager_receivers': [{'type': 'gauge',\n",
       "     'help': 'Number of configured receivers.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences': [{'type': 'gauge',\n",
       "     'help': 'How many silences by state.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_gc_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Duration of the last silence garbage collection cycle.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_gossip_messages_propagated_total': [{'type': 'counter',\n",
       "     'help': 'Number of received gossip messages that have been further gossiped.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_queries_total': [{'type': 'counter',\n",
       "     'help': 'How many silence queries were received.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_query_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Duration of silence query evaluation.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_query_errors_total': [{'type': 'counter',\n",
       "     'help': 'How many silence received queries did not succeed.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_snapshot_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'Duration of the last silence snapshot.',\n",
       "     'unit': ''}],\n",
       "   'alertmanager_silences_snapshot_size_bytes': [{'type': 'gauge',\n",
       "     'help': 'Size of the last silence snapshot in bytes.',\n",
       "     'unit': ''}],\n",
       "   'apiextensions_openapi_v2_regeneration_count': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of OpenAPI v2 spec regeneration count broken down by causing CRD name and reason.',\n",
       "     'unit': ''}],\n",
       "   'apiextensions_openapi_v3_regeneration_count': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of OpenAPI v3 spec regeneration count broken down by group, version, causing CRD and reason.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_admission_controller_admission_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[STABLE] Admission controller latency histogram in seconds, identified by name and broken out for each operation and API resource and type (validate or admit).',\n",
       "     'unit': ''}],\n",
       "   'apiserver_admission_step_admission_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[STABLE] Admission sub-step latency histogram in seconds, broken out for each operation and API resource and step type (validate or admit).',\n",
       "     'unit': ''}],\n",
       "   'apiserver_admission_step_admission_duration_seconds_summary': [{'type': 'summary',\n",
       "     'help': '[ALPHA] Admission sub-step latency summary in seconds, broken out for each operation and API resource and step type (validate or admit).',\n",
       "     'unit': ''}],\n",
       "   'apiserver_admission_webhook_admission_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[STABLE] Admission webhook latency histogram in seconds, identified by name and broken out for each operation and API resource and type (validate or admit).',\n",
       "     'unit': ''}],\n",
       "   'apiserver_admission_webhook_request_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Admission webhook request total, identified by name and broken out for each admission type (validating or mutating) and operation. Additional labels specify whether the request was rejected or not and an HTTP status code. Codes greater than 600 are truncated to 600, to keep the metrics cardinality bounded.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_audit_event_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of audit events generated and sent to the audit backend.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_audit_level_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of policy levels for audit events (1 per request).',\n",
       "     'unit': ''}],\n",
       "   'apiserver_audit_requests_rejected_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of apiserver requests rejected due to an error in audit logging backend.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_cache_list_fetched_objects_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of objects read from watch cache in the course of serving a LIST request',\n",
       "     'unit': ''}],\n",
       "   'apiserver_cache_list_returned_objects_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of objects returned for a LIST request from watch cache',\n",
       "     'unit': ''}],\n",
       "   'apiserver_cache_list_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of LIST requests served from watch cache',\n",
       "     'unit': ''}],\n",
       "   'apiserver_client_certificate_expiration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Distribution of the remaining lifetime on the certificate used to authenticate a request.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_crd_webhook_conversion_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] CRD webhook conversion duration in seconds',\n",
       "     'unit': ''}],\n",
       "   'apiserver_current_inflight_requests': [{'type': 'gauge',\n",
       "     'help': '[STABLE] Maximal number of currently used inflight request limit of this apiserver per request kind in last second.',\n",
       "     'unit': ''},\n",
       "    {'type': 'gauge',\n",
       "     'help': '[ALPHA] Maximal number of currently used inflight request limit of this apiserver per request kind in last second.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_current_inqueue_requests': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Maximal number of queued requests in this apiserver per request kind in last second.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_delegated_authn_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Request latency in seconds. Broken down by status code.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_delegated_authn_request_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of HTTP requests partitioned by status code.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_delegated_authz_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Request latency in seconds. Broken down by status code.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_delegated_authz_request_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of HTTP requests partitioned by status code.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_envelope_encryption_dek_cache_fill_percent': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Percent of the cache slots currently occupied by cached DEKs.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_current_executing_requests': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of requests in initial (for a WATCH) or any (for a non-WATCH) execution stage in the API Priority and Fairness subsystem',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_current_inqueue_requests': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of requests currently pending in queues of the API Priority and Fairness subsystem',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_current_r': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] R(time of last change)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_dispatch_r': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] R(time of last dispatch)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_dispatched_requests_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of requests executed by API Priority and Fairness subsystem',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_latest_s': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] S(most recently dispatched request)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_next_discounted_s_bounds': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] min and max, over queues, of S(oldest waiting request in queue) - estimated work in progress',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_next_s_bounds': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] min and max, over queues, of S(oldest waiting request in queue)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_priority_level_request_count_samples': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Periodic observations of the number of requests waiting or in any stage of execution (but only initial stage for WATCHes)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_priority_level_request_count_watermarks': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Watermarks of the number of requests waiting or in any stage of execution (but only initial stage for WATCHes)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_priority_level_seat_count_samples': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Periodic observations of number of seats occupied for any stage of execution (but only initial stage for WATCHes)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_priority_level_seat_count_watermarks': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Watermarks of the number of seats occupied for any stage of execution (but only initial stage for WATCHes)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_read_vs_write_request_count_samples': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Periodic observations of the number of requests waiting or in regular stage of execution',\n",
       "     'unit': ''},\n",
       "    {'type': 'histogram',\n",
       "     'help': '[ALPHA] Periodic observations of the number of requests',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_read_vs_write_request_count_watermarks': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Watermarks of the number of requests waiting or in regular stage of execution',\n",
       "     'unit': ''},\n",
       "    {'type': 'histogram',\n",
       "     'help': '[ALPHA] Watermarks of the number of requests',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_request_concurrency_in_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Concurrency (number of seats) occupied by the currently executing (initial stage for a WATCH, any stage otherwise) requests in the API Priority and Fairness subsystem',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_request_concurrency_limit': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Shared concurrency limit in the API Priority and Fairness subsystem',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_request_execution_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration of initial stage (for a WATCH) or any (for a non-WATCH) stage of request execution in the API Priority and Fairness subsystem',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_request_queue_length_after_enqueue': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Length of queue in the API Priority and Fairness subsystem, as seen by each request after it is enqueued',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_request_wait_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Length of time a request spent waiting in its queue',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_watch_count_samples': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] count of watchers for mutating requests in API Priority and Fairness',\n",
       "     'unit': ''}],\n",
       "   'apiserver_flowcontrol_work_estimated_seats': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of estimated seats (maximum of initial and final seats) associated with requests in API Priority and Fairness',\n",
       "     'unit': ''}],\n",
       "   'apiserver_init_events_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of init events processed in watch cache broken by resource type.',\n",
       "     'unit': ''},\n",
       "    {'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of init events processed in watchcache broken by resource type.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_kube_aggregator_x509_insecure_sha1_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counts the number of requests to servers with insecure SHA1 signatures in their serving certificate OR the number of connection failures due to the insecure SHA1 signatures (either/or, based on the runtime environment)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_kube_aggregator_x509_missing_san_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counts the number of requests to servers missing SAN extension in their serving certificate OR the number of connection failures due to the lack of x509 certificate SAN extension missing (either/or, based on the runtime environment)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_longrunning_gauge': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Gauge of all active long-running apiserver requests broken out by verb, group, version, resource, scope and component. Not all requests are tracked this way.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_longrunning_requests': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Gauge of all active long-running apiserver requests broken out by verb, group, version, resource, scope and component. Not all requests are tracked this way.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_registered_watchers': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of currently registered watchers for a given resources',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_aborts_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of requests which apiserver aborted possibly due to a timeout, for each group, version, verb, resource, subresource and scope',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[STABLE] Response latency distribution in seconds for each verb, dry run value, group, version, resource, subresource, scope and component.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_filter_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Request filter latency distribution in seconds, for each filter type',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_post_timeout_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Tracks the activity of the request handlers after the associated requests have been timed out by the apiserver',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_slo_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Response latency distribution (not counting webhook duration) in seconds for each verb, group, version, resource, subresource, scope and component.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_terminations_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of requests which apiserver terminated in self-defense.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_request_total': [{'type': 'counter',\n",
       "     'help': '[STABLE] Counter of apiserver requests broken out for each verb, dry run value, group, version, resource, scope, component, and HTTP response code.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_requested_deprecated_apis': [{'type': 'gauge',\n",
       "     'help': '[STABLE] Gauge of deprecated APIs that have been requested, broken out by API group, version, resource, subresource, and removed_release.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_response_sizes': [{'type': 'histogram',\n",
       "     'help': '[STABLE] Response size distribution in bytes for each group, version, verb, resource, subresource, scope and component.',\n",
       "     'unit': ''},\n",
       "    {'type': 'histogram',\n",
       "     'help': '[ALPHA] Response size distribution in bytes for each group, version, verb, resource, subresource, scope and component.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_selfrequest_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of apiserver self-requests broken out for each verb, API resource and subresource.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_data_key_generation_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Latencies in seconds of data encryption key(DEK) generation operations.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_data_key_generation_failures_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Total number of failed data encryption key(DEK) generation operations.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_envelope_transformation_cache_misses_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Total number of cache misses while accessing key decryption key(KEK).',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_list_evaluated_objects_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of objects tested in the course of serving a LIST request from storage',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_list_fetched_objects_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of objects read from storage in the course of serving a LIST request',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_list_returned_objects_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of objects returned for a LIST request from storage',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_list_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of LIST requests served from storage',\n",
       "     'unit': ''}],\n",
       "   'apiserver_storage_objects': [{'type': 'gauge',\n",
       "     'help': '[STABLE] Number of stored objects at the time of last check split by kind.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_tls_handshake_errors_total': [{'type': 'counter',\n",
       "     'help': \"[ALPHA] Number of requests dropped with 'TLS handshake error from' error\",\n",
       "     'unit': ''}],\n",
       "   'apiserver_watch_cache_events_dispatched_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of events dispatched in watch cache broken by resource type.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_watch_cache_watch_cache_initializations_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of watch cache initializations broken by resource type.',\n",
       "     'unit': ''}],\n",
       "   'apiserver_watch_events_sizes': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Watch event size distribution in bytes',\n",
       "     'unit': ''}],\n",
       "   'apiserver_watch_events_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of events sent in watch clients',\n",
       "     'unit': ''}],\n",
       "   'apiserver_webhooks_x509_insecure_sha1_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counts the number of requests to servers with insecure SHA1 signatures in their serving certificate OR the number of connection failures due to the insecure SHA1 signatures (either/or, based on the runtime environment)',\n",
       "     'unit': ''}],\n",
       "   'apiserver_webhooks_x509_missing_san_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counts the number of requests to servers missing SAN extension in their serving certificate OR the number of connection failures due to the lack of x509 certificate SAN extension missing (either/or, based on the runtime environment)',\n",
       "     'unit': ''}],\n",
       "   'attachdetach_controller_forced_detaches': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of times the A/D Controller performed a forced detach',\n",
       "     'unit': ''}],\n",
       "   'authenticated_user_requests': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of authenticated requests broken out by username.',\n",
       "     'unit': ''}],\n",
       "   'authentication_attempts': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of authenticated attempts.',\n",
       "     'unit': ''}],\n",
       "   'authentication_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Authentication duration in seconds broken out by result.',\n",
       "     'unit': ''}],\n",
       "   'authentication_token_cache_active_fetch_count': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] ',\n",
       "     'unit': ''}],\n",
       "   'authentication_token_cache_fetch_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] ',\n",
       "     'unit': ''}],\n",
       "   'authentication_token_cache_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] ',\n",
       "     'unit': ''}],\n",
       "   'authentication_token_cache_request_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] ',\n",
       "     'unit': ''}],\n",
       "   'cadvisor_version_info': [{'type': 'gauge',\n",
       "     'help': \"A metric with a constant '1' value labeled by kernel version, OS version, docker version, cadvisor version & cadvisor revision.\",\n",
       "     'unit': ''}],\n",
       "   'catalog_source_count': [{'type': 'gauge',\n",
       "     'help': 'Number of catalog sources',\n",
       "     'unit': ''}],\n",
       "   'catalogsource_ready': [{'type': 'gauge',\n",
       "     'help': 'State of a CatalogSource. 1 indicates that the CatalogSource is in a READY state. 0 indicates CatalogSource is in a Non READY state.',\n",
       "     'unit': ''}],\n",
       "   'cco_controller_reconcile_seconds': [{'type': 'histogram',\n",
       "     'help': 'Distribution of the length of time each controllers reconcile loop takes.',\n",
       "     'unit': ''}],\n",
       "   'cco_credentials_mode': [{'type': 'gauge',\n",
       "     'help': 'Track current mode the cloud-credentials-operator is functioning under.',\n",
       "     'unit': ''}],\n",
       "   'cco_credentials_requests': [{'type': 'gauge',\n",
       "     'help': 'Total number of credentials requests.',\n",
       "     'unit': ''}],\n",
       "   'cco_credentials_requests_conditions': [{'type': 'gauge',\n",
       "     'help': 'Credentials requests with asserted conditions.',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_bytes_written_slow': [{'type': 'counter',\n",
       "     'help': 'Bytes written to WAL/SSTs at slow device',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_bytes_written_sst': [{'type': 'counter',\n",
       "     'help': 'Bytes written to SSTs',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_bytes_written_wal': [{'type': 'counter',\n",
       "     'help': 'Bytes written to WAL',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_db_total_bytes': [{'type': 'gauge',\n",
       "     'help': 'Total bytes (main db device)',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_db_used_bytes': [{'type': 'gauge',\n",
       "     'help': 'Used bytes (main db device)',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_log_bytes': [{'type': 'gauge',\n",
       "     'help': 'Size of the metadata log',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_logged_bytes': [{'type': 'counter',\n",
       "     'help': 'Bytes written to the metadata log',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_num_files': [{'type': 'gauge',\n",
       "     'help': 'File count',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_read_bytes': [{'type': 'counter',\n",
       "     'help': 'Bytes requested in buffered read mode',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_read_prefetch_bytes': [{'type': 'counter',\n",
       "     'help': 'Bytes requested in prefetch read mode',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_read_random_buffer_bytes': [{'type': 'counter',\n",
       "     'help': 'Bytes read from prefetch buffer in random read mode',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_read_random_bytes': [{'type': 'counter',\n",
       "     'help': 'Bytes requested in random read mode',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_read_random_disk_bytes': [{'type': 'counter',\n",
       "     'help': 'Bytes read from disk in random read mode',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_slow_total_bytes': [{'type': 'gauge',\n",
       "     'help': 'Total bytes (slow device)',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_slow_used_bytes': [{'type': 'gauge',\n",
       "     'help': 'Used bytes (slow device)',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_wal_total_bytes': [{'type': 'gauge',\n",
       "     'help': 'Total bytes (wal device)',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluefs_wal_used_bytes': [{'type': 'gauge',\n",
       "     'help': 'Used bytes (wal device)',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_bluestore_compressed': [{'type': 'gauge',\n",
       "     'help': 'Sum for stored compressed bytes',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_bluestore_compressed_allocated': [{'type': 'gauge',\n",
       "     'help': 'Sum for bytes allocated for compressed data',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_bluestore_compressed_original': [{'type': 'gauge',\n",
       "     'help': 'Sum for original bytes that were compressed',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_commit_lat_count': [{'type': 'counter',\n",
       "     'help': 'Average commit latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_commit_lat_sum': [{'type': 'counter',\n",
       "     'help': 'Average commit latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_kv_final_lat_count': [{'type': 'counter',\n",
       "     'help': 'Average kv_finalize thread latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_kv_final_lat_sum': [{'type': 'counter',\n",
       "     'help': 'Average kv_finalize thread latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_kv_flush_lat_count': [{'type': 'counter',\n",
       "     'help': 'Average kv_thread flush latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_kv_flush_lat_sum': [{'type': 'counter',\n",
       "     'help': 'Average kv_thread flush latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_kv_sync_lat_count': [{'type': 'counter',\n",
       "     'help': 'Average kv_sync thread latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_kv_sync_lat_sum': [{'type': 'counter',\n",
       "     'help': 'Average kv_sync thread latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_onode_hits': [{'type': 'counter',\n",
       "     'help': 'Count of onode cache lookup hits',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_onode_misses': [{'type': 'counter',\n",
       "     'help': 'Count of onode cache lookup misses',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_committed_bytes': [{'type': 'gauge',\n",
       "     'help': 'total bytes committed,',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_pri0_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri0',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_pri10_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri10',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_pri11_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri11',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_pri1_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri1',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_pri2_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri2',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_pri3_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri3',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_pri4_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri4',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_pri5_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri5',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_pri6_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri6',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_pri7_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri7',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_pri8_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri8',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_pri9_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri9',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:data_reserved_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes reserved for future growth.',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_committed_bytes': [{'type': 'gauge',\n",
       "     'help': 'total bytes committed,',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_committed_bytes': [{'type': 'gauge',\n",
       "     'help': 'total bytes committed,',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_pri0_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri0',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_pri10_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri10',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_pri11_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri11',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_pri1_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri1',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_pri2_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri2',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_pri3_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri3',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_pri4_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri4',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_pri5_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri5',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_pri6_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri6',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_pri7_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri7',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_pri8_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri8',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_pri9_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri9',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_onode_reserved_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes reserved for future growth.',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_pri0_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri0',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_pri10_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri10',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_pri11_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri11',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_pri1_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri1',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_pri2_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri2',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_pri3_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri3',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_pri4_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri4',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_pri5_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri5',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_pri6_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri6',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_pri7_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri7',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_pri8_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri8',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_pri9_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri9',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:kv_reserved_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes reserved for future growth.',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_committed_bytes': [{'type': 'gauge',\n",
       "     'help': 'total bytes committed,',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_pri0_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri0',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_pri10_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri10',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_pri11_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri11',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_pri1_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri1',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_pri2_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri2',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_pri3_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri3',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_pri4_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri4',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_pri5_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri5',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_pri6_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri6',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_pri7_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri7',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_pri8_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri8',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_pri9_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri9',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache:meta_reserved_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes reserved for future growth.',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache_cache_bytes': [{'type': 'gauge',\n",
       "     'help': 'current memory available for caches.',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache_heap_bytes': [{'type': 'gauge',\n",
       "     'help': 'aggregate bytes in use by the heap',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache_mapped_bytes': [{'type': 'gauge',\n",
       "     'help': 'total bytes mapped by the process',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache_target_bytes': [{'type': 'gauge',\n",
       "     'help': 'target process memory usage in bytes',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_pricache_unmapped_bytes': [{'type': 'gauge',\n",
       "     'help': 'unmapped bytes that the kernel has yet to reclaimed',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_read_lat_count': [{'type': 'counter',\n",
       "     'help': 'Average read latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_read_lat_sum': [{'type': 'counter',\n",
       "     'help': 'Average read latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_state_aio_wait_lat_count': [{'type': 'counter',\n",
       "     'help': 'Average aio_wait state latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_state_aio_wait_lat_sum': [{'type': 'counter',\n",
       "     'help': 'Average aio_wait state latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_submit_lat_count': [{'type': 'counter',\n",
       "     'help': 'Average submit latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_submit_lat_sum': [{'type': 'counter',\n",
       "     'help': 'Average submit latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_throttle_lat_count': [{'type': 'counter',\n",
       "     'help': 'Average submit throttle latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_bluestore_throttle_lat_sum': [{'type': 'counter',\n",
       "     'help': 'Average submit throttle latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_cluster_total_bytes': [{'type': 'gauge',\n",
       "     'help': 'DF total_bytes',\n",
       "     'unit': ''}],\n",
       "   'ceph_cluster_total_used_bytes': [{'type': 'gauge',\n",
       "     'help': 'DF total_used_bytes',\n",
       "     'unit': ''}],\n",
       "   'ceph_cluster_total_used_raw_bytes': [{'type': 'gauge',\n",
       "     'help': 'DF total_used_raw_bytes',\n",
       "     'unit': ''}],\n",
       "   'ceph_disk_occupation': [{'type': 'unknown',\n",
       "     'help': 'Associate Ceph daemon with disk used',\n",
       "     'unit': ''}],\n",
       "   'ceph_disk_occupation_human': [{'type': 'unknown',\n",
       "     'help': 'Associate Ceph daemon with disk used',\n",
       "     'unit': ''}],\n",
       "   'ceph_fs_metadata': [{'type': 'unknown',\n",
       "     'help': 'FS Metadata',\n",
       "     'unit': ''}],\n",
       "   'ceph_health_detail': [{'type': 'gauge',\n",
       "     'help': 'healthcheck status by type (0=inactive, 1=active)',\n",
       "     'unit': ''}],\n",
       "   'ceph_health_status': [{'type': 'unknown',\n",
       "     'help': 'Cluster health status',\n",
       "     'unit': ''}],\n",
       "   'ceph_healthcheck_slow_ops': [{'type': 'gauge',\n",
       "     'help': 'OSD or Monitor requests taking a long time to process',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_ireq_enqueue_scrub': [{'type': 'counter',\n",
       "     'help': 'Internal Request type enqueue scrub',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_ireq_exportdir': [{'type': 'counter',\n",
       "     'help': 'Internal Request type export dir',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_ireq_flush': [{'type': 'counter',\n",
       "     'help': 'Internal Request type flush',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_ireq_fragmentdir': [{'type': 'counter',\n",
       "     'help': 'Internal Request type fragmentdir',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_ireq_fragstats': [{'type': 'counter',\n",
       "     'help': 'Internal Request type frag stats',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_ireq_inodestats': [{'type': 'counter',\n",
       "     'help': 'Internal Request type inode stats',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_num_recovering_enqueued': [{'type': 'gauge',\n",
       "     'help': 'Files waiting for recovery',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_num_recovering_prioritized': [{'type': 'gauge',\n",
       "     'help': 'Files waiting for recovery with elevated priority',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_num_recovering_processing': [{'type': 'gauge',\n",
       "     'help': 'Files currently being recovered',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_num_strays': [{'type': 'gauge',\n",
       "     'help': 'Stray dentries',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_num_strays_delayed': [{'type': 'gauge',\n",
       "     'help': 'Stray dentries delayed',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_num_strays_enqueuing': [{'type': 'gauge',\n",
       "     'help': 'Stray dentries enqueuing for purge',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_recovery_completed': [{'type': 'counter',\n",
       "     'help': 'File recoveries completed',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_recovery_started': [{'type': 'counter',\n",
       "     'help': 'File recoveries started',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_strays_created': [{'type': 'counter',\n",
       "     'help': 'Stray dentries created',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_strays_enqueued': [{'type': 'counter',\n",
       "     'help': 'Stray dentries enqueued for purge',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_strays_migrated': [{'type': 'counter',\n",
       "     'help': 'Stray dentries migrated',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_cache_strays_reintegrated': [{'type': 'counter',\n",
       "     'help': 'Stray dentries reintegrated',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_caps': [{'type': 'gauge', 'help': 'Capabilities', 'unit': ''}],\n",
       "   'ceph_mds_ceph_cap_op_flush_ack': [{'type': 'counter',\n",
       "     'help': 'caps truncate notify',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_ceph_cap_op_flushsnap_ack': [{'type': 'counter',\n",
       "     'help': 'caps truncate notify',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_ceph_cap_op_grant': [{'type': 'counter',\n",
       "     'help': 'Grant caps',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_ceph_cap_op_revoke': [{'type': 'counter',\n",
       "     'help': 'Revoke caps',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_ceph_cap_op_trunc': [{'type': 'counter',\n",
       "     'help': 'caps truncate notify',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_dir_commit': [{'type': 'counter',\n",
       "     'help': 'Directory commit',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_dir_fetch': [{'type': 'counter',\n",
       "     'help': 'Directory fetch',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_dir_merge': [{'type': 'counter',\n",
       "     'help': 'Directory merge',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_dir_split': [{'type': 'counter',\n",
       "     'help': 'Directory split',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_exported_inodes': [{'type': 'counter',\n",
       "     'help': 'Exported inodes',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_forward': [{'type': 'counter',\n",
       "     'help': 'Forwarding request',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_handle_client_cap_release': [{'type': 'counter',\n",
       "     'help': 'Client cap release msg',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_handle_client_caps': [{'type': 'counter',\n",
       "     'help': 'Client caps msg',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_handle_client_caps_dirty': [{'type': 'counter',\n",
       "     'help': 'Client dirty caps msg',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_handle_inode_file_caps': [{'type': 'counter',\n",
       "     'help': 'Inter mds caps msg',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_imported_inodes': [{'type': 'counter',\n",
       "     'help': 'Imported inodes',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_inodes': [{'type': 'gauge', 'help': 'Inodes', 'unit': ''}],\n",
       "   'ceph_mds_inodes_expired': [{'type': 'gauge',\n",
       "     'help': 'Inodes expired',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_inodes_pinned': [{'type': 'gauge',\n",
       "     'help': 'Inodes pinned',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_inodes_with_caps': [{'type': 'gauge',\n",
       "     'help': 'Inodes with capabilities',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_load_cent': [{'type': 'gauge',\n",
       "     'help': 'Load per cent',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_ev': [{'type': 'gauge', 'help': 'Events', 'unit': ''}],\n",
       "   'ceph_mds_log_evadd': [{'type': 'counter',\n",
       "     'help': 'Events submitted',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_evex': [{'type': 'counter',\n",
       "     'help': 'Total expired events',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_evexd': [{'type': 'gauge',\n",
       "     'help': 'Current expired events',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_evexg': [{'type': 'gauge',\n",
       "     'help': 'Expiring events',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_evtrm': [{'type': 'counter',\n",
       "     'help': 'Trimmed events',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_jlat_count': [{'type': 'counter',\n",
       "     'help': 'Journaler flush latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_jlat_sum': [{'type': 'counter',\n",
       "     'help': 'Journaler flush latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_replayed': [{'type': 'counter',\n",
       "     'help': 'Events replayed',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_seg': [{'type': 'gauge', 'help': 'Segments', 'unit': ''}],\n",
       "   'ceph_mds_log_segadd': [{'type': 'counter',\n",
       "     'help': 'Segments added',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_segex': [{'type': 'counter',\n",
       "     'help': 'Total expired segments',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_segexd': [{'type': 'gauge',\n",
       "     'help': 'Current expired segments',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_segexg': [{'type': 'gauge',\n",
       "     'help': 'Expiring segments',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_log_segtrm': [{'type': 'counter',\n",
       "     'help': 'Trimmed segments',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_mem_cap': [{'type': 'gauge', 'help': 'Capabilities', 'unit': ''}],\n",
       "   'ceph_mds_mem_cap_minus': [{'type': 'counter',\n",
       "     'help': 'Capabilities removed',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_mem_cap_plus': [{'type': 'counter',\n",
       "     'help': 'Capabilities added',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_mem_dir': [{'type': 'gauge', 'help': 'Directories', 'unit': ''}],\n",
       "   'ceph_mds_mem_dir_minus': [{'type': 'counter',\n",
       "     'help': 'Directories closed',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_mem_dir_plus': [{'type': 'counter',\n",
       "     'help': 'Directories opened',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_mem_dn': [{'type': 'gauge', 'help': 'Dentries', 'unit': ''}],\n",
       "   'ceph_mds_mem_dn_minus': [{'type': 'counter',\n",
       "     'help': 'Dentries closed',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_mem_dn_plus': [{'type': 'counter',\n",
       "     'help': 'Dentries opened',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_mem_heap': [{'type': 'gauge', 'help': 'Heap size', 'unit': ''}],\n",
       "   'ceph_mds_mem_ino': [{'type': 'gauge', 'help': 'Inodes', 'unit': ''}],\n",
       "   'ceph_mds_mem_ino_minus': [{'type': 'counter',\n",
       "     'help': 'Inodes closed',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_mem_ino_plus': [{'type': 'counter',\n",
       "     'help': 'Inodes opened',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_metadata': [{'type': 'unknown',\n",
       "     'help': 'MDS Metadata',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_openino_dir_fetch': [{'type': 'counter',\n",
       "     'help': 'OpenIno incomplete directory fetchings',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_process_request_cap_release': [{'type': 'counter',\n",
       "     'help': 'Process request cap release',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_reply_latency_count': [{'type': 'counter',\n",
       "     'help': 'Reply latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_reply_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Reply latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_request': [{'type': 'counter', 'help': 'Requests', 'unit': ''}],\n",
       "   'ceph_mds_root_rbytes': [{'type': 'gauge',\n",
       "     'help': 'root inode rbytes',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_root_rfiles': [{'type': 'gauge',\n",
       "     'help': 'root inode rfiles',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_root_rsnaps': [{'type': 'gauge',\n",
       "     'help': 'root inode rsnaps',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_cap_acquisition_throttle': [{'type': 'counter',\n",
       "     'help': 'Cap acquisition throttle counter',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_cap_revoke_eviction': [{'type': 'counter',\n",
       "     'help': 'Cap Revoke Client Eviction',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_handle_client_request': [{'type': 'counter',\n",
       "     'help': 'Client requests',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_handle_client_session': [{'type': 'counter',\n",
       "     'help': 'Client session messages',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_handle_peer_request': [{'type': 'counter',\n",
       "     'help': 'Peer requests',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_create_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type create latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_create_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type create latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_getattr_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type get attribute latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_getattr_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type get attribute latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_getfilelock_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type get file lock latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_getfilelock_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type get file lock latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_getvxattr_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type get virtual extended attribute latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_getvxattr_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type get virtual extended attribute latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_link_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type link latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_link_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type link latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lookup_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type lookup latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lookup_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type lookup latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lookuphash_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type lookup hash of inode latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lookuphash_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type lookup hash of inode latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lookupino_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type lookup inode latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lookupino_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type lookup inode latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lookupname_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type lookup name latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lookupname_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type lookup name latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lookupparent_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type lookup parent latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lookupparent_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type lookup parent latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lookupsnap_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type lookup snapshot latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lookupsnap_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type lookup snapshot latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lssnap_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type list snapshot latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_lssnap_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type list snapshot latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_mkdir_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type make directory latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_mkdir_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type make directory latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_mknod_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type make node latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_mknod_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type make node latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_mksnap_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type make snapshot latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_mksnap_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type make snapshot latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_open_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type open latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_open_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type open latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_readdir_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type read directory latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_readdir_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type read directory latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_rename_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type rename latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_rename_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type rename latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_renamesnap_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type rename snapshot latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_renamesnap_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type rename snapshot latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_rmdir_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type remove directory latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_rmdir_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type remove directory latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_rmsnap_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type remove snapshot latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_rmsnap_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type remove snapshot latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_rmxattr_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type remove extended attribute latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_rmxattr_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type remove extended attribute latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_setattr_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type set attribute latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_setattr_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type set attribute latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_setdirlayout_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type set directory layout latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_setdirlayout_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type set directory layout latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_setfilelock_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type set file lock latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_setfilelock_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type set file lock latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_setlayout_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type set file layout latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_setlayout_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type set file layout latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_setxattr_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type set extended attribute latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_setxattr_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type set extended attribute latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_symlink_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type symbolic link latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_symlink_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type symbolic link latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_unlink_latency_count': [{'type': 'counter',\n",
       "     'help': 'Request type unlink latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_server_req_unlink_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Request type unlink latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_sessions_average_load': [{'type': 'gauge',\n",
       "     'help': 'Average Load',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_sessions_avg_session_uptime': [{'type': 'gauge',\n",
       "     'help': 'Average session uptime',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_sessions_session_add': [{'type': 'counter',\n",
       "     'help': 'Sessions added',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_sessions_session_count': [{'type': 'gauge',\n",
       "     'help': 'Session count',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_sessions_session_remove': [{'type': 'counter',\n",
       "     'help': 'Sessions removed',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_sessions_sessions_open': [{'type': 'gauge',\n",
       "     'help': 'Sessions currently open',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_sessions_sessions_stale': [{'type': 'gauge',\n",
       "     'help': 'Sessions currently stale',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_sessions_total_load': [{'type': 'gauge',\n",
       "     'help': 'Total Load',\n",
       "     'unit': ''}],\n",
       "   'ceph_mds_subtrees': [{'type': 'gauge', 'help': 'Subtrees', 'unit': ''}],\n",
       "   'ceph_mgr_metadata': [{'type': 'gauge',\n",
       "     'help': 'MGR metadata',\n",
       "     'unit': ''}],\n",
       "   'ceph_mgr_module_can_run': [{'type': 'gauge',\n",
       "     'help': 'MGR module runnable state i.e. can it run (0=no, 1=yes)',\n",
       "     'unit': ''}],\n",
       "   'ceph_mgr_module_status': [{'type': 'gauge',\n",
       "     'help': 'MGR module status (0=disabled, 1=enabled, 2=auto-enabled)',\n",
       "     'unit': ''}],\n",
       "   'ceph_mgr_status': [{'type': 'gauge',\n",
       "     'help': 'MGR status (0=standby, 1=active)',\n",
       "     'unit': ''}],\n",
       "   'ceph_mon_election_call': [{'type': 'counter',\n",
       "     'help': 'Elections started',\n",
       "     'unit': ''}],\n",
       "   'ceph_mon_election_lose': [{'type': 'counter',\n",
       "     'help': 'Elections lost',\n",
       "     'unit': ''}],\n",
       "   'ceph_mon_election_win': [{'type': 'counter',\n",
       "     'help': 'Elections won',\n",
       "     'unit': ''}],\n",
       "   'ceph_mon_metadata': [{'type': 'unknown',\n",
       "     'help': 'MON Metadata',\n",
       "     'unit': ''}],\n",
       "   'ceph_mon_num_elections': [{'type': 'counter',\n",
       "     'help': 'Elections participated in',\n",
       "     'unit': ''}],\n",
       "   'ceph_mon_num_sessions': [{'type': 'gauge',\n",
       "     'help': 'Open sessions',\n",
       "     'unit': ''}],\n",
       "   'ceph_mon_quorum_status': [{'type': 'gauge',\n",
       "     'help': 'Monitors in quorum',\n",
       "     'unit': ''}],\n",
       "   'ceph_mon_session_add': [{'type': 'counter',\n",
       "     'help': 'Created sessions',\n",
       "     'unit': ''}],\n",
       "   'ceph_mon_session_rm': [{'type': 'counter',\n",
       "     'help': 'Removed sessions',\n",
       "     'unit': ''}],\n",
       "   'ceph_mon_session_trim': [{'type': 'counter',\n",
       "     'help': 'Trimmed sessions',\n",
       "     'unit': ''}],\n",
       "   'ceph_num_objects_degraded': [{'type': 'gauge',\n",
       "     'help': 'Number of degraded objects',\n",
       "     'unit': ''}],\n",
       "   'ceph_num_objects_misplaced': [{'type': 'gauge',\n",
       "     'help': 'Number of misplaced objects',\n",
       "     'unit': ''}],\n",
       "   'ceph_num_objects_unfound': [{'type': 'gauge',\n",
       "     'help': 'Number of unfound objects',\n",
       "     'unit': ''}],\n",
       "   'ceph_objecter_0x55dd79b6bb80_op_active': [{'type': 'gauge',\n",
       "     'help': 'Operations active',\n",
       "     'unit': ''}],\n",
       "   'ceph_objecter_0x55dd79b6bb80_op_r': [{'type': 'counter',\n",
       "     'help': 'Read operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_objecter_0x55dd79b6bb80_op_rmw': [{'type': 'counter',\n",
       "     'help': 'Read-modify-write operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_objecter_0x55dd79b6bb80_op_w': [{'type': 'counter',\n",
       "     'help': 'Write operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_objecter_0x55e0dd56db80_op_active': [{'type': 'gauge',\n",
       "     'help': 'Operations active',\n",
       "     'unit': ''}],\n",
       "   'ceph_objecter_0x55e0dd56db80_op_r': [{'type': 'counter',\n",
       "     'help': 'Read operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_objecter_0x55e0dd56db80_op_rmw': [{'type': 'counter',\n",
       "     'help': 'Read-modify-write operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_objecter_0x55e0dd56db80_op_w': [{'type': 'counter',\n",
       "     'help': 'Write operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_objecter_op_active': [{'type': 'gauge',\n",
       "     'help': 'Operations active',\n",
       "     'unit': ''}],\n",
       "   'ceph_objecter_op_r': [{'type': 'counter',\n",
       "     'help': 'Read operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_objecter_op_rmw': [{'type': 'counter',\n",
       "     'help': 'Read-modify-write operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_objecter_op_w': [{'type': 'counter',\n",
       "     'help': 'Write operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_apply_latency_ms': [{'type': 'gauge',\n",
       "     'help': 'OSD stat apply_latency_ms',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_commit_latency_ms': [{'type': 'gauge',\n",
       "     'help': 'OSD stat commit_latency_ms',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_flag_nobackfill': [{'type': 'unknown',\n",
       "     'help': 'OSD Flag nobackfill',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_flag_nodeep_scrub': [{'type': 'unknown',\n",
       "     'help': 'OSD Flag nodeep-scrub',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_flag_nodown': [{'type': 'unknown',\n",
       "     'help': 'OSD Flag nodown',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_flag_noin': [{'type': 'unknown',\n",
       "     'help': 'OSD Flag noin',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_flag_noout': [{'type': 'unknown',\n",
       "     'help': 'OSD Flag noout',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_flag_norebalance': [{'type': 'unknown',\n",
       "     'help': 'OSD Flag norebalance',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_flag_norecover': [{'type': 'unknown',\n",
       "     'help': 'OSD Flag norecover',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_flag_noscrub': [{'type': 'unknown',\n",
       "     'help': 'OSD Flag noscrub',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_flag_noup': [{'type': 'unknown',\n",
       "     'help': 'OSD Flag noup',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_in': [{'type': 'unknown', 'help': 'OSD status in', 'unit': ''}],\n",
       "   'ceph_osd_metadata': [{'type': 'unknown',\n",
       "     'help': 'OSD Metadata',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_numpg': [{'type': 'gauge',\n",
       "     'help': 'Placement groups',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_numpg_removing': [{'type': 'gauge',\n",
       "     'help': 'Placement groups queued for local deletion',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op': [{'type': 'counter',\n",
       "     'help': 'Client operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_in_bytes': [{'type': 'counter',\n",
       "     'help': 'Client operations total write size',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of client operations (including queue time) Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of client operations (including queue time) Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_out_bytes': [{'type': 'counter',\n",
       "     'help': 'Client operations total read size',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_prepare_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of client operations (excluding queue time and wait for finished) Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_prepare_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of client operations (excluding queue time and wait for finished) Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_process_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of client operations (excluding queue time) Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_process_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of client operations (excluding queue time) Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_r': [{'type': 'counter',\n",
       "     'help': 'Client read operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_r_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of read operation (including queue time) Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_r_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of read operation (including queue time) Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_r_out_bytes': [{'type': 'counter',\n",
       "     'help': 'Client data read',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_r_prepare_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of read operations (excluding queue time and wait for finished) Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_r_prepare_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of read operations (excluding queue time and wait for finished) Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_r_process_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of read operation (excluding queue time) Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_r_process_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of read operation (excluding queue time) Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_rw': [{'type': 'counter',\n",
       "     'help': 'Client read-modify-write operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_rw_in_bytes': [{'type': 'counter',\n",
       "     'help': 'Client read-modify-write operations write in',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_rw_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of read-modify-write operation (including queue time) Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_rw_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of read-modify-write operation (including queue time) Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_rw_out_bytes': [{'type': 'counter',\n",
       "     'help': 'Client read-modify-write operations read out ',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_rw_prepare_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of read-modify-write operations (excluding queue time and wait for finished) Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_rw_prepare_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of read-modify-write operations (excluding queue time and wait for finished) Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_rw_process_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of read-modify-write operation (excluding queue time) Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_rw_process_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of read-modify-write operation (excluding queue time) Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_w': [{'type': 'counter',\n",
       "     'help': 'Client write operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_w_in_bytes': [{'type': 'counter',\n",
       "     'help': 'Client data written',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_w_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of write operation (including queue time) Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_w_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of write operation (including queue time) Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_w_prepare_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of write operations (excluding queue time and wait for finished) Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_w_prepare_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of write operations (excluding queue time and wait for finished) Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_w_process_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of write operation (excluding queue time) Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_w_process_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of write operation (excluding queue time) Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_op_wip': [{'type': 'gauge',\n",
       "     'help': 'Replication operations currently being processed (primary)',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_recovery_bytes': [{'type': 'counter',\n",
       "     'help': 'recovery bytes',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_recovery_ops': [{'type': 'counter',\n",
       "     'help': 'Started recovery operations',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_stat_bytes': [{'type': 'gauge', 'help': 'OSD size', 'unit': ''}],\n",
       "   'ceph_osd_stat_bytes_used': [{'type': 'gauge',\n",
       "     'help': 'Used space',\n",
       "     'unit': ''}],\n",
       "   'ceph_osd_up': [{'type': 'unknown', 'help': 'OSD status up', 'unit': ''}],\n",
       "   'ceph_osd_weight': [{'type': 'unknown',\n",
       "     'help': 'OSD status weight',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_accept_timeout': [{'type': 'counter',\n",
       "     'help': 'Accept timeouts',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_begin': [{'type': 'counter',\n",
       "     'help': 'Started and handled begins',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_begin_bytes_count': [{'type': 'counter',\n",
       "     'help': 'Data in transaction on begin Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_begin_bytes_sum': [{'type': 'counter',\n",
       "     'help': 'Data in transaction on begin Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_begin_keys_count': [{'type': 'counter',\n",
       "     'help': 'Keys in transaction on begin Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_begin_keys_sum': [{'type': 'counter',\n",
       "     'help': 'Keys in transaction on begin Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_begin_latency_count': [{'type': 'counter',\n",
       "     'help': 'Latency of begin operation Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_begin_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Latency of begin operation Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_collect': [{'type': 'counter',\n",
       "     'help': 'Peon collects',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_collect_bytes_count': [{'type': 'counter',\n",
       "     'help': 'Data in transaction on peon collect Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_collect_bytes_sum': [{'type': 'counter',\n",
       "     'help': 'Data in transaction on peon collect Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_collect_keys_count': [{'type': 'counter',\n",
       "     'help': 'Keys in transaction on peon collect Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_collect_keys_sum': [{'type': 'counter',\n",
       "     'help': 'Keys in transaction on peon collect Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_collect_latency_count': [{'type': 'counter',\n",
       "     'help': 'Peon collect latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_collect_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Peon collect latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_collect_timeout': [{'type': 'counter',\n",
       "     'help': 'Collect timeouts',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_collect_uncommitted': [{'type': 'counter',\n",
       "     'help': 'Uncommitted values in started and handled collects',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_commit': [{'type': 'counter', 'help': 'Commits', 'unit': ''}],\n",
       "   'ceph_paxos_commit_bytes_count': [{'type': 'counter',\n",
       "     'help': 'Data in transaction on commit Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_commit_bytes_sum': [{'type': 'counter',\n",
       "     'help': 'Data in transaction on commit Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_commit_keys_count': [{'type': 'counter',\n",
       "     'help': 'Keys in transaction on commit Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_commit_keys_sum': [{'type': 'counter',\n",
       "     'help': 'Keys in transaction on commit Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_commit_latency_count': [{'type': 'counter',\n",
       "     'help': 'Commit latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_commit_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Commit latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_lease_ack_timeout': [{'type': 'counter',\n",
       "     'help': 'Lease acknowledgement timeouts',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_lease_timeout': [{'type': 'counter',\n",
       "     'help': 'Lease timeouts',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_new_pn': [{'type': 'counter',\n",
       "     'help': 'New proposal number queries',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_new_pn_latency_count': [{'type': 'counter',\n",
       "     'help': 'New proposal number getting latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_new_pn_latency_sum': [{'type': 'counter',\n",
       "     'help': 'New proposal number getting latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_refresh': [{'type': 'counter',\n",
       "     'help': 'Refreshes',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_refresh_latency_count': [{'type': 'counter',\n",
       "     'help': 'Refresh latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_refresh_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Refresh latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_restart': [{'type': 'counter', 'help': 'Restarts', 'unit': ''}],\n",
       "   'ceph_paxos_share_state': [{'type': 'counter',\n",
       "     'help': 'Sharings of state',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_share_state_bytes_count': [{'type': 'counter',\n",
       "     'help': 'Data in shared state Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_share_state_bytes_sum': [{'type': 'counter',\n",
       "     'help': 'Data in shared state Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_share_state_keys_count': [{'type': 'counter',\n",
       "     'help': 'Keys in shared state Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_share_state_keys_sum': [{'type': 'counter',\n",
       "     'help': 'Keys in shared state Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_start_leader': [{'type': 'counter',\n",
       "     'help': 'Starts in leader role',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_start_peon': [{'type': 'counter',\n",
       "     'help': 'Starts in peon role',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_store_state': [{'type': 'counter',\n",
       "     'help': 'Store a shared state on disk',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_store_state_bytes_count': [{'type': 'counter',\n",
       "     'help': 'Data in transaction in stored state Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_store_state_bytes_sum': [{'type': 'counter',\n",
       "     'help': 'Data in transaction in stored state Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_store_state_keys_count': [{'type': 'counter',\n",
       "     'help': 'Keys in transaction in stored state Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_store_state_keys_sum': [{'type': 'counter',\n",
       "     'help': 'Keys in transaction in stored state Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_store_state_latency_count': [{'type': 'counter',\n",
       "     'help': 'Storing state latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_paxos_store_state_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Storing state latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_activating': [{'type': 'gauge',\n",
       "     'help': 'PG activating per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_active': [{'type': 'gauge',\n",
       "     'help': 'PG active per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_backfill_toofull': [{'type': 'gauge',\n",
       "     'help': 'PG backfill_toofull per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_backfill_unfound': [{'type': 'gauge',\n",
       "     'help': 'PG backfill_unfound per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_backfill_wait': [{'type': 'gauge',\n",
       "     'help': 'PG backfill_wait per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_backfilling': [{'type': 'gauge',\n",
       "     'help': 'PG backfilling per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_clean': [{'type': 'gauge',\n",
       "     'help': 'PG clean per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_creating': [{'type': 'gauge',\n",
       "     'help': 'PG creating per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_deep': [{'type': 'gauge', 'help': 'PG deep per pool', 'unit': ''}],\n",
       "   'ceph_pg_degraded': [{'type': 'gauge',\n",
       "     'help': 'PG degraded per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_down': [{'type': 'gauge', 'help': 'PG down per pool', 'unit': ''}],\n",
       "   'ceph_pg_failed_repair': [{'type': 'gauge',\n",
       "     'help': 'PG failed_repair per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_forced_backfill': [{'type': 'gauge',\n",
       "     'help': 'PG forced_backfill per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_forced_recovery': [{'type': 'gauge',\n",
       "     'help': 'PG forced_recovery per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_incomplete': [{'type': 'gauge',\n",
       "     'help': 'PG incomplete per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_inconsistent': [{'type': 'gauge',\n",
       "     'help': 'PG inconsistent per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_laggy': [{'type': 'gauge',\n",
       "     'help': 'PG laggy per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_peered': [{'type': 'gauge',\n",
       "     'help': 'PG peered per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_peering': [{'type': 'gauge',\n",
       "     'help': 'PG peering per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_premerge': [{'type': 'gauge',\n",
       "     'help': 'PG premerge per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_recovering': [{'type': 'gauge',\n",
       "     'help': 'PG recovering per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_recovery_toofull': [{'type': 'gauge',\n",
       "     'help': 'PG recovery_toofull per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_recovery_unfound': [{'type': 'gauge',\n",
       "     'help': 'PG recovery_unfound per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_recovery_wait': [{'type': 'gauge',\n",
       "     'help': 'PG recovery_wait per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_remapped': [{'type': 'gauge',\n",
       "     'help': 'PG remapped per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_repair': [{'type': 'gauge',\n",
       "     'help': 'PG repair per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_scrubbing': [{'type': 'gauge',\n",
       "     'help': 'PG scrubbing per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_snaptrim': [{'type': 'gauge',\n",
       "     'help': 'PG snaptrim per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_snaptrim_error': [{'type': 'gauge',\n",
       "     'help': 'PG snaptrim_error per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_snaptrim_wait': [{'type': 'gauge',\n",
       "     'help': 'PG snaptrim_wait per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_stale': [{'type': 'gauge',\n",
       "     'help': 'PG stale per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_total': [{'type': 'gauge',\n",
       "     'help': 'PG Total Count per Pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_undersized': [{'type': 'gauge',\n",
       "     'help': 'PG undersized per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_unknown': [{'type': 'gauge',\n",
       "     'help': 'PG unknown per pool',\n",
       "     'unit': ''}],\n",
       "   'ceph_pg_wait': [{'type': 'gauge', 'help': 'PG wait per pool', 'unit': ''}],\n",
       "   'ceph_pool_avail_raw': [{'type': 'gauge',\n",
       "     'help': 'DF pool avail_raw',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_bytes_used': [{'type': 'gauge',\n",
       "     'help': 'DF pool bytes_used',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_compress_bytes_used': [{'type': 'gauge',\n",
       "     'help': 'DF pool compress_bytes_used',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_compress_under_bytes': [{'type': 'gauge',\n",
       "     'help': 'DF pool compress_under_bytes',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_dirty': [{'type': 'gauge', 'help': 'DF pool dirty', 'unit': ''}],\n",
       "   'ceph_pool_max_avail': [{'type': 'gauge',\n",
       "     'help': 'DF pool max_avail',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_metadata': [{'type': 'unknown',\n",
       "     'help': 'POOL Metadata',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_num_bytes_recovered': [{'type': 'gauge',\n",
       "     'help': 'OSD pool stats: num_bytes_recovered',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_num_objects_recovered': [{'type': 'gauge',\n",
       "     'help': 'OSD pool stats: num_objects_recovered',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_objects': [{'type': 'gauge',\n",
       "     'help': 'DF pool objects',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_percent_used': [{'type': 'gauge',\n",
       "     'help': 'DF pool percent_used',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_quota_bytes': [{'type': 'gauge',\n",
       "     'help': 'DF pool quota_bytes',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_quota_objects': [{'type': 'gauge',\n",
       "     'help': 'DF pool quota_objects',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_rd': [{'type': 'counter', 'help': 'DF pool rd', 'unit': ''}],\n",
       "   'ceph_pool_rd_bytes': [{'type': 'counter',\n",
       "     'help': 'DF pool rd_bytes',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_recovering_bytes_per_sec': [{'type': 'gauge',\n",
       "     'help': 'OSD pool stats: recovering_bytes_per_sec',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_recovering_keys_per_sec': [{'type': 'gauge',\n",
       "     'help': 'OSD pool stats: recovering_keys_per_sec',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_recovering_objects_per_sec': [{'type': 'gauge',\n",
       "     'help': 'OSD pool stats: recovering_objects_per_sec',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_stored': [{'type': 'gauge',\n",
       "     'help': 'DF pool stored',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_stored_raw': [{'type': 'gauge',\n",
       "     'help': 'DF pool stored_raw',\n",
       "     'unit': ''}],\n",
       "   'ceph_pool_wr': [{'type': 'counter', 'help': 'DF pool wr', 'unit': ''}],\n",
       "   'ceph_pool_wr_bytes': [{'type': 'counter',\n",
       "     'help': 'DF pool wr_bytes',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_committed_bytes': [{'type': 'gauge',\n",
       "     'help': 'total bytes committed,',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_pri0_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri0',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_pri10_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri10',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_pri11_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri11',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_pri1_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri1',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_pri2_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri2',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_pri3_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri3',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_pri4_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri4',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_pri5_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri5',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_pri6_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri6',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_pri7_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri7',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_pri8_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri8',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_pri9_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri9',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:full_reserved_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes reserved for future growth.',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_committed_bytes': [{'type': 'gauge',\n",
       "     'help': 'total bytes committed,',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_pri0_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri0',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_pri10_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri10',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_pri11_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri11',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_pri1_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri1',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_pri2_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri2',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_pri3_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri3',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_pri4_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri4',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_pri5_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri5',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_pri6_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri6',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_pri7_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri7',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_pri8_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri8',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_pri9_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri9',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:inc_reserved_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes reserved for future growth.',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_committed_bytes': [{'type': 'gauge',\n",
       "     'help': 'total bytes committed,',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_pri0_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri0',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_pri10_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri10',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_pri11_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri11',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_pri1_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri1',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_pri2_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri2',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_pri3_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri3',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_pri4_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri4',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_pri5_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri5',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_pri6_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri6',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_pri7_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri7',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_pri8_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri8',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_pri9_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes allocated to pri9',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache:kv_reserved_bytes': [{'type': 'gauge',\n",
       "     'help': 'bytes reserved for future growth.',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache_cache_bytes': [{'type': 'gauge',\n",
       "     'help': 'current memory available for caches.',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache_heap_bytes': [{'type': 'gauge',\n",
       "     'help': 'aggregate bytes in use by the heap',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache_mapped_bytes': [{'type': 'gauge',\n",
       "     'help': 'total bytes mapped by the process',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache_target_bytes': [{'type': 'gauge',\n",
       "     'help': 'target process memory usage in bytes',\n",
       "     'unit': ''}],\n",
       "   'ceph_prioritycache_unmapped_bytes': [{'type': 'gauge',\n",
       "     'help': 'unmapped bytes that the kernel has yet to reclaimed',\n",
       "     'unit': ''}],\n",
       "   'ceph_prometheus_collect_duration_seconds_count': [{'type': 'counter',\n",
       "     'help': 'The amount of metrics gathered for this exporter',\n",
       "     'unit': ''}],\n",
       "   'ceph_prometheus_collect_duration_seconds_sum': [{'type': 'counter',\n",
       "     'help': 'The sum of seconds took to collect all metrics of this exporter',\n",
       "     'unit': ''}],\n",
       "   'ceph_purge_queue_pq_executed': [{'type': 'counter',\n",
       "     'help': 'Purge queue tasks executed',\n",
       "     'unit': ''}],\n",
       "   'ceph_purge_queue_pq_executing': [{'type': 'gauge',\n",
       "     'help': 'Purge queue tasks in flight',\n",
       "     'unit': ''}],\n",
       "   'ceph_purge_queue_pq_executing_high_water': [{'type': 'gauge',\n",
       "     'help': 'Maximum number of executing file purges',\n",
       "     'unit': ''}],\n",
       "   'ceph_purge_queue_pq_executing_ops': [{'type': 'gauge',\n",
       "     'help': 'Purge queue ops in flight',\n",
       "     'unit': ''}],\n",
       "   'ceph_purge_queue_pq_executing_ops_high_water': [{'type': 'gauge',\n",
       "     'help': 'Maximum number of executing file purge ops',\n",
       "     'unit': ''}],\n",
       "   'ceph_purge_queue_pq_item_in_journal': [{'type': 'gauge',\n",
       "     'help': 'Purge item left in journal',\n",
       "     'unit': ''}],\n",
       "   'ceph_rbd_mirror_metadata': [{'type': 'unknown',\n",
       "     'help': 'RBD Mirror Metadata',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_cache_hit': [{'type': 'counter',\n",
       "     'help': 'Cache hits',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_cache_miss': [{'type': 'counter',\n",
       "     'help': 'Cache miss',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_failed_req': [{'type': 'counter',\n",
       "     'help': 'Aborted requests',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_gc_retire_object': [{'type': 'counter',\n",
       "     'help': 'GC object retires',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_get': [{'type': 'counter', 'help': 'Gets', 'unit': ''}],\n",
       "   'ceph_rgw_get_b': [{'type': 'counter', 'help': 'Size of gets', 'unit': ''}],\n",
       "   'ceph_rgw_get_initial_lat_count': [{'type': 'counter',\n",
       "     'help': 'Get latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_get_initial_lat_sum': [{'type': 'counter',\n",
       "     'help': 'Get latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_keystone_token_cache_hit': [{'type': 'counter',\n",
       "     'help': 'Keystone token cache hits',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_keystone_token_cache_miss': [{'type': 'counter',\n",
       "     'help': 'Keystone token cache miss',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_lc_abort_mpu': [{'type': 'counter',\n",
       "     'help': 'Lifecycle abort multipart upload',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_lc_expire_current': [{'type': 'counter',\n",
       "     'help': 'Lifecycle current expiration',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_lc_expire_dm': [{'type': 'counter',\n",
       "     'help': 'Lifecycle delete-marker expiration',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_lc_expire_noncurrent': [{'type': 'counter',\n",
       "     'help': 'Lifecycle non-current expiration',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_lc_transition_current': [{'type': 'counter',\n",
       "     'help': 'Lifecycle current transition',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_lc_transition_noncurrent': [{'type': 'counter',\n",
       "     'help': 'Lifecycle non-current transition',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_metadata': [{'type': 'unknown',\n",
       "     'help': 'RGW Metadata',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_pubsub_event_lost': [{'type': 'counter',\n",
       "     'help': 'Pubsub events lost',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_pubsub_event_triggered': [{'type': 'counter',\n",
       "     'help': 'Pubsub events with at least one topic',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_pubsub_events': [{'type': 'gauge',\n",
       "     'help': 'Pubsub events in store',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_pubsub_missing_conf': [{'type': 'counter',\n",
       "     'help': 'Pubsub events could not be handled because of missing configuration',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_pubsub_push_failed': [{'type': 'counter',\n",
       "     'help': 'Pubsub events failed to be pushed to an endpoint',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_pubsub_push_ok': [{'type': 'counter',\n",
       "     'help': 'Pubsub events pushed to an endpoint',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_pubsub_push_pending': [{'type': 'gauge',\n",
       "     'help': 'Pubsub events pending reply from endpoint',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_pubsub_store_fail': [{'type': 'counter',\n",
       "     'help': 'Pubsub events failed to be stored',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_pubsub_store_ok': [{'type': 'counter',\n",
       "     'help': 'Pubsub events successfully stored',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_put': [{'type': 'counter', 'help': 'Puts', 'unit': ''}],\n",
       "   'ceph_rgw_put_b': [{'type': 'counter', 'help': 'Size of puts', 'unit': ''}],\n",
       "   'ceph_rgw_put_initial_lat_count': [{'type': 'counter',\n",
       "     'help': 'Put latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_put_initial_lat_sum': [{'type': 'counter',\n",
       "     'help': 'Put latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_qactive': [{'type': 'gauge',\n",
       "     'help': 'Active requests queue',\n",
       "     'unit': ''}],\n",
       "   'ceph_rgw_qlen': [{'type': 'gauge', 'help': 'Queue length', 'unit': ''}],\n",
       "   'ceph_rgw_req': [{'type': 'counter', 'help': 'Requests', 'unit': ''}],\n",
       "   'ceph_rocksdb_compact': [{'type': 'counter',\n",
       "     'help': 'Compactions',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_compact_queue_len': [{'type': 'gauge',\n",
       "     'help': 'Length of compaction queue',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_compact_queue_merge': [{'type': 'counter',\n",
       "     'help': 'Mergings of ranges in compaction queue',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_compact_range': [{'type': 'counter',\n",
       "     'help': 'Compactions by range',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_get': [{'type': 'counter', 'help': 'Gets', 'unit': ''}],\n",
       "   'ceph_rocksdb_get_latency_count': [{'type': 'counter',\n",
       "     'help': 'Get latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_get_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Get latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_rocksdb_write_delay_time_count': [{'type': 'counter',\n",
       "     'help': 'Rocksdb write delay time Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_rocksdb_write_delay_time_sum': [{'type': 'counter',\n",
       "     'help': 'Rocksdb write delay time Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_rocksdb_write_memtable_time_count': [{'type': 'counter',\n",
       "     'help': 'Rocksdb write memtable time Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_rocksdb_write_memtable_time_sum': [{'type': 'counter',\n",
       "     'help': 'Rocksdb write memtable time Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_rocksdb_write_pre_and_post_time_count': [{'type': 'counter',\n",
       "     'help': 'total time spent on writing a record, excluding write process Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_rocksdb_write_pre_and_post_time_sum': [{'type': 'counter',\n",
       "     'help': 'total time spent on writing a record, excluding write process Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_rocksdb_write_wal_time_count': [{'type': 'counter',\n",
       "     'help': 'Rocksdb write wal time Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_rocksdb_write_wal_time_sum': [{'type': 'counter',\n",
       "     'help': 'Rocksdb write wal time Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_submit_latency_count': [{'type': 'counter',\n",
       "     'help': 'Submit Latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_submit_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Submit Latency Total',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_submit_sync_latency_count': [{'type': 'counter',\n",
       "     'help': 'Submit Sync Latency Count',\n",
       "     'unit': ''}],\n",
       "   'ceph_rocksdb_submit_sync_latency_sum': [{'type': 'counter',\n",
       "     'help': 'Submit Sync Latency Total',\n",
       "     'unit': ''}],\n",
       "   'certwatcher_read_certificate_errors_total': [{'type': 'counter',\n",
       "     'help': 'Total number of certificate read errors',\n",
       "     'unit': ''}],\n",
       "   'certwatcher_read_certificate_total': [{'type': 'counter',\n",
       "     'help': 'Total number of certificate reads',\n",
       "     'unit': ''}],\n",
       "   'cluster_default_node_selector': [{'type': 'gauge',\n",
       "     'help': 'Reports whether the cluster scheduler is configured with a default node selector.',\n",
       "     'unit': ''}],\n",
       "   'cluster_feature_set': [{'type': 'gauge',\n",
       "     'help': 'Reports the feature set the cluster is configured to expose. name corresponds to the featureSet field of the cluster. The value is 1 if a cloud provider is supported.',\n",
       "     'unit': ''}],\n",
       "   'cluster_infrastructure_provider': [{'type': 'gauge',\n",
       "     'help': 'Reports whether the cluster is configured with an infrastructure provider. type is unset if no cloud provider is recognized or set to the constant used by the Infrastructure config. region is set when the cluster clearly identifies a region within the provider. The value is 1 if a cloud provider is set or 0 if it is unset.',\n",
       "     'unit': ''}],\n",
       "   'cluster_installer': [{'type': 'gauge',\n",
       "     'help': \"Reports info about the installation process and, if applicable, the install tool. The type is either 'openshift-install', indicating that openshift-install was used to install the cluster, or 'other', indicating that an unknown process installed the cluster. The invoker is 'user' by default, but it may be overridden by a consuming tool. The version reported is that of the openshift-install that was used to generate the manifests and, if applicable, provision the infrastructure.\",\n",
       "     'unit': ''}],\n",
       "   'cluster_legacy_scheduler_policy': [{'type': 'gauge',\n",
       "     'help': 'Reports whether the cluster scheduler is configured with a legacy KubeScheduler Policy.',\n",
       "     'unit': ''}],\n",
       "   'cluster_master_schedulable': [{'type': 'gauge',\n",
       "     'help': 'Reports whether the cluster master nodes are schedulable.',\n",
       "     'unit': ''}],\n",
       "   'cluster_monitoring_operator_last_reconciliation_successful': [{'type': 'gauge',\n",
       "     'help': 'Latest reconciliation state. Set to 1 if last reconciliation succeeded, else 0.',\n",
       "     'unit': ''}],\n",
       "   'cluster_monitoring_operator_reconcile_attempts_total': [{'type': 'counter',\n",
       "     'help': 'Number of attempts to reconcile the operator configuration',\n",
       "     'unit': ''}],\n",
       "   'cluster_operator_condition_transitions': [{'type': 'gauge',\n",
       "     'help': 'Reports the number of times that a condition on a cluster operator changes status',\n",
       "     'unit': ''}],\n",
       "   'cluster_operator_conditions': [{'type': 'gauge',\n",
       "     'help': 'Report the conditions for active cluster operators. 0 is False and 1 is True.',\n",
       "     'unit': ''}],\n",
       "   'cluster_operator_payload_errors': [{'type': 'gauge',\n",
       "     'help': 'Report the number of errors encountered applying the payload.',\n",
       "     'unit': ''}],\n",
       "   'cluster_operator_up': [{'type': 'gauge',\n",
       "     'help': 'Reports key highlights of the active cluster operators.',\n",
       "     'unit': ''}],\n",
       "   'cluster_proxy_enabled': [{'type': 'gauge',\n",
       "     'help': 'Reports whether the cluster has been configured to use a proxy. type is which type of proxy configuration has been set - http for an http proxy, https for an https proxy, and trusted_ca if a custom CA was specified.',\n",
       "     'unit': ''}],\n",
       "   'cluster_version': [{'type': 'gauge',\n",
       "     'help': \"Reports the version of the cluster in terms of seconds since\\nthe epoch. Type 'current' is the version being applied and\\nthe value is the creation date of the payload. The type\\n'desired' is returned if spec.desiredUpdate is set but the\\noperator has not yet updated and the value is the most \\nrecent status transition time. The type 'failure' is set \\nif an error is preventing sync or upgrade with the last \\ntransition timestamp of the condition. The type 'completed' \\nis the timestamp when the last image was successfully\\napplied. The type 'cluster' is the creation date of the\\ncluster version object and the current version. The type\\n'updating' is set when the cluster is transitioning to a\\nnew version but has not reached the completed state and\\nis the time the update was started. The type 'initial' is\\nset to the oldest entry in the history. The from_version label\\nwill be set to the last completed version, the initial\\nversion for 'cluster', or empty for 'initial'.\\n.\",\n",
       "     'unit': ''}],\n",
       "   'cluster_version_available_updates': [{'type': 'gauge',\n",
       "     'help': 'Report the count of available versions for an upstream and channel.',\n",
       "     'unit': ''}],\n",
       "   'cluster_version_capability': [{'type': 'gauge',\n",
       "     'help': 'Report currently enabled cluster capabilities.  0 is disabled, and 1 is enabled.',\n",
       "     'unit': ''}],\n",
       "   'cluster_version_operator_update_retrieval_timestamp_seconds': [{'type': 'gauge',\n",
       "     'help': 'Reports when updates were last successfully retrieved.',\n",
       "     'unit': ''}],\n",
       "   'cluster_version_payload': [{'type': 'gauge',\n",
       "     'help': 'Report the number of entries in the payload.',\n",
       "     'unit': ''}],\n",
       "   'console_url': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] URL of the console exposed on the cluster',\n",
       "     'unit': ''}],\n",
       "   'container_blkio_device_usage_total': [{'type': 'counter',\n",
       "     'help': 'Blkio Device bytes usage',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_cfs_periods_total': [{'type': 'counter',\n",
       "     'help': 'Number of elapsed enforcement period intervals.',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_cfs_throttled_periods_total': [{'type': 'counter',\n",
       "     'help': 'Number of throttled period intervals.',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_cfs_throttled_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Total time duration the container has been throttled.',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_load_average_10s': [{'type': 'gauge',\n",
       "     'help': 'Value of container cpu load average over the last 10 seconds.',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_system_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative system cpu time consumed in seconds.',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_usage_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative cpu time consumed in seconds.',\n",
       "     'unit': ''}],\n",
       "   'container_cpu_user_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative user cpu time consumed in seconds.',\n",
       "     'unit': ''}],\n",
       "   'container_file_descriptors': [{'type': 'gauge',\n",
       "     'help': 'Number of open file descriptors for the container.',\n",
       "     'unit': ''}],\n",
       "   'container_fs_inodes_free': [{'type': 'gauge',\n",
       "     'help': 'Number of available Inodes',\n",
       "     'unit': ''}],\n",
       "   'container_fs_inodes_total': [{'type': 'gauge',\n",
       "     'help': 'Number of Inodes',\n",
       "     'unit': ''}],\n",
       "   'container_fs_io_current': [{'type': 'gauge',\n",
       "     'help': 'Number of I/Os currently in progress',\n",
       "     'unit': ''}],\n",
       "   'container_fs_io_time_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of seconds spent doing I/Os',\n",
       "     'unit': ''}],\n",
       "   'container_fs_io_time_weighted_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative weighted I/O time in seconds',\n",
       "     'unit': ''}],\n",
       "   'container_fs_limit_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes that can be consumed by the container on this filesystem.',\n",
       "     'unit': ''}],\n",
       "   'container_fs_read_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of seconds spent reading',\n",
       "     'unit': ''}],\n",
       "   'container_fs_reads_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of bytes read',\n",
       "     'unit': ''}],\n",
       "   'container_fs_reads_merged_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of reads merged',\n",
       "     'unit': ''}],\n",
       "   'container_fs_reads_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of reads completed',\n",
       "     'unit': ''}],\n",
       "   'container_fs_sector_reads_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of sector reads completed',\n",
       "     'unit': ''}],\n",
       "   'container_fs_sector_writes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of sector writes completed',\n",
       "     'unit': ''}],\n",
       "   'container_fs_usage_bytes': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes that are consumed by the container on this filesystem.',\n",
       "     'unit': ''}],\n",
       "   'container_fs_write_seconds_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of seconds spent writing',\n",
       "     'unit': ''}],\n",
       "   'container_fs_writes_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of bytes written',\n",
       "     'unit': ''}],\n",
       "   'container_fs_writes_merged_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of writes merged',\n",
       "     'unit': ''}],\n",
       "   'container_fs_writes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of writes completed',\n",
       "     'unit': ''}],\n",
       "   'container_last_seen': [{'type': 'gauge',\n",
       "     'help': 'Last time a container was seen by the exporter',\n",
       "     'unit': ''}],\n",
       "   'container_memory_cache': [{'type': 'gauge',\n",
       "     'help': 'Number of bytes of page cache memory.',\n",
       "     'unit': ''}],\n",
       "   'container_memory_failcnt': [{'type': 'counter',\n",
       "     'help': 'Number of memory usage hits limits',\n",
       "     'unit': ''}],\n",
       "   'container_memory_failures_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of memory allocation failures.',\n",
       "     'unit': ''}],\n",
       "   'container_memory_mapped_file': [{'type': 'gauge',\n",
       "     'help': 'Size of memory mapped files in bytes.',\n",
       "     'unit': ''}],\n",
       "   'container_memory_max_usage_bytes': [{'type': 'gauge',\n",
       "     'help': 'Maximum memory usage recorded in bytes',\n",
       "     'unit': ''}],\n",
       "   'container_memory_rss': [{'type': 'gauge',\n",
       "     'help': 'Size of RSS in bytes.',\n",
       "     'unit': ''}],\n",
       "   'container_memory_swap': [{'type': 'gauge',\n",
       "     'help': 'Container swap usage in bytes.',\n",
       "     'unit': ''}],\n",
       "   'container_memory_usage_bytes': [{'type': 'gauge',\n",
       "     'help': 'Current memory usage in bytes, including all memory regardless of when it was accessed',\n",
       "     'unit': ''}],\n",
       "   'container_memory_working_set_bytes': [{'type': 'gauge',\n",
       "     'help': 'Current working set in bytes.',\n",
       "     'unit': ''}],\n",
       "   'container_network_receive_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of bytes received',\n",
       "     'unit': ''}],\n",
       "   'container_network_receive_errors_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of errors encountered while receiving',\n",
       "     'unit': ''}],\n",
       "   'container_network_receive_packets_dropped_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of packets dropped while receiving',\n",
       "     'unit': ''}],\n",
       "   'container_network_receive_packets_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of packets received',\n",
       "     'unit': ''}],\n",
       "   'container_network_transmit_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of bytes transmitted',\n",
       "     'unit': ''}],\n",
       "   'container_network_transmit_errors_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of errors encountered while transmitting',\n",
       "     'unit': ''}],\n",
       "   'container_network_transmit_packets_dropped_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of packets dropped while transmitting',\n",
       "     'unit': ''}],\n",
       "   'container_network_transmit_packets_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of packets transmitted',\n",
       "     'unit': ''}],\n",
       "   'container_oom_events_total': [{'type': 'counter',\n",
       "     'help': 'Count of out of memory events observed for the container',\n",
       "     'unit': ''}],\n",
       "   'container_processes': [{'type': 'gauge',\n",
       "     'help': 'Number of processes running inside the container.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_containers_oom_total': [{'type': 'counter',\n",
       "     'help': 'Amount of containers killed because they ran out of memory (OOM)',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_image_layer_reuse_total': [{'type': 'counter',\n",
       "     'help': 'Reused (not pulled) local image layer count by name',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_image_pulls_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Bytes transferred by CRI-O image pulls',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_image_pulls_failure_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative number of CRI-O image pull failures by error.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_image_pulls_layer_size': [{'type': 'histogram',\n",
       "     'help': 'Bytes transferred by CRI-O image pulls per layer',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_image_pulls_skipped_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Bytes skipped by CRI-O image pulls',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_image_pulls_success_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative number of CRI-O image pull successes.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations': [{'type': 'counter',\n",
       "     'help': '[DEPRECATED: in favour of `operations_total`] Cumulative number of CRI-O operations by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_errors': [{'type': 'counter',\n",
       "     'help': '[DEPRECATED: in favour of `operations_errors_total`] Cumulative number of CRI-O operation errors by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_errors_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative number of CRI-O operation errors by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_latency_microseconds': [{'type': 'gauge',\n",
       "     'help': '[DEPRECATED: in favour of `operations_latency_seconds`] Latency in microseconds of individual CRI calls for CRI-O operations. Broken down by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_latency_microseconds_total': [{'type': 'summary',\n",
       "     'help': '[DEPRECATED:  in favour of `operations_latency_seconds_total`] Latency in microseconds of CRI-O operations. Broken down by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_latency_seconds': [{'type': 'gauge',\n",
       "     'help': 'Latency in seconds of individual CRI calls for CRI-O operations. Broken down by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_latency_seconds_total': [{'type': 'summary',\n",
       "     'help': 'Latency in seconds of CRI-O operations. Broken down by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_operations_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative number of CRI-O operations by operation type.',\n",
       "     'unit': ''}],\n",
       "   'container_runtime_crio_processes_defunct': [{'type': 'gauge',\n",
       "     'help': 'Total number of defunct processes in the node',\n",
       "     'unit': ''}],\n",
       "   'container_scrape_error': [{'type': 'gauge',\n",
       "     'help': '1 if there was an error while getting container metrics, 0 otherwise',\n",
       "     'unit': ''}],\n",
       "   'container_sockets': [{'type': 'gauge',\n",
       "     'help': 'Number of open sockets for the container.',\n",
       "     'unit': ''}],\n",
       "   'container_spec_cpu_period': [{'type': 'gauge',\n",
       "     'help': 'CPU period of the container.',\n",
       "     'unit': ''}],\n",
       "   'container_spec_cpu_quota': [{'type': 'gauge',\n",
       "     'help': 'CPU quota of the container.',\n",
       "     'unit': ''}],\n",
       "   'container_spec_cpu_shares': [{'type': 'gauge',\n",
       "     'help': 'CPU share of the container.',\n",
       "     'unit': ''}],\n",
       "   'container_spec_memory_limit_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory limit for the container.',\n",
       "     'unit': ''}],\n",
       "   'container_spec_memory_reservation_limit_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory reservation limit for the container.',\n",
       "     'unit': ''}],\n",
       "   'container_spec_memory_swap_limit_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory swap limit for the container.',\n",
       "     'unit': ''}],\n",
       "   'container_start_time_seconds': [{'type': 'gauge',\n",
       "     'help': 'Start time of the container since unix epoch in seconds.',\n",
       "     'unit': ''}],\n",
       "   'container_tasks_state': [{'type': 'gauge',\n",
       "     'help': 'Number of tasks in given state',\n",
       "     'unit': ''}],\n",
       "   'container_threads': [{'type': 'gauge',\n",
       "     'help': 'Number of threads running inside the container',\n",
       "     'unit': ''}],\n",
       "   'container_threads_max': [{'type': 'gauge',\n",
       "     'help': 'Maximum number of threads allowed inside the container, infinity if value is zero',\n",
       "     'unit': ''}],\n",
       "   'container_ulimits_soft': [{'type': 'gauge',\n",
       "     'help': 'Soft ulimit values for the container root process. Unlimited if -1, except priority and nice',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_active_workers': [{'type': 'gauge',\n",
       "     'help': 'Number of currently used workers per controller',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_max_concurrent_reconciles': [{'type': 'gauge',\n",
       "     'help': 'Maximum number of concurrent reconciles per controller',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_reconcile_errors_total': [{'type': 'counter',\n",
       "     'help': 'Total number of reconciliation errors per controller',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_reconcile_time_seconds': [{'type': 'histogram',\n",
       "     'help': 'Length of time per reconciliation per controller',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_reconcile_total': [{'type': 'counter',\n",
       "     'help': 'Total number of reconciliations per controller',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_webhook_requests_in_flight': [{'type': 'gauge',\n",
       "     'help': 'Current number of admission requests being served.',\n",
       "     'unit': ''}],\n",
       "   'controller_runtime_webhook_requests_total': [{'type': 'counter',\n",
       "     'help': 'Total number of admission requests by HTTP status code.',\n",
       "     'unit': ''}],\n",
       "   'coredns_build_info': [{'type': 'gauge',\n",
       "     'help': \"A metric with a constant '1' value labeled by version, revision, and goversion from which CoreDNS was built.\",\n",
       "     'unit': ''}],\n",
       "   'coredns_cache_entries': [{'type': 'gauge',\n",
       "     'help': 'The number of elements in the cache.',\n",
       "     'unit': ''}],\n",
       "   'coredns_cache_hits_total': [{'type': 'counter',\n",
       "     'help': 'The count of cache hits.',\n",
       "     'unit': ''}],\n",
       "   'coredns_cache_misses_total': [{'type': 'counter',\n",
       "     'help': 'The count of cache misses. Deprecated, derive misses from cache hits/requests counters.',\n",
       "     'unit': ''}],\n",
       "   'coredns_cache_requests_total': [{'type': 'counter',\n",
       "     'help': 'The count of cache requests.',\n",
       "     'unit': ''}],\n",
       "   'coredns_dns_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of the time (in seconds) each request took per zone.',\n",
       "     'unit': ''}],\n",
       "   'coredns_dns_request_size_bytes': [{'type': 'histogram',\n",
       "     'help': 'Size of the EDNS0 UDP buffer in bytes (64K for TCP) per zone and protocol.',\n",
       "     'unit': ''}],\n",
       "   'coredns_dns_requests_total': [{'type': 'counter',\n",
       "     'help': 'Counter of DNS requests made per zone, protocol and family.',\n",
       "     'unit': ''}],\n",
       "   'coredns_dns_response_size_bytes': [{'type': 'histogram',\n",
       "     'help': 'Size of the returned response in bytes.',\n",
       "     'unit': ''}],\n",
       "   'coredns_dns_responses_total': [{'type': 'counter',\n",
       "     'help': 'Counter of response status codes.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_conn_cache_hits_total': [{'type': 'counter',\n",
       "     'help': 'Counter of connection cache hits per upstream and protocol.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_conn_cache_misses_total': [{'type': 'counter',\n",
       "     'help': 'Counter of connection cache misses per upstream and protocol.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_healthcheck_broken_total': [{'type': 'counter',\n",
       "     'help': 'Counter of the number of complete failures of the healthchecks.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_healthcheck_failures_total': [{'type': 'counter',\n",
       "     'help': 'Counter of the number of failed healthchecks.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_max_concurrent_rejects_total': [{'type': 'counter',\n",
       "     'help': 'Counter of the number of queries rejected because the concurrent queries were at maximum.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of the time each request took.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_requests_total': [{'type': 'counter',\n",
       "     'help': 'Counter of requests made per upstream.',\n",
       "     'unit': ''}],\n",
       "   'coredns_forward_responses_total': [{'type': 'counter',\n",
       "     'help': 'Counter of responses received per upstream.',\n",
       "     'unit': ''}],\n",
       "   'coredns_health_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of the time (in seconds) each request took.',\n",
       "     'unit': ''}],\n",
       "   'coredns_health_request_failures_total': [{'type': 'counter',\n",
       "     'help': 'The number of times the health check failed.',\n",
       "     'unit': ''}],\n",
       "   'coredns_hosts_reload_timestamp_seconds': [{'type': 'gauge',\n",
       "     'help': 'The timestamp of the last reload of hosts file.',\n",
       "     'unit': ''}],\n",
       "   'coredns_kubernetes_dns_programming_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'Histogram of the time (in seconds) it took to program a dns instance.',\n",
       "     'unit': ''}],\n",
       "   'coredns_local_localhost_requests_total': [{'type': 'counter',\n",
       "     'help': 'Counter of localhost.<domain> requests.',\n",
       "     'unit': ''}],\n",
       "   'coredns_panics_total': [{'type': 'counter',\n",
       "     'help': 'A metrics that counts the number of panics.',\n",
       "     'unit': ''}],\n",
       "   'coredns_plugin_enabled': [{'type': 'gauge',\n",
       "     'help': 'A metric that indicates whether a plugin is enabled on per server and zone basis.',\n",
       "     'unit': ''}],\n",
       "   'coredns_reload_failed_total': [{'type': 'counter',\n",
       "     'help': 'Counter of the number of failed reload attempts.',\n",
       "     'unit': ''}],\n",
       "   'cronjob_controller_cronjob_job_creation_skew_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Time between when a cronjob is scheduled to be run, and when the corresponding job is created',\n",
       "     'unit': ''}],\n",
       "   'cronjob_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for cronjob_controller',\n",
       "     'unit': ''}],\n",
       "   'csi_operations_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Container Storage Interface operation duration with gRPC error code status total',\n",
       "     'unit': ''}],\n",
       "   'csv_abnormal': [{'type': 'gauge',\n",
       "     'help': 'CSV is not installed',\n",
       "     'unit': ''}],\n",
       "   'csv_count': [{'type': 'gauge',\n",
       "     'help': 'Number of CSVs successfully registered',\n",
       "     'unit': ''}],\n",
       "   'csv_succeeded': [{'type': 'gauge',\n",
       "     'help': 'Successful CSV install',\n",
       "     'unit': ''}],\n",
       "   'csv_upgrade_count': [{'type': 'counter',\n",
       "     'help': 'Monotonic count of CSV upgrades',\n",
       "     'unit': ''}],\n",
       "   'daemon_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for daemon_controller',\n",
       "     'unit': ''}],\n",
       "   'default_storage_class_count': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of default storage classes currently configured.',\n",
       "     'unit': ''}],\n",
       "   'deployment_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for deployment_controller',\n",
       "     'unit': ''}],\n",
       "   'endpoint_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for endpoint_controller',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_changes': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of EndpointSlice changes',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_desired_endpoint_slices': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of EndpointSlices that would exist with perfect endpoint allocation',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_endpoints_added_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of endpoints added on each Service sync',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_endpoints_desired': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of endpoints desired',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_endpoints_removed_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of endpoints removed on each Service sync',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_endpointslices_changed_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of EndpointSlices changed on each Service sync',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_num_endpoint_slices': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of EndpointSlices',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for endpoint_slice_controller',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_controller_syncs': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of EndpointSlice syncs',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_addresses_skipped_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of addresses skipped on each Endpoints sync due to being invalid or exceeding MaxEndpointsPerSubset',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_changes': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of EndpointSlice changes',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_desired_endpoint_slices': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of EndpointSlices that would exist with perfect endpoint allocation',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_endpoints_added_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of endpoints added on each Endpoints sync',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_endpoints_desired': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of endpoints desired',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_endpoints_removed_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of endpoints removed on each Endpoints sync',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_endpoints_sync_duration': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Duration of syncEndpoints() in seconds',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_endpoints_updated_per_sync': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of endpoints updated on each Endpoints sync',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_num_endpoint_slices': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of EndpointSlices',\n",
       "     'unit': ''}],\n",
       "   'endpoint_slice_mirroring_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for endpoint_slice_mirroring_controller',\n",
       "     'unit': ''}],\n",
       "   'ephemeral_volume_controller_create_failures_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of PersistenVolumeClaims creation requests',\n",
       "     'unit': ''}],\n",
       "   'ephemeral_volume_controller_create_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of PersistenVolumeClaims creation requests',\n",
       "     'unit': ''}],\n",
       "   'etcd_bookmark_counts': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of etcd bookmarks (progress notify events) split by kind.',\n",
       "     'unit': ''}],\n",
       "   'etcd_cluster_version': [{'type': 'gauge',\n",
       "     'help': \"Which version is running. 1 for 'cluster_version' label with current cluster version\",\n",
       "     'unit': ''}],\n",
       "   'etcd_db_total_size_in_bytes': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Total size of the etcd database file physically allocated in bytes.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_auth_revision': [{'type': 'gauge',\n",
       "     'help': 'The current revision of auth store.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_disk_backend_commit_rebalance_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of commit.rebalance called by bboltdb backend.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_disk_backend_commit_spill_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of commit.spill called by bboltdb backend.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_disk_backend_commit_write_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of commit.write called by bboltdb backend.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_lease_granted_total': [{'type': 'counter',\n",
       "     'help': 'The total number of granted leases.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_lease_renewed_total': [{'type': 'counter',\n",
       "     'help': 'The number of renewed leases seen by the leader.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_lease_revoked_total': [{'type': 'counter',\n",
       "     'help': 'The total number of revoked leases.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_lease_ttl_total': [{'type': 'histogram',\n",
       "     'help': 'Bucketed histogram of lease TTLs.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_compact_revision': [{'type': 'gauge',\n",
       "     'help': 'The revision of the last compaction in store.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_current_revision': [{'type': 'gauge',\n",
       "     'help': 'The current revision of store.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_db_compaction_keys_total': [{'type': 'counter',\n",
       "     'help': 'Total number of db keys compacted.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_db_compaction_last': [{'type': 'gauge',\n",
       "     'help': 'The unix time of the last db compaction. Resets to 0 on start.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_db_compaction_pause_duration_milliseconds': [{'type': 'histogram',\n",
       "     'help': 'Bucketed histogram of db compaction pause duration.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_db_compaction_total_duration_milliseconds': [{'type': 'histogram',\n",
       "     'help': 'Bucketed histogram of db compaction total duration.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_events_total': [{'type': 'counter',\n",
       "     'help': 'Total number of events sent by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_index_compaction_pause_duration_milliseconds': [{'type': 'histogram',\n",
       "     'help': 'Bucketed histogram of index compaction pause duration.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_keys_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of keys.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_pending_events_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of pending events to be sent.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_range_total': [{'type': 'counter',\n",
       "     'help': 'Total number of ranges seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_slow_watcher_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of unsynced slow watchers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_total_put_size_in_bytes': [{'type': 'gauge',\n",
       "     'help': 'The total size of put kv pairs seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_watch_stream_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of watch streams.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_mvcc_watcher_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of watchers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_raft_terms_total': [{'type': 'counter',\n",
       "     'help': 'Number of etcd raft terms as observed by each member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_server_lease_expired_total': [{'type': 'counter',\n",
       "     'help': 'The total number of expired leases.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_snap_save_marshalling_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The marshalling cost distributions of save called by snapshot.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_snap_save_total_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The total latency distributions of save called by snapshot.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_store_expires_total': [{'type': 'counter',\n",
       "     'help': 'Total number of expired keys.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_store_reads_total': [{'type': 'counter',\n",
       "     'help': 'Total number of reads action by (get/getRecursive), local to this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_store_watch_requests_total': [{'type': 'counter',\n",
       "     'help': 'Total number of incoming watch requests (new or reestablished).',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_store_watchers': [{'type': 'gauge',\n",
       "     'help': 'Count of currently active watchers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_debugging_store_writes_total': [{'type': 'counter',\n",
       "     'help': 'Total number of writes (e.g. set/compareAndDelete) seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_disk_backend_commit_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of commit called by backend.',\n",
       "     'unit': ''}],\n",
       "   'etcd_disk_backend_defrag_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distribution of backend defragmentation.',\n",
       "     'unit': ''}],\n",
       "   'etcd_disk_backend_snapshot_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distribution of backend snapshots.',\n",
       "     'unit': ''}],\n",
       "   'etcd_disk_defrag_inflight': [{'type': 'gauge',\n",
       "     'help': 'Whether or not defrag is active on the member. 1 means active, 0 means not.',\n",
       "     'unit': ''}],\n",
       "   'etcd_disk_wal_fsync_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of fsync called by WAL.',\n",
       "     'unit': ''}],\n",
       "   'etcd_disk_wal_write_bytes_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of bytes written in WAL.',\n",
       "     'unit': ''}],\n",
       "   'etcd_grpc_proxy_cache_hits_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of cache hits',\n",
       "     'unit': ''}],\n",
       "   'etcd_grpc_proxy_cache_keys_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of keys/ranges cached',\n",
       "     'unit': ''}],\n",
       "   'etcd_grpc_proxy_cache_misses_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of cache misses',\n",
       "     'unit': ''}],\n",
       "   'etcd_grpc_proxy_events_coalescing_total': [{'type': 'counter',\n",
       "     'help': 'Total number of events coalescing',\n",
       "     'unit': ''}],\n",
       "   'etcd_grpc_proxy_watchers_coalescing_total': [{'type': 'gauge',\n",
       "     'help': 'Total number of current watchers coalescing',\n",
       "     'unit': ''}],\n",
       "   'etcd_lease_object_counts': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Number of objects attached to a single etcd lease.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_db_open_read_transactions': [{'type': 'gauge',\n",
       "     'help': 'The number of currently open read transactions',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_db_total_size_in_bytes': [{'type': 'gauge',\n",
       "     'help': 'Total size of the underlying database physically allocated in bytes.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_db_total_size_in_use_in_bytes': [{'type': 'gauge',\n",
       "     'help': 'Total size of the underlying database logically in use in bytes.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_delete_total': [{'type': 'counter',\n",
       "     'help': 'Total number of deletes seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_hash_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distribution of storage hash operation.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_hash_rev_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distribution of storage hash by revision operation.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_put_total': [{'type': 'counter',\n",
       "     'help': 'Total number of puts seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_range_total': [{'type': 'counter',\n",
       "     'help': 'Total number of ranges seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_mvcc_txn_total': [{'type': 'counter',\n",
       "     'help': 'Total number of txns seen by this member.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_active_peers': [{'type': 'gauge',\n",
       "     'help': 'The current number of active peer connections.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_client_grpc_received_bytes_total': [{'type': 'counter',\n",
       "     'help': 'The total number of bytes received from grpc clients.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_client_grpc_sent_bytes_total': [{'type': 'counter',\n",
       "     'help': 'The total number of bytes sent to grpc clients.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_disconnected_peers_total': [{'type': 'counter',\n",
       "     'help': 'The total number of disconnected peers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_peer_received_bytes_total': [{'type': 'counter',\n",
       "     'help': 'The total number of bytes received from peers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_peer_round_trip_time_seconds': [{'type': 'histogram',\n",
       "     'help': 'Round-Trip-Time histogram between peers',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_peer_sent_bytes_total': [{'type': 'counter',\n",
       "     'help': 'The total number of bytes sent to peers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_network_peer_sent_failures_total': [{'type': 'counter',\n",
       "     'help': 'The total number of send failures from peers.',\n",
       "     'unit': ''}],\n",
       "   'etcd_object_counts': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] Number of stored objects at the time of last check split by kind. This metric is replaced by apiserver_storage_object_counts.',\n",
       "     'unit': ''}],\n",
       "   'etcd_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Etcd request latency in seconds for each operation and object type.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_apply_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of v2 apply called by backend.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_client_requests_total': [{'type': 'counter',\n",
       "     'help': 'The total number of client requests per client version.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_go_version': [{'type': 'gauge',\n",
       "     'help': \"Which Go version server is running with. 1 for 'server_go_version' label with current version.\",\n",
       "     'unit': ''}],\n",
       "   'etcd_server_has_leader': [{'type': 'gauge',\n",
       "     'help': 'Whether or not a leader exists. 1 is existence, 0 is not.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_health_failures': [{'type': 'counter',\n",
       "     'help': 'The total number of failed health checks',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_health_success': [{'type': 'counter',\n",
       "     'help': 'The total number of successful health checks',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_heartbeat_send_failures_total': [{'type': 'counter',\n",
       "     'help': 'The total number of leader heartbeat send failures (likely overloaded from slow disk).',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_id': [{'type': 'gauge',\n",
       "     'help': \"Server or member ID in hexadecimal format. 1 for 'server_id' label with current ID.\",\n",
       "     'unit': ''}],\n",
       "   'etcd_server_is_leader': [{'type': 'gauge',\n",
       "     'help': 'Whether or not this member is a leader. 1 if is, 0 otherwise.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_is_learner': [{'type': 'gauge',\n",
       "     'help': 'Whether or not this member is a learner. 1 if is, 0 otherwise.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_leader_changes_seen_total': [{'type': 'counter',\n",
       "     'help': 'The number of leader changes seen.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_learner_promote_successes': [{'type': 'counter',\n",
       "     'help': 'The total number of successful learner promotions while this member is leader.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_proposals_applied_total': [{'type': 'gauge',\n",
       "     'help': 'The total number of consensus proposals applied.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_proposals_committed_total': [{'type': 'gauge',\n",
       "     'help': 'The total number of consensus proposals committed.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_proposals_failed_total': [{'type': 'counter',\n",
       "     'help': 'The total number of failed proposals seen.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_proposals_pending': [{'type': 'gauge',\n",
       "     'help': 'The current number of pending proposals to commit.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_quota_backend_bytes': [{'type': 'gauge',\n",
       "     'help': 'Current backend storage quota size in bytes.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_read_indexes_failed_total': [{'type': 'counter',\n",
       "     'help': 'The total number of failed read indexes seen.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_slow_apply_total': [{'type': 'counter',\n",
       "     'help': 'The total number of slow apply requests (likely overloaded from slow disk).',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_slow_read_indexes_total': [{'type': 'counter',\n",
       "     'help': \"The total number of pending read indexes not in sync with leader's or timed out read index requests.\",\n",
       "     'unit': ''}],\n",
       "   'etcd_server_snapshot_apply_in_progress_total': [{'type': 'gauge',\n",
       "     'help': '1 if the server is applying the incoming snapshot. 0 if none.',\n",
       "     'unit': ''}],\n",
       "   'etcd_server_version': [{'type': 'gauge',\n",
       "     'help': \"Which version is running. 1 for 'server_version' label with current version.\",\n",
       "     'unit': ''}],\n",
       "   'etcd_snap_db_fsync_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of fsyncing .snap.db file',\n",
       "     'unit': ''}],\n",
       "   'etcd_snap_db_save_total_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The total latency distributions of v3 snapshot save',\n",
       "     'unit': ''}],\n",
       "   'etcd_snap_fsync_duration_seconds': [{'type': 'histogram',\n",
       "     'help': 'The latency distributions of fsync called by snap.',\n",
       "     'unit': ''}],\n",
       "   'event_recorder_total_events_count': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Total count of events processed by this event recorder per involved object',\n",
       "     'unit': ''}],\n",
       "   'federate_errors': [{'type': 'gauge',\n",
       "     'help': 'The number of times forwarding federated metrics has failed',\n",
       "     'unit': ''}],\n",
       "   'federate_filtered_samples': [{'type': 'gauge',\n",
       "     'help': 'Tracks the number of samples filtered per federation',\n",
       "     'unit': ''}],\n",
       "   'federate_samples': [{'type': 'gauge',\n",
       "     'help': 'Tracks the number of samples per federation',\n",
       "     'unit': ''}],\n",
       "   'field_validation_request_duration_seconds': [{'type': 'histogram',\n",
       "     'help': '[ALPHA] Response latency distribution in seconds for each field validation value and whether field validation is enabled or not',\n",
       "     'unit': ''}],\n",
       "   'garbagecollector_controller_resources_sync_error_total': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Number of garbage collector resources sync errors',\n",
       "     'unit': ''}],\n",
       "   'gc_controller_rate_limiter_use': [{'type': 'gauge',\n",
       "     'help': '[ALPHA] A metric measuring the saturation of the rate limiter for gc_controller',\n",
       "     'unit': ''}],\n",
       "   'get_token_count': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of total Token() requests to the alternate token source',\n",
       "     'unit': ''}],\n",
       "   'get_token_fail_count': [{'type': 'counter',\n",
       "     'help': '[ALPHA] Counter of failed Token() requests to the alternate token source',\n",
       "     'unit': ''}],\n",
       "   'go_gc_cycles_automatic_gc_cycles': [{'type': 'counter',\n",
       "     'help': 'Count of completed GC cycles generated by the Go runtime.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_cycles_automatic_gc_cycles_total': [{'type': 'counter',\n",
       "     'help': 'Count of completed GC cycles generated by the Go runtime.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_cycles_forced_gc_cycles': [{'type': 'counter',\n",
       "     'help': 'Count of completed GC cycles forced by the application.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_cycles_forced_gc_cycles_total': [{'type': 'counter',\n",
       "     'help': 'Count of completed GC cycles forced by the application.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_cycles_total_gc_cycles': [{'type': 'counter',\n",
       "     'help': 'Count of all completed GC cycles.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_cycles_total_gc_cycles_total': [{'type': 'counter',\n",
       "     'help': 'Count of all completed GC cycles.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_duration_seconds': [{'type': 'summary',\n",
       "     'help': 'A summary of the pause duration of garbage collection cycles.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_allocs_by_size_bytes_total': [{'type': 'histogram',\n",
       "     'help': 'Distribution of heap allocations by approximate size. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_allocs_bytes': [{'type': 'counter',\n",
       "     'help': 'Cumulative sum of memory allocated to the heap by the application.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_allocs_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative sum of memory allocated to the heap by the application.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_allocs_objects': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of heap allocations triggered by the application. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_allocs_objects_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of heap allocations triggered by the application. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_frees_by_size_bytes_total': [{'type': 'histogram',\n",
       "     'help': 'Distribution of freed heap allocations by approximate size. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_frees_bytes': [{'type': 'counter',\n",
       "     'help': 'Cumulative sum of heap memory freed by the garbage collector.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_frees_bytes_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative sum of heap memory freed by the garbage collector.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_frees_objects': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of heap allocations whose storage was freed by the garbage collector. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_frees_objects_total': [{'type': 'counter',\n",
       "     'help': 'Cumulative count of heap allocations whose storage was freed by the garbage collector. Note that this does not include tiny objects as defined by /gc/heap/tiny/allocs:objects, only tiny blocks.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_goal_bytes': [{'type': 'gauge',\n",
       "     'help': 'Heap size target for the end of the GC cycle.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_objects_objects': [{'type': 'gauge',\n",
       "     'help': 'Number of objects, live or unswept, occupying heap memory.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_tiny_allocs_objects': [{'type': 'counter',\n",
       "     'help': 'Count of small allocations that are packed together into blocks. These allocations are counted separately from other allocations because each individual allocation is not tracked by the runtime, only their block. Each block is already accounted for in allocs-by-size and frees-by-size.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_heap_tiny_allocs_objects_total': [{'type': 'counter',\n",
       "     'help': 'Count of small allocations that are packed together into blocks. These allocations are counted separately from other allocations because each individual allocation is not tracked by the runtime, only their block. Each block is already accounted for in allocs-by-size and frees-by-size.',\n",
       "     'unit': ''}],\n",
       "   'go_gc_pauses_seconds_total': [{'type': 'histogram',\n",
       "     'help': 'Distribution individual GC-related stop-the-world pause latencies.',\n",
       "     'unit': ''}],\n",
       "   'go_goroutines': [{'type': 'gauge',\n",
       "     'help': 'Number of goroutines that currently exist.',\n",
       "     'unit': ''}],\n",
       "   'go_info': [{'type': 'gauge',\n",
       "     'help': 'Information about the Go environment.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_heap_free_bytes': [{'type': 'gauge',\n",
       "     'help': \"Memory that is completely free and eligible to be returned to the underlying system, but has not been. This metric is the runtime's estimate of free address space that is backed by physical memory.\",\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_heap_objects_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory occupied by live objects and dead objects that have not yet been marked free by the garbage collector.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_heap_released_bytes': [{'type': 'gauge',\n",
       "     'help': \"Memory that is completely free and has been returned to the underlying system. This metric is the runtime's estimate of free address space that is still mapped into the process, but is not backed by physical memory.\",\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_heap_stacks_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory allocated from the heap that is reserved for stack space, whether or not it is currently in-use.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_heap_unused_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory that is reserved for heap objects but is not currently used to hold heap objects.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_metadata_mcache_free_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory that is reserved for runtime mcache structures, but not in-use.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_metadata_mcache_inuse_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory that is occupied by runtime mcache structures that are currently being used.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_metadata_mspan_free_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory that is reserved for runtime mspan structures, but not in-use.',\n",
       "     'unit': ''}],\n",
       "   'go_memory_classes_metadata_mspan_inuse_bytes': [{'type': 'gauge',\n",
       "     'help': 'Memory that is occupied by runtime mspan structures that are currently being used.',\n",
       "     'unit': ''}],\n",
       "   ...}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://prometheus.io/docs/prometheus/latest/querying/api/\n",
    "r = requests.get(\n",
    "    f\"https://{prometheus_host}/v1/metadata\",\n",
    "    verify=False,\n",
    "    headers=kubeConfig.api_key,\n",
    ")\n",
    "r.status_code, r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " {'status': 'success',\n",
       "  'data': {'groups': [{'name': 'CloudCredentialOperator',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cloud-credential-operator-cloud-credential-operator-alerts-08a3b917-b260-4e67-acd0-3c15656d961d.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'CloudCredentialOperatorTargetNamespaceMissing',\n",
       "       'query': 'cco_credentials_requests_conditions{condition=\"MissingTargetNamespace\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"At least one CredentialsRequest custom resource has specified in its .spec.secretRef.namespace field a namespace which does not presently exist. This means the Cloud Credential Operator in the openshift-cloud-credential-operator namespace cannot process the CredentialsRequest resource. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .status.condition showing a condition type of MissingTargetNamespace set to True.\",\n",
       "        'message': 'CredentialsRequest(s) pointing to non-existent namespace',\n",
       "        'summary': 'One ore more CredentialsRequest CRs are asking to save credentials to a non-existent namespace.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00026505,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.267225665Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CloudCredentialOperatorProvisioningFailed',\n",
       "       'query': 'cco_credentials_requests_conditions{condition=\"CredentialsProvisionFailure\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"While processing a CredentialsRequest, the Cloud Credential Operator encountered an issue. Check the conditions of all CredentialsRequets with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .stats.condition showing a condition type of CredentialsProvisionFailure set to True for more details on the issue.\",\n",
       "        'message': 'CredentialsRequest(s) unable to be fulfilled',\n",
       "        'summary': 'One or more CredentialsRequest CRs are unable to be processed.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.7204e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.267492298Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CloudCredentialOperatorDeprovisioningFailed',\n",
       "       'query': 'cco_credentials_requests_conditions{condition=\"CredentialsDeprovisionFailure\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"While processing a CredentialsRequest marked for deletion, the Cloud Credential Operator encountered an issue. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .status.condition showing a condition type of CredentialsDeprovisionFailure set to True for more details on the issue.\",\n",
       "        'message': 'CredentialsRequest(s) unable to be cleaned up',\n",
       "        'summary': 'One or more CredentialsRequest CRs are unable to be deleted.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.2086e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.267570394Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CloudCredentialOperatorInsufficientCloudCreds',\n",
       "       'query': 'cco_credentials_requests_conditions{condition=\"InsufficientCloudCreds\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"The Cloud Credential Operator has determined that there are insufficient permissions to process one or more CredentialsRequest CRs. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .status.condition showing a condition type of InsufficientCloudCreds set to True for more details.\",\n",
       "        'message': \"Cluster's cloud credentials insufficient for minting or passthrough\",\n",
       "        'summary': 'Problem with the available platform credentials.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.8417e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.267643412Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CloudCredentialOperatorStaleCredentials',\n",
       "       'query': 'cco_credentials_requests_conditions{condition=\"StaleCredentials\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"The Cloud Credential Operator (CCO) has detected one or more stale CredentialsRequest CRs that need to be manually deleted. When the CCO is in Manual credentials mode, it will not automatially clean up stale CredentialsRequest CRs (that may no longer be necessary in the present version of OpenShift because it could involve needing to clean up manually created cloud resources. Check the conditions of all CredentialsRequests with 'oc get credentialsrequest -A' to find any CredentialsRequest(s) with a .status.condition showing a condition type of StaleCredentials set to True. Determine the appropriate steps to clean up/deprovision any previously provisioned cloud resources. Finally, delete the CredentialsRequest with an 'oc delete'.\",\n",
       "        'message': '1 or more credentials requests are stale and should be deleted. Check the status.conditions on CredentialsRequest CRs to identify the stale one(s).',\n",
       "        'summary': 'One or more CredentialsRequest CRs are stale and should be deleted.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000271591,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.267722671Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000801018,\n",
       "     'lastEvaluation': '2022-12-14T21:00:21.267203744Z'},\n",
       "    {'name': 'cluster-machine-approver.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-machine-approver-machineapprover-rules-fee85f22-83c5-4a4f-bea7-b67391740a46.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineApproverMaxPendingCSRsReached',\n",
       "       'query': 'mapi_current_pending_csr > mapi_max_pending_csr',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The number of pending CertificateSigningRequests has exceeded the\\nmaximum threshold (current number of machine + 100). Check the\\npending CSRs to determine which machines need approval, also check\\nthat the nodelink controller is running in the openshift-machine-api\\nnamespace.\\n',\n",
       "        'summary': 'max pending CSRs threshold reached.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000303732,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.88023962Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000328639,\n",
       "     'lastEvaluation': '2022-12-14T21:00:22.880218951Z'},\n",
       "    {'name': 'node-tuning-operator.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-node-tuning-operator-node-tuning-operator-246e1f8e-e5f0-4416-aa9e-2989354478b9.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'NTOPodsNotReady',\n",
       "       'query': 'kube_pod_status_ready{condition=\"true\",namespace=\"openshift-cluster-node-tuning-operator\"} == 0',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Pod {{ $labels.pod }} is not ready.\\nReview the \"Event\" objects in \"openshift-cluster-node-tuning-operator\" namespace for further details.\\n',\n",
       "        'summary': 'Pod {{ $labels.pod }} is not ready.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000278354,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.435808104Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NTODegraded',\n",
       "       'query': 'nto_degraded_info == 1',\n",
       "       'duration': 7200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The Node Tuning Operator is degraded. Review the \"node-tuning\" ClusterOperator object for further details.',\n",
       "        'summary': 'The Node Tuning Operator is degraded.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 5.1116e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.43608778Z',\n",
       "       'type': 'alerting'},\n",
       "      {'name': 'nto_custom_profiles:count',\n",
       "       'query': 'count(nto_profile_calculated_total{profile!~\"openshift\",profile!~\"openshift-control-plane\",profile!~\"openshift-node\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000102914,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.436139818Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000460296,\n",
       "     'lastEvaluation': '2022-12-14T21:00:12.43578481Z'},\n",
       "    {'name': 'SamplesOperator',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-samples-operator-samples-operator-alerts-0c49042c-7cce-489c-a6f0-6cbd308c10bf.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'SamplesRetriesMissingOnImagestreamImportFailing',\n",
       "       'query': 'sum(openshift_samples_failed_imagestream_import_info) > sum(openshift_samples_retry_imagestream_import_total) - sum(openshift_samples_retry_imagestream_import_total offset 30m)',\n",
       "       'duration': 7200,\n",
       "       'labels': {'namespace': 'openshift-cluster-samples-operator',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'Samples operator is detecting problems with imagestream image imports, and the periodic retries of those\\nimports are not occurring.  Contact support.  You can look at the \"openshift-samples\" ClusterOperator object\\nfor details. Most likely there are issues with the external image registry hosting the images that need to\\nbe investigated.  The list of ImageStreams that have failing imports are:\\n{{ range query \"openshift_samples_failed_imagestream_import_info > 0\" }}\\n  {{ .Labels.name }}\\n{{ end }}\\nHowever, the list of ImageStreams for which samples operator is retrying imports is:\\nretrying imports:\\n{{ range query \"openshift_samples_retry_imagestream_import_total > 0\" }}\\n   {{ .Labels.imagestreamname }}\\n{{ end }}\\n',\n",
       "        'summary': 'Samples operator is having problems with imagestream imports and its retries.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00049406,\n",
       "       'lastEvaluation': '2022-12-14T21:00:09.343634825Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SamplesImagestreamImportFailing',\n",
       "       'query': 'sum(openshift_samples_retry_imagestream_import_total) - sum(openshift_samples_retry_imagestream_import_total offset 30m) > sum(openshift_samples_failed_imagestream_import_info)',\n",
       "       'duration': 7200,\n",
       "       'labels': {'namespace': 'openshift-cluster-samples-operator',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'Samples operator is detecting problems with imagestream image imports.  You can look at the \"openshift-samples\"\\nClusterOperator object for details. Most likely there are issues with the external image registry hosting\\nthe images that needs to be investigated.  Or you can consider marking samples operator Removed if you do not\\ncare about having sample imagestreams available.  The list of ImageStreams for which samples operator is\\nretrying imports:\\n{{ range query \"openshift_samples_retry_imagestream_import_total > 0\" }}\\n   {{ .Labels.imagestreamname }}\\n{{ end }}\\n',\n",
       "        'summary': 'Samples operator is detecting problems with imagestream image imports'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000260481,\n",
       "       'lastEvaluation': '2022-12-14T21:00:09.344130097Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SamplesDegraded',\n",
       "       'query': 'openshift_samples_degraded_info == 1',\n",
       "       'duration': 7200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Samples could not be deployed and the operator is degraded. Review the \"openshift-samples\" ClusterOperator object for further details.\\n',\n",
       "        'summary': 'Samples operator is degraded.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 5.0595e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:09.34439141Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SamplesInvalidConfig',\n",
       "       'query': 'openshift_samples_invalidconfig_info == 1',\n",
       "       'duration': 7200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Samples operator has been given an invalid configuration.\\n',\n",
       "        'summary': 'Samples operator Invalid configuration'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 3.9113e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:09.344443317Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SamplesMissingSecret',\n",
       "       'query': 'openshift_samples_invalidsecret_info{reason=\"missing_secret\"} == 1',\n",
       "       'duration': 7200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Samples operator cannot find the samples pull secret in the openshift namespace.\\n',\n",
       "        'summary': 'Samples operator is not able to find secret'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 4.9303e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:09.344483122Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SamplesMissingTBRCredential',\n",
       "       'query': 'openshift_samples_invalidsecret_info{reason=\"missing_tbr_credential\"} == 1',\n",
       "       'duration': 7200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"The samples operator cannot find credentials for 'registry.redhat.io'. Many of the sample ImageStreams will fail to import unless the 'samplesRegistry' in the operator configuration is changed.\\n\",\n",
       "        'summary': 'Samples operator is not able to find the credentials for registry'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 4.5446e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:09.344533106Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SamplesTBRInaccessibleOnBoot',\n",
       "       'query': 'openshift_samples_tbr_inaccessible_info == 1',\n",
       "       'duration': 172800,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': \"One of two situations has occurred.  Either\\nsamples operator could not access 'registry.redhat.io' during its initial installation and it bootstrapped as removed.\\nIf this is expected, and stems from installing in a restricted network environment, please note that if you\\nplan on mirroring images associated with sample imagestreams into a registry available in your restricted\\nnetwork environment, and subsequently moving samples operator back to 'Managed' state, a list of the images\\nassociated with each image stream tag from the samples catalog is\\nprovided in the 'imagestreamtag-to-image' config map in the 'openshift-cluster-samples-operator' namespace to\\nassist the mirroring process.\\nOr, the use of allowed registries or blocked registries with global imagestream configuration will not allow\\nsamples operator to create imagestreams using the default image registry 'registry.redhat.io'.\\n\",\n",
       "        'summary': 'Samples operator is not able to access the registry on boot'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 4.3652e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:09.344579213Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.00100988,\n",
       "     'lastEvaluation': '2022-12-14T21:00:09.343615379Z'},\n",
       "    {'name': 'default-storage-classes.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-storage-operator-prometheus-a32fbdf2-0e35-4a69-b5cc-ece00aab3d5c.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MultipleDefaultStorageClasses',\n",
       "       'query': 'max_over_time(default_storage_class_count[5m]) > 1',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Cluster storage operator monitors all storage classes configured in the cluster\\nand checks there is not more than one default StorageClass configured.\\n',\n",
       "        'message': 'StorageClass count check is failing (there should not be more than one default StorageClass)',\n",
       "        'summary': 'More than one default StorageClass detected.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00035583,\n",
       "       'lastEvaluation': '2022-12-14T21:00:10.446122481Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000391267,\n",
       "     'lastEvaluation': '2022-12-14T21:00:10.44609011Z'},\n",
       "    {'name': 'cluster-operators',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-version-cluster-version-operator-20b54cd0-0354-4ed2-a3a4-7e6bb35123df.yaml',\n",
       "     'rules': [{'state': 'firing',\n",
       "       'name': 'ClusterNotUpgradeable',\n",
       "       'query': 'max by(name, condition, endpoint) (cluster_operator_conditions{condition=\"Upgradeable\",endpoint=\"metrics\",name=\"version\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'In most cases, you will still be able to apply patch releases. Reason {{ with $cluster_operator_conditions := \"cluster_operator_conditions\" | query}}{{range $value := .}}{{if and (eq (label \"name\" $value) \"version\") (eq (label \"condition\" $value) \"Upgradeable\") (eq (label \"endpoint\" $value) \"metrics\") (eq (value $value) 0.0) (ne (len (label \"reason\" $value)) 0) }}{{label \"reason\" $value}}.{{end}}{{end}}{{end}} For more information refer to \\'oc adm upgrade\\'{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}.',\n",
       "        'summary': 'One or more cluster operators have been blocking minor version cluster upgrades for at least an hour.'},\n",
       "       'alerts': [{'labels': {'alertname': 'ClusterNotUpgradeable',\n",
       "          'condition': 'Upgradeable',\n",
       "          'endpoint': 'metrics',\n",
       "          'name': 'version',\n",
       "          'severity': 'info'},\n",
       "         'annotations': {'description': \"In most cases, you will still be able to apply patch releases. Reason AdminAckRequired. For more information refer to 'oc adm upgrade' or https://console-openshift-console.apps.sharedocp4upi411ovn.lab.upshift.rdu2.redhat.com/settings/cluster/.\",\n",
       "          'summary': 'One or more cluster operators have been blocking minor version cluster upgrades for at least an hour.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2022-12-11T04:20:34.315116068Z',\n",
       "         'value': '0e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.022350109,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.333127405Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ClusterOperatorDown',\n",
       "       'query': 'cluster_operator_up{job=\"cluster-version-operator\"} == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'The {{ $labels.name }} operator may be down or disabled, and the components it manages may be unavailable or degraded.  Cluster upgrades may not complete. For more information refer to \\'oc get -o yaml clusteroperator {{ $labels.name }}\\'{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}.',\n",
       "        'summary': 'Cluster operator has not been available for 10 minutes.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000493479,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.355483846Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ClusterOperatorDegraded',\n",
       "       'query': '(cluster_operator_conditions{condition=\"Degraded\",job=\"cluster-version-operator\"} or on(name) group by(name) (cluster_operator_up{job=\"cluster-version-operator\"})) == 1',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The {{ $labels.name }} operator is degraded because {{ $labels.reason }}, and the components it manages may have reduced quality of service.  Cluster upgrades may not complete. For more information refer to \\'oc get -o yaml clusteroperator {{ $labels.name }}\\'{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}.',\n",
       "        'summary': 'Cluster operator has been degraded for 30 minutes.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000820745,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.355978497Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ClusterOperatorFlapping',\n",
       "       'query': 'changes(cluster_operator_up{job=\"cluster-version-operator\"}[2m]) > 2',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The  {{ $labels.name }} operator behavior might cause upgrades to be unstable. For more information refer to \\'oc get -o yaml clusteroperator {{ $labels.name }}\\'{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}.',\n",
       "        'summary': 'Cluster operator up status is changing often.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000380576,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.356800404Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.024063052,\n",
       "     'lastEvaluation': '2022-12-14T21:00:04.333119791Z'},\n",
       "    {'name': 'cluster-version',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-cluster-version-cluster-version-operator-20b54cd0-0354-4ed2-a3a4-7e6bb35123df.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ClusterVersionOperatorDown',\n",
       "       'query': 'absent(up{job=\"cluster-version-operator\"} == 1)',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'The operator may be down or disabled. The cluster will not be kept up to date and upgrades will not be possible. Inspect the openshift-cluster-version namespace for events or changes to the cluster-version-operator deployment or pods to diagnose and repair. {{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} For more information refer to {{ label \"url\" (first $console_url ) }}/k8s/cluster/projects/openshift-cluster-version.{{ end }}{{ end }}',\n",
       "        'summary': 'Cluster version operator has disappeared from Prometheus target discovery.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000256843,\n",
       "       'lastEvaluation': '2022-12-14T21:00:11.856042459Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CannotRetrieveUpdates',\n",
       "       'query': '(time() - cluster_version_operator_update_retrieval_timestamp_seconds) >= 3600 and ignoring(condition, name, reason) cluster_operator_conditions{condition=\"RetrievedUpdates\",endpoint=\"metrics\",name=\"version\",reason!=\"NoChannel\"}',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Failure to retrieve updates means that cluster administrators will need to monitor for available updates on their own or risk falling behind on security or other bugfixes. If the failure is expected, you can clear spec.channel in the ClusterVersion object to tell the cluster-version operator to not retrieve updates. Failure reason {{ with $cluster_operator_conditions := \"cluster_operator_conditions\" | query}}{{range $value := .}}{{if and (eq (label \"name\" $value) \"version\") (eq (label \"condition\" $value) \"RetrievedUpdates\") (eq (label \"endpoint\" $value) \"metrics\") (eq (value $value) 0.0)}}{{label \"reason\" $value}} {{end}}{{end}}{{end}}. {{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} For more information refer to {{ label \"url\" (first $console_url ) }}/settings/cluster/.{{ end }}{{ end }}',\n",
       "        'summary': 'Cluster version operator has not retrieved updates in {{ $value | humanizeDuration }}.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000162836,\n",
       "       'lastEvaluation': '2022-12-14T21:00:11.856301085Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'firing',\n",
       "       'name': 'UpdateAvailable',\n",
       "       'query': 'sum by(channel, upstream) (cluster_version_available_updates) > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'For more information refer to \\'oc adm upgrade\\'{{ with $console_url := \"console_url\" | query }}{{ if ne (len (label \"url\" (first $console_url ) ) ) 0}} or {{ label \"url\" (first $console_url ) }}/settings/cluster/{{ end }}{{ end }}.',\n",
       "        'summary': 'Your upstream update recommendation service recommends you update your cluster.'},\n",
       "       'alerts': [{'labels': {'alertname': 'UpdateAvailable',\n",
       "          'channel': 'stable-4.11',\n",
       "          'severity': 'info',\n",
       "          'upstream': '<default>'},\n",
       "         'annotations': {'description': \"For more information refer to 'oc adm upgrade' or https://console-openshift-console.apps.sharedocp4upi411ovn.lab.upshift.rdu2.redhat.com/settings/cluster/.\",\n",
       "          'summary': 'Your upstream update recommendation service recommends you update your cluster.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2022-12-12T09:38:11.854684099Z',\n",
       "         'value': '1e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000742187,\n",
       "       'lastEvaluation': '2022-12-14T21:00:11.856464703Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001179229,\n",
       "     'lastEvaluation': '2022-12-14T21:00:11.856034634Z'},\n",
       "    {'name': 'openshift-dns.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-dns-operator-dns-2a359bb7-3f04-4bd9-a544-ff8af5f3e9bf.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'CoreDNSPanicking',\n",
       "       'query': 'increase(coredns_panics_total[10m]) > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $value }} CoreDNS panics observed on {{ $labels.instance }}',\n",
       "        'summary': 'CoreDNS panic'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000292771,\n",
       "       'lastEvaluation': '2022-12-14T21:00:19.62251308Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CoreDNSHealthCheckSlow',\n",
       "       'query': 'histogram_quantile(0.95, sum by(instance, le) (rate(coredns_health_request_duration_seconds_bucket[5m]))) > 10',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'CoreDNS Health Checks are slowing down (instance {{ $labels.instance }})',\n",
       "        'summary': 'CoreDNS health checks'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000505331,\n",
       "       'lastEvaluation': '2022-12-14T21:00:19.622807835Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CoreDNSErrorsHigh',\n",
       "       'query': '(sum by(namespace) (rate(coredns_dns_responses_total{rcode=\"SERVFAIL\"}[5m])) / sum by(namespace) (rate(coredns_dns_responses_total[5m]))) > 0.01',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage }} of requests.',\n",
       "        'summary': 'CoreDNS serverfail'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00025,\n",
       "       'lastEvaluation': '2022-12-14T21:00:19.623314057Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001074301,\n",
       "     'lastEvaluation': '2022-12-14T21:00:19.62249173Z'},\n",
       "    {'name': 'etcd',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-etcd-operator-etcd-prometheus-rules-aae60bf2-2901-49fc-a7bc-aaab52ec532e.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'etcdMembersDown',\n",
       "       'query': 'max without(endpoint) (sum without(instance) (up{job=~\".*etcd.*\"} == bool 0) or count without(To) (sum without(instance) (rate(etcd_network_peer_sent_failures_total{job=~\".*etcd.*\"}[2m])) > 0.01)) > 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": members are down ({{ $value }}).',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdMembersDown.md',\n",
       "        'summary': 'etcd cluster members are down.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000577075,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.690943577Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdNoLeader',\n",
       "       'query': 'etcd_server_has_leader{job=~\".*etcd.*\"} == 0',\n",
       "       'duration': 60,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": member {{ $labels.instance }} has no leader.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdNoLeader.md',\n",
       "        'summary': 'etcd cluster has no leader.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000109597,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.691522105Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdMemberCommunicationSlow',\n",
       "       'query': 'histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.15',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": member communication with {{ $labels.To }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}.',\n",
       "        'summary': 'etcd cluster member communication is slow.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001155374,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.691632453Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighNumberOfFailedProposals',\n",
       "       'query': 'rate(etcd_server_proposals_failed_total{job=~\".*etcd.*\"}[15m]) > 5',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": {{ $value }} proposal failures within the last 30 minutes on etcd instance {{ $labels.instance }}.',\n",
       "        'summary': 'etcd cluster has high number of proposal failures.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000123383,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.692788939Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighFsyncDurations',\n",
       "       'query': 'histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.5',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": 99th percentile fsync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.',\n",
       "        'summary': 'etcd cluster 99th percentile fsync durations are too high.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000545076,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.692913093Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighFsyncDurations',\n",
       "       'query': 'histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 1',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": 99th percentile fsync durations are {{ $value }}s on etcd instance {{ $labels.instance }}.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdHighFsyncDurations.md',\n",
       "        'summary': 'etcd cluster 99th percentile fsync durations are too high.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00053722,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.693459211Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighCommitDurations',\n",
       "       'query': 'histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~\".*etcd.*\"}[5m])) > 0.25',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": 99th percentile commit durations {{ $value }}s on etcd instance {{ $labels.instance }}.',\n",
       "        'summary': 'etcd cluster 99th percentile commit durations are too high.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000561027,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.693997493Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdDatabaseQuotaLowSpace',\n",
       "       'query': '(last_over_time(etcd_mvcc_db_total_size_in_bytes[5m]) / last_over_time(etcd_server_quota_backend_bytes[5m])) * 100 > 95',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": database size exceeds the defined quota on etcd instance {{ $labels.instance }}, please defrag or increase the quota as the writes to etcd will be disabled when it is full.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdDatabaseQuotaLowSpace.md',\n",
       "        'summary': 'etcd cluster database is running full.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000173536,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.694559652Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdExcessiveDatabaseGrowth',\n",
       "       'query': 'predict_linear(etcd_mvcc_db_total_size_in_bytes[4h], 4 * 60 * 60) > etcd_server_quota_backend_bytes',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": Predicting running out of disk space in the next four hours, based on write observations within the past four hours on etcd instance {{ $labels.instance }}, please check as it might be disruptive.',\n",
       "        'summary': 'etcd cluster database growing very fast.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000301788,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.69474997Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdDatabaseHighFragmentationRatio',\n",
       "       'query': '(last_over_time(etcd_mvcc_db_total_size_in_use_in_bytes[5m]) / last_over_time(etcd_mvcc_db_total_size_in_bytes[5m])) < 0.5',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": database size in use on instance {{ $labels.instance }} is {{ $value | humanizePercentage }} of the actual allocated disk space, please run defragmentation (e.g. etcdctl defrag) to retrieve the unused fragmented disk space.',\n",
       "        'runbook_url': 'https://etcd.io/docs/v3.5/op-guide/maintenance/#defragmentation',\n",
       "        'summary': 'etcd database size in use is less than 50% of the actual allocated storage.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000177224,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.69505288Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.004298268,\n",
       "     'lastEvaluation': '2022-12-14T20:59:54.690933859Z'},\n",
       "    {'name': 'openshift-etcd.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-etcd-operator-etcd-prometheus-rules-aae60bf2-2901-49fc-a7bc-aaab52ec532e.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'etcdGRPCRequestsSlow',\n",
       "       'query': 'histogram_quantile(0.99, sum without(grpc_type) (rate(grpc_server_handling_seconds_bucket{grpc_method!=\"Defragment\",grpc_type=\"unary\",job=\"etcd\"}[10m]))) > 1',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": 99th percentile of gRPC requests is {{ $value }}s on etcd instance {{ $labels.instance }} for {{ $labels.grpc_method }} method.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdGRPCRequestsSlow.md',\n",
       "        'summary': 'etcd grpc requests are slow'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.033692923,\n",
       "       'lastEvaluation': '2022-12-14T21:00:14.882957051Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighNumberOfFailedGRPCRequests',\n",
       "       'query': '(sum without(grpc_type, grpc_code) (rate(grpc_server_handled_total{grpc_code=~\"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\",job=\"etcd\"}[5m])) / (sum without(grpc_type, grpc_code) (rate(grpc_server_handled_total{job=\"etcd\"}[5m])) > 2 and on() (sum(cluster_infrastructure_provider{type!~\"ipi|BareMetal\"} == bool 1)))) * 100 > 10',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": {{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.',\n",
       "        'summary': 'etcd cluster has high number of failed grpc requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.056780931,\n",
       "       'lastEvaluation': '2022-12-14T21:00:14.916653061Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighNumberOfFailedGRPCRequests',\n",
       "       'query': '(sum without(grpc_type, grpc_code) (rate(grpc_server_handled_total{grpc_code=~\"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\",job=\"etcd\"}[5m])) / (sum without(grpc_type, grpc_code) (rate(grpc_server_handled_total{job=\"etcd\"}[5m])) > 2 and on() (sum(cluster_infrastructure_provider{type!~\"ipi|BareMetal\"} == bool 1)))) * 100 > 50',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": {{ $value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdHighNumberOfFailedGRPCRequests.md',\n",
       "        'summary': 'etcd cluster has high number of failed grpc requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.03641207,\n",
       "       'lastEvaluation': '2022-12-14T21:00:14.973436398Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdHighNumberOfLeaderChanges',\n",
       "       'query': 'avg(changes(etcd_server_is_leader[10m])) > 5',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'etcd cluster \"{{ $labels.job }}\": {{ $value }} average leader changes within the last 10 minutes. Frequent elections may be a sign of insufficient resources, high network latency, or disruptions by other components and should be investigated.',\n",
       "        'summary': 'etcd cluster has high number of leader changes.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000184808,\n",
       "       'lastEvaluation': '2022-12-14T21:00:15.009850792Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'etcdInsufficientMembers',\n",
       "       'query': 'sum without(instance, pod) (up{job=\"etcd\"} == bool 1 and etcd_server_has_leader{job=\"etcd\"} == bool 1) < ((count without(instance, pod) (up{job=\"etcd\"}) + 1) / 2)',\n",
       "       'duration': 180,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'etcd is reporting fewer instances are available than are needed ({{ $value }}). When etcd does not have a majority of instances available the Kubernetes and OpenShift APIs will reject read and write requests and operations that preserve the health of workloads cannot be performed. This can occur when multiple control plane nodes are powered off or are unable to connect to each other via the network. Check that all control plane nodes are powered on and that network connections between each machine are functional.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-etcd-operator/etcdInsufficientMembers.md',\n",
       "        'summary': 'etcd is reporting that a majority of instances are unavailable.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000282941,\n",
       "       'lastEvaluation': '2022-12-14T21:00:15.010036393Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.127371119,\n",
       "     'lastEvaluation': '2022-12-14T21:00:14.882950319Z'},\n",
       "    {'name': 'imageregistry.operations.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-image-registry-image-registry-rules-ef4e3e1c-ebaf-4f92-9f53-d0350cd8e786.yaml',\n",
       "     'rules': [{'name': 'imageregistry:operations_count:sum',\n",
       "       'query': 'label_replace(label_replace(sum by(operation) (imageregistry_request_duration_seconds_count{operation=\"BlobStore.ServeBlob\"}), \"operation\", \"get\", \"operation\", \"(.+)\"), \"resource_type\", \"blob\", \"resource_type\", \"\")',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000380096,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.102892992Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'imageregistry:operations_count:sum',\n",
       "       'query': 'label_replace(label_replace(sum by(operation) (imageregistry_request_duration_seconds_count{operation=\"BlobStore.Create\"}), \"operation\", \"create\", \"operation\", \"(.+)\"), \"resource_type\", \"blob\", \"resource_type\", \"\")',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000142078,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.103274911Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'imageregistry:operations_count:sum',\n",
       "       'query': 'label_replace(label_replace(sum by(operation) (imageregistry_request_duration_seconds_count{operation=\"ManifestService.Get\"}), \"operation\", \"get\", \"operation\", \"(.+)\"), \"resource_type\", \"manifest\", \"resource_type\", \"\")',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000118182,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.103418191Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'imageregistry:operations_count:sum',\n",
       "       'query': 'label_replace(label_replace(sum by(operation) (imageregistry_request_duration_seconds_count{operation=\"ManifestService.Put\"}), \"operation\", \"create\", \"operation\", \"(.+)\"), \"resource_type\", \"manifest\", \"resource_type\", \"\")',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000114065,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.103537385Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000766723,\n",
       "     'lastEvaluation': '2022-12-14T21:00:22.10288642Z'},\n",
       "    {'name': 'imagestreams.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-image-registry-imagestreams-rules-9a53f922-0c7a-457f-ba70-23ab77357ba1.yaml',\n",
       "     'rules': [{'name': 'imageregistry:imagestreamtags_count:sum',\n",
       "       'query': 'sum by(location, source) (image_registry_image_stream_tags_total)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000441821,\n",
       "       'lastEvaluation': '2022-12-14T21:00:09.709783132Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000456258,\n",
       "     'lastEvaluation': '2022-12-14T21:00:09.70977676Z'},\n",
       "    {'name': 'openshift-ingress.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-ingress-operator-ingress-operator-9c249f12-8d0c-47e4-a19b-9504e6ff07be.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'HAProxyReloadFail',\n",
       "       'query': 'template_router_reload_failure == 1',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'This alert fires when HAProxy fails to reload its configuration, which will result in the router not picking up recently created or modified routes.',\n",
       "        'message': 'HAProxy reloads are failing on {{ $labels.pod }}. Router is not respecting recently created or modified routes',\n",
       "        'summary': 'HAProxy reload failure'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000280909,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.898144335Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'HAProxyDown',\n",
       "       'query': 'haproxy_up == 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'This alert fires when metrics report that HAProxy is down.',\n",
       "        'message': 'HAProxy metrics are reporting that HAProxy is down on pod {{ $labels.namespace }} / {{ $labels.pod }}',\n",
       "        'summary': 'HAProxy is down'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 6.3259e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.898426526Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'IngressControllerDegraded',\n",
       "       'query': 'ingress_controller_conditions{condition=\"Degraded\"} == 1',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'This alert fires when the IngressController status is degraded.',\n",
       "        'message': 'The {{ $labels.namespace }}/{{ $labels.name }} ingresscontroller is\\ndegraded: {{ $labels.reason }}.\\n',\n",
       "        'summary': 'IngressController is degraded'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 5.9743e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.898490446Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'IngressControllerUnavailable',\n",
       "       'query': 'ingress_controller_conditions{condition=\"Available\"} == 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'This alert fires when the IngressController is not available.',\n",
       "        'message': 'The {{ $labels.namespace }}/{{ $labels.name }} ingresscontroller is\\nunavailable: {{ $labels.reason }}.\\n',\n",
       "        'summary': 'IngressController is unavailable'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 6.5293e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.89855085Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000496574,\n",
       "     'lastEvaluation': '2022-12-14T21:00:23.898121392Z'},\n",
       "    {'name': 'insights',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-insights-insights-prometheus-rules-ca8f733c-db4d-4dad-a2c6-354eb18e994b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'InsightsDisabled',\n",
       "       'query': 'cluster_operator_conditions{condition=\"Disabled\",name=\"insights\"} == 1',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'Insights operator is disabled. In order to enable Insights and benefit from recommendations specific to your cluster, please follow steps listed in the documentation: https://docs.openshift.com/container-platform/latest/support/remote_health_monitoring/enabling-remote-health-reporting.html',\n",
       "        'summary': 'Insights operator is disabled.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000204695,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.563154411Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SimpleContentAccessNotAvailable',\n",
       "       'query': 'max_over_time(cluster_operator_conditions{condition=\"SCAAvailable\",name=\"insights\",reason=\"NotFound\"}[5m]) == 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'Simple content access (SCA) is not enabled. Once enabled, Insights Operator can automatically import the SCA certificates from Red Hat OpenShift Cluster Manager making it easier to use the content provided by your Red Hat subscriptions when creating container images. See https://docs.openshift.com/container-platform/latest/cicd/builds/running-entitled-builds.html for more information.',\n",
       "        'summary': 'Simple content access certificates are not available.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 8.1163e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.563360308Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000304504,\n",
       "     'lastEvaluation': '2022-12-14T21:00:21.56313856Z'},\n",
       "    {'name': 'pre-release-lifecycle',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-api-usage-4f88ccbf-8a71-4c31-9a6f-6b0bd0d3124f.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'APIRemovedInNextReleaseInUse',\n",
       "       'query': 'group by(group, version, resource) (apiserver_requested_deprecated_apis{removed_release=\"1.25\"}) and (sum by(group, version, resource) (rate(apiserver_request_total{system_client!=\"cluster-policy-controller\",system_client!=\"kube-controller-manager\"}[4h]))) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver', 'severity': 'info'},\n",
       "       'annotations': {'description': 'Deprecated API that will be removed in the next version is being used. Removing the workload that is using the {{ $labels.group }}.{{ $labels.version }}/{{ $labels.resource }} API might be necessary for a successful upgrade to the next cluster version. Refer to `oc get apirequestcounts {{ $labels.resource }}.{{ $labels.version }}.{{ $labels.group }} -o yaml` to identify the workload.',\n",
       "        'summary': 'Deprecated API that will be removed in the next version is being used.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.136795283,\n",
       "       'lastEvaluation': '2022-12-14T21:00:10.870786203Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'APIRemovedInNextEUSReleaseInUse',\n",
       "       'query': 'group by(group, version, resource) (apiserver_requested_deprecated_apis{removed_release=~\"1\\\\\\\\.2[5]\"}) and (sum by(group, version, resource) (rate(apiserver_request_total{system_client!=\"cluster-policy-controller\",system_client!=\"kube-controller-manager\"}[4h]))) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver', 'severity': 'info'},\n",
       "       'annotations': {'description': 'Deprecated API that will be removed in the next EUS version is being used. Removing the workload that is using the {{ $labels.group }}.{{ $labels.version }}/{{ $labels.resource }} API might be necessary for a successful upgrade to the next EUS cluster version. Refer to `oc get apirequestcounts {{ $labels.resource }}.{{ $labels.version }}.{{ $labels.group }} -o yaml` to identify the workload.',\n",
       "        'summary': 'Deprecated API that will be removed in the next EUS version is being used.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.142017642,\n",
       "       'lastEvaluation': '2022-12-14T21:00:11.00758364Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.278839907,\n",
       "     'lastEvaluation': '2022-12-14T21:00:10.870766194Z'},\n",
       "    {'name': 'apiserver-audit',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-audit-errors-00a0f7da-3245-4d49-b7d3-18aec935f17f.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'AuditLogError',\n",
       "       'query': 'sum by(apiserver, instance) (rate(apiserver_audit_error_total{apiserver=~\".+-apiserver\"}[5m])) / sum by(apiserver, instance) (rate(apiserver_audit_event_total{apiserver=~\".+-apiserver\"}[5m])) > 0',\n",
       "       'duration': 60,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'An API Server had an error writing to an audit log.',\n",
       "        'summary': 'An API Server instance was unable to write audit logs. This could be\\ntriggered by the node running out of space, or a malicious actor\\ntampering with the audit logs.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000460276,\n",
       "       'lastEvaluation': '2022-12-14T21:00:20.02503772Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000483901,\n",
       "     'lastEvaluation': '2022-12-14T21:00:20.02501658Z'},\n",
       "    {'name': 'control-plane-cpu-utilization',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-cpu-utilization-47bd82bc-ed4a-4fd3-b01e-668267265cef.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'HighOverallControlPlaneCPU',\n",
       "       'query': 'sum(100 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])) * 100) and on(instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\")) / count(kube_node_role{role=\"master\"}) > 60',\n",
       "       'duration': 600,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'Given three control plane nodes, the overall CPU utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the remaining two must handle the load of the cluster in order to be HA. If the cluster is using more than 2/3 of all capacity, if one control plane node fails, the remaining two are likely to fail when they take the load. To fix this, increase the CPU and memory on your control plane nodes.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/ExtremelyHighIndividualControlPlaneCPU.md',\n",
       "        'summary': 'CPU utilization across all three control plane nodes is higher than two control plane nodes can sustain; a single control plane node outage may cause a cascading failure; increase available CPU.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000956609,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.459607194Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ExtremelyHighIndividualControlPlaneCPU',\n",
       "       'query': '100 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])) * 100) > 90 and on(instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\")',\n",
       "       'duration': 300,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'Extreme CPU pressure can cause slow serialization and poor performance from the kube-apiserver and etcd. When this happens, there is a risk of clients seeing non-responsive API requests which are issued again causing even more CPU pressure. It can also cause failing liveness probes due to slow etcd responsiveness on the backend. If one kube-apiserver fails under this condition, chances are you will experience a cascade as the remaining kube-apiservers are also under-provisioned. To fix this, increase the CPU and memory on your control plane nodes.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/ExtremelyHighIndividualControlPlaneCPU.md',\n",
       "        'summary': 'CPU utilization on a single control plane node is very high, more CPU pressure is likely to cause a failover; increase available CPU.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000572418,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.460566087Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ExtremelyHighIndividualControlPlaneCPU',\n",
       "       'query': '100 - (avg by(instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[1m])) * 100) > 90 and on(instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\")',\n",
       "       'duration': 3600,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'critical'},\n",
       "       'annotations': {'description': 'Extreme CPU pressure can cause slow serialization and poor performance from the kube-apiserver and etcd. When this happens, there is a risk of clients seeing non-responsive API requests which are issued again causing even more CPU pressure. It can also cause failing liveness probes due to slow etcd responsiveness on the backend. If one kube-apiserver fails under this condition, chances are you will experience a cascade as the remaining kube-apiservers are also under-provisioned. To fix this, increase the CPU and memory on your control plane nodes.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/ExtremelyHighIndividualControlPlaneCPU.md',\n",
       "        'summary': 'Sustained high CPU utilization on a single control plane node, more CPU pressure is likely to cause a failover; increase available CPU.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000511302,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.461139587Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.002053323,\n",
       "     'lastEvaluation': '2022-12-14T21:00:21.45959972Z'},\n",
       "    {'name': 'apiserver-requests-in-flight',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-kube-apiserver-requests-4982ffaa-f481-47c2-b049-cb090d86f6a0.yaml',\n",
       "     'rules': [{'name': 'cluster:apiserver_current_inflight_requests:sum:max_over_time:2m',\n",
       "       'query': 'max_over_time(sum by(apiserver, requestKind) (apiserver_current_inflight_requests{apiserver=~\"openshift-apiserver|kube-apiserver\"})[2m:])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000489892,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.475382059Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000501724,\n",
       "     'lastEvaluation': '2022-12-14T20:59:54.475372962Z'},\n",
       "    {'name': 'kube-apiserver-slos-basic',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-kube-apiserver-slos-basic-c8755877-826a-4ce7-869c-5a7c3af5323b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeAPIErrorBudgetBurn',\n",
       "       'query': 'sum(apiserver_request:burnrate1h) > (14.4 * 0.01) and sum(apiserver_request:burnrate5m) > (14.4 * 0.01)',\n",
       "       'duration': 120,\n",
       "       'labels': {'long': '1h',\n",
       "        'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'critical',\n",
       "        'short': '5m'},\n",
       "       'annotations': {'description': \"The API server is burning too much error budget. This alert fires when too many requests are failing with high latency. Use the 'API Performance' monitoring dashboards to narrow down the request states and latency. The 'etcd' monitoring dashboards also provides metrics to help determine etcd stability and performance.\",\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/KubeAPIErrorBudgetBurn.md',\n",
       "        'summary': 'The API server is burning too much error budget.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000410803,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.174951335Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeAPIErrorBudgetBurn',\n",
       "       'query': 'sum(apiserver_request:burnrate6h) > (6 * 0.01) and sum(apiserver_request:burnrate30m) > (6 * 0.01)',\n",
       "       'duration': 900,\n",
       "       'labels': {'long': '6h',\n",
       "        'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'critical',\n",
       "        'short': '30m'},\n",
       "       'annotations': {'description': \"The API server is burning too much error budget. This alert fires when too many requests are failing with high latency. Use the 'API Performance' monitoring dashboards to narrow down the request states and latency. The 'etcd' monitoring dashboards also provides metrics to help determine etcd stability and performance.\",\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/KubeAPIErrorBudgetBurn.md',\n",
       "        'summary': 'The API server is burning too much error budget.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000184297,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.17536339Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000606741,\n",
       "     'lastEvaluation': '2022-12-14T21:00:03.1749435Z'},\n",
       "    {'name': 'kube-apiserver.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-kube-apiserver-slos-basic-c8755877-826a-4ce7-869c-5a7c3af5323b.yaml',\n",
       "     'rules': [{'name': 'apiserver_request:burnrate5m',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[5m])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[5m])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[5m])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[5m]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[5m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[5m]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[5m]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[5m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[5m]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.076540225,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.579426345Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate30m',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[30m])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[30m]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[30m])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[30m])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[30m]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[30m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[30m]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[30m]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[30m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[30m]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[30m]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.089545549,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.65597205Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate1h',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[1h])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1h]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1h])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1h])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1h]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1h]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1h]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[1h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[1h]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1h]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.098532721,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.745522578Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate6h',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[6h])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[6h]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[6h])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[6h])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[6h]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[6h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[6h]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[6h]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[6h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[6h]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[6h]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.457315236,\n",
       "       'lastEvaluation': '2022-12-14T20:59:53.858152883Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate1h',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1h]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.02562465,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.315478028Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate30m',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[30m]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.020877969,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.341105343Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate5m',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.019025482,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.36198745Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate6h',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[6h]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.113699503,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.381016689Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'code_resource:apiserver_request_total:rate5m',\n",
       "       'query': 'sum by(code, resource) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.012961141,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.49472558Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'code_resource:apiserver_request_total:rate5m',\n",
       "       'query': 'sum by(code, resource) (rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.007904766,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.507691409Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum by(le, resource) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",verb=~\"LIST|GET\"}[5m]))) > 0',\n",
       "       'labels': {'quantile': '0.99', 'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.141758002,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.515600964Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum by(le, resource) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[5m]))) > 0',\n",
       "       'labels': {'quantile': '0.99', 'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.078852527,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.657363144Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum without(instance, pod) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.146544761,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.736219057Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.9, sum without(instance, pod) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])))',\n",
       "       'labels': {'quantile': '0.9'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.11942182,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.88276983Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:apiserver_request_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.5, sum without(instance, pod) (rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",subresource!=\"log\",verb!~\"LIST|WATCH|WATCHLIST|DELETECOLLECTION|PROXY|CONNECT\"}[5m])))',\n",
       "       'labels': {'quantile': '0.5'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.117564415,\n",
       "       'lastEvaluation': '2022-12-14T20:59:55.002197902Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 1.539886321,\n",
       "     'lastEvaluation': '2022-12-14T20:59:53.579881687Z'},\n",
       "    {'name': 'kube-apiserver-slos-extended',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-kube-apiserver-slos-extended-d330b76d-b208-47cb-8ea9-63bd0a9c40e9.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeAPIErrorBudgetBurn',\n",
       "       'query': 'sum(apiserver_request:burnrate1d) > (3 * 0.01) and sum(apiserver_request:burnrate2h) > (3 * 0.01)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'long': '1d',\n",
       "        'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'warning',\n",
       "        'short': '2h'},\n",
       "       'annotations': {'description': \"The API server is burning too much error budget. This alert fires when too many requests are failing with high latency. Use the 'API Performance' monitoring dashboards to narrow down the request states and latency. The 'etcd' monitoring dashboards also provides metrics to help determine etcd stability and performance.\",\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/KubeAPIErrorBudgetBurn.md',\n",
       "        'summary': 'The API server is burning too much error budget.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000540187,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.157141036Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeAPIErrorBudgetBurn',\n",
       "       'query': 'sum(apiserver_request:burnrate3d) > (1 * 0.01) and sum(apiserver_request:burnrate6h) > (1 * 0.01)',\n",
       "       'duration': 10800,\n",
       "       'labels': {'long': '3d',\n",
       "        'namespace': 'openshift-kube-apiserver',\n",
       "        'severity': 'warning',\n",
       "        'short': '6h'},\n",
       "       'annotations': {'description': \"The API server is burning too much error budget. This alert fires when too many requests are failing with high latency. Use the 'API Performance' monitoring dashboards to narrow down the request states and latency. The 'etcd' monitoring dashboards also provides metrics to help determine etcd stability and performance.\",\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-apiserver-operator/KubeAPIErrorBudgetBurn.md',\n",
       "        'summary': 'The API server is burning too much error budget.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000336052,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.157684058Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000892811,\n",
       "     'lastEvaluation': '2022-12-14T21:00:21.157130876Z'},\n",
       "    {'name': 'kube-apiserver.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-kube-apiserver-slos-extended-d330b76d-b208-47cb-8ea9-63bd0a9c40e9.yaml',\n",
       "     'rules': [{'name': 'apiserver_request:burnrate2h',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[2h])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[2h]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[2h])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[2h])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[2h]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[2h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[2h]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[2h]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[2h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[2h]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[2h]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.143617042,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.749785623Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate1d',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[1d])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1d]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1d])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1d])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1d]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1d]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[1d]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[1d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[1d]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[1d]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 1.266368961,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.893406842Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate3d',\n",
       "       'query': 'label_replace(sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"LIST|GET\"}[3d])) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[3d]))), \"type\", \"error\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[3d])) - (sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",scope=\"resource\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[3d])) or vector(0))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[3d]))), \"type\", \"slow-resource\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[3d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"5\",scope=\"namespace\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[3d]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",subresource!~\"proxy|log|exec\",verb=~\"LIST|GET\"}[3d]))), \"type\", \"slow-namespace\", \"_none_\", \"\") or label_replace((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",scope=\"cluster\",verb=~\"LIST|GET\"}[3d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"30\",scope=\"cluster\",verb=~\"LIST|GET\"}[3d]))) / scalar(sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"LIST|GET\"}[3d]))), \"type\", \"slow-cluster\", \"_none_\", \"\")',\n",
       "       'labels': {'verb': 'read'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 3.6327974469999997,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.159782385Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate1d',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[1d]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.342853139,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.792585442Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate2h',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[2h]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.038717028,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.135445423Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'apiserver_request:burnrate3d',\n",
       "       'query': '((sum(rate(apiserver_request_duration_seconds_count{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d])) - sum(rate(apiserver_request_duration_seconds_bucket{job=\"apiserver\",le=\"1\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))) + sum(rate(apiserver_request_total{code=~\"5..\",job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))) / sum(rate(apiserver_request_total{job=\"apiserver\",verb=~\"POST|PUT|PATCH|DELETE\"}[3d]))',\n",
       "       'labels': {'verb': 'write'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 1.013455803,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.174168283Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 6.437851778,\n",
       "     'lastEvaluation': '2022-12-14T20:59:58.749776936Z'},\n",
       "    {'name': 'cluster-version',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-operator-kube-apiserver-operator-8ea7bfb1-530e-47f2-8e7d-4c9df646ffd1.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'TechPreviewNoUpgrade',\n",
       "       'query': 'cluster_feature_set{name!=\"\",namespace=\"openshift-kube-apiserver-operator\"} == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Cluster has enabled Technology Preview features that cannot be undone and will prevent upgrades. The TechPreviewNoUpgrade feature set is not recommended on production clusters.',\n",
       "        'summary': 'Cluster has enabled tech preview features that will prevent upgrades.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001244652,\n",
       "       'lastEvaluation': '2022-12-14T20:59:54.621356727Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001254951,\n",
       "     'lastEvaluation': '2022-12-14T20:59:54.621348762Z'},\n",
       "    {'name': 'pod-security-violation',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-apiserver-podsecurity-a5d3a278-6e6a-441c-8c4f-9bb7aaa1cf53.yaml',\n",
       "     'rules': [{'state': 'firing',\n",
       "       'name': 'PodSecurityViolation',\n",
       "       'query': 'sum by(policy_level) (increase(pod_security_evaluations_total{decision=\"deny\",mode=\"audit\",resource=\"pod\"}[1d])) > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'namespace': 'openshift-kube-apiserver', 'severity': 'info'},\n",
       "       'annotations': {'description': 'A workload (pod, deployment, deamonset, ...) was created somewhere in the cluster but it did not match the PodSecurity \"{{ $labels.policy_level }}\" profile defined by its namespace either via the cluster-wide configuration (which triggers on a \"restricted\" profile violations) or by the namespace local Pod Security labels. Refer to Kubernetes documentation on Pod Security Admission to learn more about these violations.',\n",
       "        'summary': \"One or more workloads users created in the cluster don't match their Pod Security profile\"},\n",
       "       'alerts': [{'labels': {'alertname': 'PodSecurityViolation',\n",
       "          'namespace': 'openshift-kube-apiserver',\n",
       "          'policy_level': 'restricted',\n",
       "          'severity': 'info'},\n",
       "         'annotations': {'description': 'A workload (pod, deployment, deamonset, ...) was created somewhere in the cluster but it did not match the PodSecurity \"restricted\" profile defined by its namespace either via the cluster-wide configuration (which triggers on a \"restricted\" profile violations) or by the namespace local Pod Security labels. Refer to Kubernetes documentation on Pod Security Admission to learn more about these violations.',\n",
       "          'summary': \"One or more workloads users created in the cluster don't match their Pod Security profile\"},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2022-12-12T23:45:48.993979439Z',\n",
       "         'value': '2.1007294199374783e+01'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001616943,\n",
       "       'lastEvaluation': '2022-12-14T21:00:18.995377873Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001627973,\n",
       "     'lastEvaluation': '2022-12-14T21:00:18.995370098Z'},\n",
       "    {'name': 'cluster-version',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-controller-manager-operator-kube-controller-manager-operator-fc61d916-9720-4cb0-8b8e-2f5a614a01d9.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeControllerManagerDown',\n",
       "       'query': 'absent(up{job=\"kube-controller-manager\"} == 1)',\n",
       "       'duration': 900,\n",
       "       'labels': {'namespace': 'openshift-kube-controller-manager',\n",
       "        'severity': 'critical'},\n",
       "       'annotations': {'description': 'KubeControllerManager has disappeared from Prometheus target discovery.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-controller-manager-operator/KubeControllerManagerDown.md',\n",
       "        'summary': 'Target disappeared from Prometheus target discovery.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00039891,\n",
       "       'lastEvaluation': '2022-12-14T21:00:17.653814812Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PodDisruptionBudgetAtLimit',\n",
       "       'query': 'max by(namespace, poddisruptionbudget) (kube_poddisruptionbudget_status_current_healthy == kube_poddisruptionbudget_status_desired_healthy and on(namespace, poddisruptionbudget) kube_poddisruptionbudget_status_expected_pods > 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The pod disruption budget is at the minimum disruptions allowed level. The number of current healthy pods is equal to the desired healthy pods.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-controller-manager-operator/PodDisruptionBudgetAtLimit.md',\n",
       "        'summary': 'The pod disruption budget is preventing further disruption to pods.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000548913,\n",
       "       'lastEvaluation': '2022-12-14T21:00:17.654215365Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PodDisruptionBudgetLimit',\n",
       "       'query': 'max by(namespace, poddisruptionbudget) (kube_poddisruptionbudget_status_current_healthy < kube_poddisruptionbudget_status_desired_healthy)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'The pod disruption budget is below the minimum disruptions allowed level and is not satisfied. The number of current healthy pods is less than the desired healthy pods.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-controller-manager-operator/PodDisruptionBudgetLimit.md',\n",
       "        'summary': 'The pod disruption budget registers insufficient amount of pods.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000292341,\n",
       "       'lastEvaluation': '2022-12-14T21:00:17.65476566Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'GarbageCollectorSyncFailed',\n",
       "       'query': 'rate(garbagecollector_controller_resources_sync_error_total[5m]) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Garbage Collector had a problem with syncing and monitoring the available resources. Please see KubeControllerManager logs for more details.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-kube-controller-manager-operator/GarbageCollectorSyncFailed.md',\n",
       "        'summary': 'There was a problem with syncing the resources for garbage collection.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000139262,\n",
       "       'lastEvaluation': '2022-12-14T21:00:17.655059133Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001396459,\n",
       "     'lastEvaluation': '2022-12-14T21:00:17.65380403Z'},\n",
       "    {'name': 'cluster-version',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-scheduler-operator-kube-scheduler-operator-0a88002e-dd71-4384-af96-2e16da5169e6.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeSchedulerDown',\n",
       "       'query': 'absent(up{job=\"scheduler\"} == 1)',\n",
       "       'duration': 900,\n",
       "       'labels': {'namespace': 'openshift-kube-scheduler',\n",
       "        'severity': 'critical'},\n",
       "       'annotations': {'description': 'KubeScheduler has disappeared from Prometheus target discovery.',\n",
       "        'summary': 'Target disappeared from Prometheus target discovery.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000263936,\n",
       "       'lastEvaluation': '2022-12-14T21:00:18.179240499Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000280869,\n",
       "     'lastEvaluation': '2022-12-14T21:00:18.17922577Z'},\n",
       "    {'name': 'scheduler-legacy-policy-deprecated',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-kube-scheduler-operator-kube-scheduler-operator-0a88002e-dd71-4384-af96-2e16da5169e6.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'SchedulerLegacyPolicySet',\n",
       "       'query': 'cluster_legacy_scheduler_policy > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The scheduler is currently configured to use a legacy scheduler policy API. Use of the policy API is deprecated and removed in 4.10.',\n",
       "        'summary': 'Legacy scheduler policy API in use by the scheduler.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000208242,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.086411118Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000231505,\n",
       "     'lastEvaluation': '2022-12-14T21:00:23.086390489Z'},\n",
       "    {'name': 'machine-api-operator-metrics-collector-up',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules-4d1f7c08-4e2e-4002-8fe8-e5dad995ee63.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineAPIOperatorMetricsCollectionFailing',\n",
       "       'query': 'mapi_mao_collector_up == 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'For more details:  oc logs <machine-api-operator-pod-name> -n openshift-machine-api',\n",
       "        'summary': 'machine api operator metrics collection is failing.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000342585,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.40489535Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000362043,\n",
       "     'lastEvaluation': '2022-12-14T21:00:04.404878427Z'},\n",
       "    {'name': 'machine-health-check-unterminated-short-circuit',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules-4d1f7c08-4e2e-4002-8fe8-e5dad995ee63.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineHealthCheckUnterminatedShortCircuit',\n",
       "       'query': 'mapi_machinehealthcheck_short_circuit == 1',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The number of unhealthy machines has exceeded the `maxUnhealthy` limit for the check, you should check\\nthe status of machines in the cluster.\\n',\n",
       "        'summary': 'machine health check {{ $labels.name }} has been disabled by short circuit for more than 30 minutes'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000364666,\n",
       "       'lastEvaluation': '2022-12-14T21:00:13.588315958Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000415543,\n",
       "     'lastEvaluation': '2022-12-14T21:00:13.588271013Z'},\n",
       "    {'name': 'machine-not-yet-deleted',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules-4d1f7c08-4e2e-4002-8fe8-e5dad995ee63.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineNotYetDeleted',\n",
       "       'query': 'sum by(name, namespace) (avg_over_time(mapi_machine_created_timestamp_seconds{phase=\"Deleting\"}[15m])) > 0',\n",
       "       'duration': 21600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The machine is not properly deleting, this may be due to a configuration issue with the\\ninfrastructure provider, or because workloads on the node have PodDisruptionBudgets or\\nlong termination periods which are preventing deletion.\\n',\n",
       "        'summary': 'machine {{ $labels.name }} has been in Deleting phase for more than 6 hours'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000179448,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.432804661Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000185269,\n",
       "     'lastEvaluation': '2022-12-14T20:59:58.432800974Z'},\n",
       "    {'name': 'machine-with-no-running-phase',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules-4d1f7c08-4e2e-4002-8fe8-e5dad995ee63.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineWithNoRunningPhase',\n",
       "       'query': 'sum by(name, namespace) (mapi_machine_created_timestamp_seconds{phase!~\"Running|Deleting\"}) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The machine has been without a Running or Deleting phase for more than 60 minutes.\\nThe machine may not have been provisioned properly from the infrastructure provider, or\\nit might have issues with CertificateSigningRequests being approved.\\n',\n",
       "        'summary': 'machine {{ $labels.name }} is in phase: {{ $labels.phase }}'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000266872,\n",
       "       'lastEvaluation': '2022-12-14T21:00:10.725669288Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000288381,\n",
       "     'lastEvaluation': '2022-12-14T21:00:10.725650694Z'},\n",
       "    {'name': 'machine-without-valid-node-ref',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-api-machine-api-operator-prometheus-rules-4d1f7c08-4e2e-4002-8fe8-e5dad995ee63.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineWithoutValidNode',\n",
       "       'query': 'sum by(name, namespace) (mapi_machine_created_timestamp_seconds unless on(node) kube_node_info) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'If the machine never became a node, you should diagnose the machine related failures.\\nIf the node was deleted from the API, you may delete the machine if appropriate.\\n',\n",
       "        'summary': 'machine {{ $labels.name }} does not have valid node reference'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000435129,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.043438705Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.00046795,\n",
       "     'lastEvaluation': '2022-12-14T21:00:12.04340933Z'},\n",
       "    {'name': 'mcc-paused-pool-kubelet-ca',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-controller-b28620b5-2ca2-4dc3-8a95-0bebbb41c93e.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MachineConfigControllerPausedPoolKubeletCA',\n",
       "       'query': 'max by(namespace, pool) (last_over_time(machine_config_controller_paused_pool_kubelet_ca[5m])) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"Machine config pools have a 'pause' feature, which allows config to be rendered, but prevents it from being rolled out to the nodes. This alert indicates that a certificate rotation has taken place, and the new kubelet-ca certificate bundle has been rendered into a machine config, but because the pool '{{$labels.pool}}' is paused, the config cannot be rolled out to the nodes in that pool. You will notice almost immediately that for nodes in pool '{{$labels.pool}}', pod logs will not be visible in the console and interactive commands (oc log, oc exec, oc debug, oc attach) will not work. You must unpause machine config pool '{{$labels.pool}}' to let the certificates through before the kube-apiserver-to-kubelet-signer certificate expires on {{ $value | humanizeTimestamp }} or this pool's nodes will cease to function properly.\",\n",
       "        'runbook_url': 'https://github.com/openshift/blob/master/alerts/machine-config-operator/MachineConfigControllerPausedPoolKubeletCA.md',\n",
       "        'summary': \"Paused machine configuration pool '{{$labels.pool}}' is blocking a necessary certificate rotation and must be unpaused before the current kube-apiserver-to-kubelet-signer certificate expires on {{ $value | humanizeTimestamp }}.\"},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000305194,\n",
       "       'lastEvaluation': '2022-12-14T21:00:19.359940305Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'MachineConfigControllerPausedPoolKubeletCA',\n",
       "       'query': 'max by(namespace, pool) (last_over_time(machine_config_controller_paused_pool_kubelet_ca[5m]) - time()) < (86400 * 14) and max by(namespace, pool) (last_over_time(machine_config_controller_paused_pool_kubelet_ca[5m])) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': \"Machine config pools have a 'pause' feature, which allows config to be rendered, but prevents it from being rolled out to the nodes. This alert indicates that a certificate rotation has taken place, and the new kubelet-ca certificate bundle has been rendered into a machine config, but because the pool '{{$labels.pool}}' is paused, the config cannot be rolled out to the nodes in that pool. You will notice almost immediately that for nodes in pool '{{$labels.pool}}', pod logs will not be visible in the console and interactive commands (oc log, oc exec, oc debug, oc attach) will not work. You must unpause machine config pool '{{$labels.pool}}' to let the certificates through before the kube-apiserver-to-kubelet-signer certificate expires. You have approximately {{ $value | humanizeDuration }} remaining before this happens and nodes in '{{$labels.pool}}' cease to function properly.\",\n",
       "        'runbook_url': 'https://github.com/openshift/blob/master/alerts/machine-config-operator/MachineConfigControllerPausedPoolKubeletCA.md',\n",
       "        'summary': \"Paused machine configuration pool '{{$labels.pool}}' is blocking a necessary certificate rotation and must be unpaused before the current kube-apiserver-to-kubelet-signer certificate expires in {{ $value | humanizeDuration }}.\"},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000279896,\n",
       "       'lastEvaluation': '2022-12-14T21:00:19.360246892Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000597954,\n",
       "     'lastEvaluation': '2022-12-14T21:00:19.35993222Z'},\n",
       "    {'name': 'extremely-high-individual-control-plane-memory',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-4093103d-4f12-4dc5-8760-32ece9d4579b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ExtremelyHighIndividualControlPlaneMemory',\n",
       "       'query': '(1 - sum by(instance) (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes and on(instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\")) / sum by(instance) (node_memory_MemTotal_bytes and on(instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\"))) * 100 > 90',\n",
       "       'duration': 2700,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'The memory utilization per instance within control plane nodes influence the stability, and responsiveness of the cluster. This can lead to cluster instability and slow responses from kube-apiserver or failing requests specially on etcd. Moreover, OOM kill is expected which negatively influences the pod scheduling. If this happens on container level, the descheduler will not be able to detect it, as it works on the pod level. To fix this, increase memory of the affected node of control plane nodes.',\n",
       "        'summary': 'Extreme memory utilization per node within control plane nodes is extremely high, and could impact responsiveness and stability.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001093448,\n",
       "       'lastEvaluation': '2022-12-14T21:00:14.49481134Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001105631,\n",
       "     'lastEvaluation': '2022-12-14T21:00:14.494803315Z'},\n",
       "    {'name': 'high-overall-control-plane-memory',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-4093103d-4f12-4dc5-8760-32ece9d4579b.yaml',\n",
       "     'rules': [{'state': 'firing',\n",
       "       'name': 'HighOverallControlPlaneMemory',\n",
       "       'query': '(1 - sum(node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes and on(instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\")) / sum(node_memory_MemTotal_bytes and on(instance) label_replace(kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\"))) * 100 > 60',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Given three control plane nodes, the overall memory utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the kube-apiserver and etcd my be slow to respond. To fix this, increase memory of the control plane nodes.',\n",
       "        'summary': 'Memory utilization across all control plane nodes is high, and could impact responsiveness and stability.'},\n",
       "       'alerts': [{'labels': {'alertname': 'HighOverallControlPlaneMemory',\n",
       "          'severity': 'warning'},\n",
       "         'annotations': {'description': 'Given three control plane nodes, the overall memory utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the kube-apiserver and etcd my be slow to respond. To fix this, increase memory of the control plane nodes.',\n",
       "          'summary': 'Memory utilization across all control plane nodes is high, and could impact responsiveness and stability.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2022-12-14T07:26:50.221912522Z',\n",
       "         'value': '7.144292585063158e+01'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001130607,\n",
       "       'lastEvaluation': '2022-12-14T21:00:20.22345772Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.00114232,\n",
       "     'lastEvaluation': '2022-12-14T21:00:20.223449655Z'},\n",
       "    {'name': 'mcd-drain-error',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-4093103d-4f12-4dc5-8760-32ece9d4579b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MCDDrainError',\n",
       "       'query': 'mcd_drain_err > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'message': 'Drain failed on {{ $labels.node }} , updates may be blocked. For more details check MachineConfigController pod logs: oc logs -f -n {{ $labels.namespace }} machine-config-controller-xxxxx -c machine-config-controller'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000294083,\n",
       "       'lastEvaluation': '2022-12-14T21:00:14.651866691Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000311074,\n",
       "     'lastEvaluation': '2022-12-14T21:00:14.651852645Z'},\n",
       "    {'name': 'mcd-kubelet-health-state-error',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-4093103d-4f12-4dc5-8760-32ece9d4579b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeletHealthState',\n",
       "       'query': 'mcd_kubelet_state > 2',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'message': 'Kubelet health failure threshold reached'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000257074,\n",
       "       'lastEvaluation': '2022-12-14T21:00:02.624265071Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000265801,\n",
       "     'lastEvaluation': '2022-12-14T21:00:02.624258679Z'},\n",
       "    {'name': 'mcd-pivot-error',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-4093103d-4f12-4dc5-8760-32ece9d4579b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MCDPivotError',\n",
       "       'query': 'mcd_pivot_err > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'message': 'Error detected in pivot logs on {{ $labels.node }} '},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000218491,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.026105295Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000226737,\n",
       "     'lastEvaluation': '2022-12-14T21:00:04.026099804Z'},\n",
       "    {'name': 'mcd-reboot-error',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-4093103d-4f12-4dc5-8760-32ece9d4579b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'MCDRebootError',\n",
       "       'query': 'mcd_reboot_err > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'message': 'Reboot failed on {{ $labels.node }} , update may be blocked'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000245862,\n",
       "       'lastEvaluation': '2022-12-14T21:00:18.210486267Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000270408,\n",
       "     'lastEvaluation': '2022-12-14T21:00:18.210463855Z'},\n",
       "    {'name': 'system-memory-exceeds-reservation',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-machine-config-operator-machine-config-daemon-4093103d-4f12-4dc5-8760-32ece9d4579b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'SystemMemoryExceedsReservation',\n",
       "       'query': 'sum by(node) (container_memory_rss{id=\"/system.slice\"}) > ((sum by(node) (kube_node_status_capacity{resource=\"memory\"} - kube_node_status_allocatable{resource=\"memory\"})) * 0.95)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'message': 'System memory usage of {{ $value | humanize }} on {{ $labels.node }} exceeds 95% of the reservation. Reserved memory ensures system processes can function even when the node is fully allocated and protects against workload out of memory events impacting the proper functioning of the node. The default reservation is expected to be sufficient for most configurations and should be increased (https://docs.openshift.com/container-platform/latest/nodes/nodes/nodes-nodes-managing.html) when running nodes with high numbers of pods (either due to rate of change or at steady state).'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000540297,\n",
       "       'lastEvaluation': '2022-12-14T21:00:17.303176923Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000545177,\n",
       "     'lastEvaluation': '2022-12-14T21:00:17.303174899Z'},\n",
       "    {'name': 'marketplace.certified_operators.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-marketplace-marketplace-alert-rules-69dc7f65-8dc8-4244-8ada-d77bd41cecef.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'CertifiedOperatorsCatalogError',\n",
       "       'query': 'catalogsource_ready{exported_namespace=\"openshift-marketplace\",name=\"certified-operators\"} == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Fires whenever the certified-operators source is not ready for more than 10 mins.',\n",
       "        'message': 'Default OperatorHub source \"certified-operators\" is in Non-Ready state for more than 10 mins.',\n",
       "        'summary': 'Certified-operators not ready for more than 10 minutes'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000274587,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.405020174Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000280087,\n",
       "     'lastEvaluation': '2022-12-14T21:00:04.405015786Z'},\n",
       "    {'name': 'marketplace.community_operators.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-marketplace-marketplace-alert-rules-69dc7f65-8dc8-4244-8ada-d77bd41cecef.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'CommunityOperatorsCatalogError',\n",
       "       'query': 'catalogsource_ready{exported_namespace=\"openshift-marketplace\",name=\"community-operators\"} == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Fires whenever the community-operators source is not ready for more than 10 mins.',\n",
       "        'message': 'Default OperatorHub source \"community-operators\" is in Non-Ready state for more than 10 mins.',\n",
       "        'summary': 'Community-operators not ready for 10 minutes'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000236305,\n",
       "       'lastEvaluation': '2022-12-14T21:00:01.652890978Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000246564,\n",
       "     'lastEvaluation': '2022-12-14T21:00:01.652883184Z'},\n",
       "    {'name': 'marketplace.redhat_marketplace.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-marketplace-marketplace-alert-rules-69dc7f65-8dc8-4244-8ada-d77bd41cecef.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'RedhatMarketplaceCatalogError',\n",
       "       'query': 'catalogsource_ready{exported_namespace=\"openshift-marketplace\",name=\"redhat-marketplace\"} == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Fires whenever the redhat-marketplace source is not ready for more than 10 mins.',\n",
       "        'message': 'Default OperatorHub source \"redhat-marketplace\" is in Non-Ready state for more than 10 mins.',\n",
       "        'summary': 'Redhat-marketplace not ready for more than 10 minutes'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000281541,\n",
       "       'lastEvaluation': '2022-12-14T21:00:10.895839968Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000310304,\n",
       "     'lastEvaluation': '2022-12-14T21:00:10.89581411Z'},\n",
       "    {'name': 'marketplace.redhat_operators.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-marketplace-marketplace-alert-rules-69dc7f65-8dc8-4244-8ada-d77bd41cecef.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'RedhatOperatorsCatalogError',\n",
       "       'query': 'catalogsource_ready{exported_namespace=\"openshift-marketplace\",name=\"redhat-operators\"} == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Fires whenever the redhat-operators source is not ready for more than 10 mins.',\n",
       "        'message': 'Default OperatorHub source \"redhat-operators\" is in Non-Ready state for more than 10 mins.',\n",
       "        'summary': 'Redhat-operators not ready for more than 10 minutes'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000301578,\n",
       "       'lastEvaluation': '2022-12-14T21:00:17.841646326Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000336283,\n",
       "     'lastEvaluation': '2022-12-14T21:00:17.841614797Z'},\n",
       "    {'name': 'alertmanager.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-alertmanager-main-rules-d0d1c4ca-97f8-42da-b9bc-b0373cff3564.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'AlertmanagerFailedReload',\n",
       "       'query': 'max_over_time(alertmanager_config_last_reload_successful{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]) == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedReload.md',\n",
       "        'summary': 'Reloading an Alertmanager configuration has failed.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000394813,\n",
       "       'lastEvaluation': '2022-12-14T21:00:07.111184974Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'AlertmanagerMembersInconsistent',\n",
       "       'query': 'max_over_time(alertmanager_cluster_members{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]) < on(namespace, service) group_left() count by(namespace, service) (max_over_time(alertmanager_cluster_members{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]))',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only found {{ $value }} members of the {{$labels.job}} cluster.',\n",
       "        'summary': 'A member of an Alertmanager cluster has not found all other cluster members.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000365487,\n",
       "       'lastEvaluation': '2022-12-14T21:00:07.11158135Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'AlertmanagerFailedToSendAlerts',\n",
       "       'query': '(rate(alertmanager_notifications_failed_total{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]) / rate(alertmanager_notifications_total{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m])) > 0.01',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration }}.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedToSendAlerts.md',\n",
       "        'summary': 'An Alertmanager instance failed to send notifications.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000684127,\n",
       "       'lastEvaluation': '2022-12-14T21:00:07.11194816Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'AlertmanagerClusterFailedToSendAlerts',\n",
       "       'query': 'min by(namespace, service, integration) (rate(alertmanager_notifications_failed_total{integration=~\".*\",job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]) / rate(alertmanager_notifications_total{integration=~\".*\",job=~\"alertmanager-main|alertmanager-user-workload\"}[5m])) > 0.01',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerClusterFailedToSendAlerts.md',\n",
       "        'summary': 'All Alertmanager instances in a cluster failed to send notifications to a critical integration.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000558181,\n",
       "       'lastEvaluation': '2022-12-14T21:00:07.112633359Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'AlertmanagerConfigInconsistent',\n",
       "       'query': 'count by(namespace, service) (count_values by(namespace, service) (\"config_hash\", alertmanager_config_hash{job=~\"alertmanager-main|alertmanager-user-workload\"})) != 1',\n",
       "       'duration': 1200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Alertmanager instances within the {{$labels.job}} cluster have different configurations.',\n",
       "        'summary': 'Alertmanager instances within the same cluster have different configurations.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000126978,\n",
       "       'lastEvaluation': '2022-12-14T21:00:07.113192612Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'AlertmanagerClusterDown',\n",
       "       'query': '(count by(namespace, service) (avg_over_time(up{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]) < 0.5) / count by(namespace, service) (up{job=~\"alertmanager-main|alertmanager-user-workload\"})) >= 0.5',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have been up for less than half of the last 5m.',\n",
       "        'summary': 'Half or more of the Alertmanager instances within the same cluster are down.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000218712,\n",
       "       'lastEvaluation': '2022-12-14T21:00:07.113320402Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.002362525,\n",
       "     'lastEvaluation': '2022-12-14T21:00:07.111178412Z'},\n",
       "    {'name': 'general.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-bcb44649-067e-47ea-ac05-f4300b55f04c.yaml',\n",
       "     'rules': [{'state': 'firing',\n",
       "       'name': 'Watchdog',\n",
       "       'query': 'vector(1)',\n",
       "       'duration': 0,\n",
       "       'labels': {'namespace': 'openshift-monitoring', 'severity': 'none'},\n",
       "       'annotations': {'description': 'This is an alert meant to ensure that the entire alerting pipeline is functional.\\nThis alert is always firing, therefore it should always be firing in Alertmanager\\nand always fire against a receiver. There are integrations with various notification\\nmechanisms that send a notification when this alert is not firing. For example the\\n\"DeadMansSnitch\" integration in PagerDuty.\\n',\n",
       "        'summary': 'An alert that should always be firing to certify that Alertmanager is working properly.'},\n",
       "       'alerts': [{'labels': {'alertname': 'Watchdog',\n",
       "          'namespace': 'openshift-monitoring',\n",
       "          'severity': 'none'},\n",
       "         'annotations': {'description': 'This is an alert meant to ensure that the entire alerting pipeline is functional.\\nThis alert is always firing, therefore it should always be firing in Alertmanager\\nand always fire against a receiver. There are integrations with various notification\\nmechanisms that send a notification when this alert is not firing. For example the\\n\"DeadMansSnitch\" integration in PagerDuty.\\n',\n",
       "          'summary': 'An alert that should always be firing to certify that Alertmanager is working properly.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2022-12-11T04:20:32.010036175Z',\n",
       "         'value': '1e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000391366,\n",
       "       'lastEvaluation': '2022-12-14T21:00:02.01115595Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000402647,\n",
       "     'lastEvaluation': '2022-12-14T21:00:02.011148075Z'},\n",
       "    {'name': 'kube-prometheus-general.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-bcb44649-067e-47ea-ac05-f4300b55f04c.yaml',\n",
       "     'rules': [{'name': 'count:up1',\n",
       "       'query': 'count without(instance, pod, node) (up == 1)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001735336,\n",
       "       'lastEvaluation': '2022-12-14T21:00:07.092444958Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'count:up0',\n",
       "       'query': 'count without(instance, pod, node) (up == 0)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000865169,\n",
       "       'lastEvaluation': '2022-12-14T21:00:07.094182387Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.002611585,\n",
       "     'lastEvaluation': '2022-12-14T21:00:07.092437965Z'},\n",
       "    {'name': 'kube-prometheus-node-recording.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-bcb44649-067e-47ea-ac05-f4300b55f04c.yaml',\n",
       "     'rules': [{'name': 'instance:node_cpu:rate:sum',\n",
       "       'query': 'sum by(instance) (rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[3m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001469846,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.855985822Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_network_receive_bytes:rate:sum',\n",
       "       'query': 'sum by(instance) (rate(node_network_receive_bytes_total[3m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000504799,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.857457712Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_network_transmit_bytes:rate:sum',\n",
       "       'query': 'sum by(instance) (rate(node_network_transmit_bytes_total[3m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000487357,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.857963994Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:node_cpu:sum_rate5m',\n",
       "       'query': 'sum(rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001239391,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.858453065Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:node_cpu:ratio',\n",
       "       'query': 'cluster:node_cpu:sum_rate5m / count(sum by(instance, cpu) (node_cpu_seconds_total))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001692846,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.859694119Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.005411384,\n",
       "     'lastEvaluation': '2022-12-14T21:00:04.855978788Z'},\n",
       "    {'name': 'kubernetes-recurring.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-bcb44649-067e-47ea-ac05-f4300b55f04c.yaml',\n",
       "     'rules': [{'name': 'cluster:usage:workload:capacity_physical_cpu_core_seconds',\n",
       "       'query': 'sum_over_time(workload:capacity_physical_cpu_cores:sum[30s:1s]) + ((cluster:usage:workload:capacity_physical_cpu_core_seconds offset 25s) or (absent(cluster:usage:workload:capacity_physical_cpu_core_seconds offset 25s) * 0))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000506303,\n",
       "       'lastEvaluation': '2022-12-14T20:59:55.386883732Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000522373,\n",
       "     'lastEvaluation': '2022-12-14T20:59:55.38687178Z'},\n",
       "    {'name': 'node-network',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-bcb44649-067e-47ea-ac05-f4300b55f04c.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'NodeNetworkInterfaceFlapping',\n",
       "       'query': 'changes(node_network_up{device!~\"veth.+|tunbr\",job=\"node-exporter\"}[2m]) > 2',\n",
       "       'duration': 120,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Network interface \"{{ $labels.device }}\" changing its up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}',\n",
       "        'summary': 'Network interface is often changing its status'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000844268,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.431292325Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.00086063,\n",
       "     'lastEvaluation': '2022-12-14T20:59:58.431280042Z'},\n",
       "    {'name': 'openshift-build.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-bcb44649-067e-47ea-ac05-f4300b55f04c.yaml',\n",
       "     'rules': [{'name': 'openshift:build_by_strategy:sum',\n",
       "       'query': 'sum by(strategy) (openshift_build_status_phase_total)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000376789,\n",
       "       'lastEvaluation': '2022-12-14T21:00:17.733024089Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000387779,\n",
       "     'lastEvaluation': '2022-12-14T21:00:17.733016294Z'},\n",
       "    {'name': 'openshift-etcd-telemetry.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-bcb44649-067e-47ea-ac05-f4300b55f04c.yaml',\n",
       "     'rules': [{'name': 'instance:etcd_mvcc_db_total_size_in_bytes:sum',\n",
       "       'query': 'sum by(instance) (etcd_mvcc_db_total_size_in_bytes{job=\"etcd\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000291768,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.9291274Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:etcd_disk_wal_fsync_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum by(instance, le) (rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=\"etcd\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000616751,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.929421011Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:etcd_network_peer_round_trip_time_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum by(instance, le) (rate(etcd_network_peer_round_trip_time_seconds_bucket{job=\"etcd\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001264861,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.930039255Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:etcd_mvcc_db_total_size_in_use_in_bytes:sum',\n",
       "       'query': 'sum by(instance) (etcd_mvcc_db_total_size_in_use_in_bytes{job=\"etcd\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000136046,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.93130622Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum by(instance, le) (rate(etcd_disk_backend_commit_duration_seconds_bucket{job=\"etcd\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000608915,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.931443869Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.002935152,\n",
       "     'lastEvaluation': '2022-12-14T21:00:03.929119906Z'},\n",
       "    {'name': 'openshift-general.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-bcb44649-067e-47ea-ac05-f4300b55f04c.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'TargetDown',\n",
       "       'query': '100 * (count by(job, namespace, service) (up == 0 unless on(node) max by(node) (kube_node_spec_unschedulable == 1)) / count by(job, namespace, service) (up unless on(node) max by(node) (kube_node_spec_unschedulable == 1))) > 10',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace have been unreachable for more than 15 minutes. This may be a symptom of network connectivity issues, down nodes, or failures within these components. Assess the health of the infrastructure and nodes running these targets and then contact support.',\n",
       "        'summary': 'Some targets were not reachable from the monitoring server for an extended period of time.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002840505,\n",
       "       'lastEvaluation': '2022-12-14T21:00:18.433299522Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.002851656,\n",
       "     'lastEvaluation': '2022-12-14T21:00:18.433291878Z'},\n",
       "    {'name': 'openshift-ingress.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-bcb44649-067e-47ea-ac05-f4300b55f04c.yaml',\n",
       "     'rules': [{'name': 'code:cluster:ingress_http_request_count:rate5m:sum',\n",
       "       'query': 'sum by(code) (rate(haproxy_server_http_responses_total[5m]) > 0)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002534339,\n",
       "       'lastEvaluation': '2022-12-14T21:00:05.819791732Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:ingress_frontend_bytes_in:rate5m:sum',\n",
       "       'query': 'sum(rate(haproxy_frontend_bytes_in_total[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00013815,\n",
       "       'lastEvaluation': '2022-12-14T21:00:05.822327734Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:ingress_frontend_bytes_out:rate5m:sum',\n",
       "       'query': 'sum(rate(haproxy_frontend_bytes_out_total[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000115348,\n",
       "       'lastEvaluation': '2022-12-14T21:00:05.822466936Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:ingress_frontend_connections:sum',\n",
       "       'query': 'sum(haproxy_frontend_current_sessions)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.6111e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:05.822583205Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:workload:ingress_request_error:fraction5m',\n",
       "       'query': 'sum(max without(service, endpoint, container, pod, job, namespace) (increase(haproxy_server_http_responses_total{code!~\"2xx|1xx|4xx|3xx\",exported_namespace!~\"openshift-.*\"}[5m]) > 0)) / sum(max without(service, endpoint, container, pod, job, namespace) (increase(haproxy_server_http_responses_total{exported_namespace!~\"openshift-.*\"}[5m]))) or absent(__does_not_exist__) * 0',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001254952,\n",
       "       'lastEvaluation': '2022-12-14T21:00:05.822680598Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:workload:ingress_request_total:irate5m',\n",
       "       'query': 'sum(max without(service, endpoint, container, pod, job, namespace) (irate(haproxy_server_http_responses_total{exported_namespace!~\"openshift-.*\"}[5m]))) or absent(__does_not_exist__) * 0',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000898131,\n",
       "       'lastEvaluation': '2022-12-14T21:00:05.823936792Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:openshift:ingress_request_error:fraction5m',\n",
       "       'query': 'sum(max without(service, endpoint, container, pod, job, namespace) (increase(haproxy_server_http_responses_total{code!~\"2xx|1xx|4xx|3xx\",exported_namespace=~\"openshift-.*\"}[5m]) > 0)) / sum(max without(service, endpoint, container, pod, job, namespace) (increase(haproxy_server_http_responses_total{exported_namespace=~\"openshift-.*\"}[5m]))) or absent(__does_not_exist__) * 0',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002302683,\n",
       "       'lastEvaluation': '2022-12-14T21:00:05.824835975Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:openshift:ingress_request_total:irate5m',\n",
       "       'query': 'sum(max without(service, endpoint, container, pod, job, namespace) (irate(haproxy_server_http_responses_total{exported_namespace=~\"openshift-.*\"}[5m]))) or absent(__does_not_exist__) * 0',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001830605,\n",
       "       'lastEvaluation': '2022-12-14T21:00:05.82713978Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:ingress_controller_aws_nlb_active:sum',\n",
       "       'query': 'sum(ingress_controller_aws_nlb_active) or vector(0)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.7856e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:05.828971728Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.009267139,\n",
       "     'lastEvaluation': '2022-12-14T21:00:05.819784068Z'},\n",
       "    {'name': 'openshift-kubernetes.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-bcb44649-067e-47ea-ac05-f4300b55f04c.yaml',\n",
       "     'rules': [{'name': 'pod:container_cpu_usage:sum',\n",
       "       'query': 'sum by(pod, namespace) (rate(container_cpu_usage_seconds_total{container=\"\",pod!=\"\"}[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00532448,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.419453212Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'pod:container_fs_usage_bytes:sum',\n",
       "       'query': 'sum by(pod, namespace) (container_fs_usage_bytes{pod!=\"\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.007307863,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.424780026Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace:container_memory_usage_bytes:sum',\n",
       "       'query': 'sum by(namespace) (container_memory_usage_bytes{container!=\"\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.006384344,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.432090123Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace:container_cpu_usage:sum',\n",
       "       'query': 'sum by(namespace) (rate(container_cpu_usage_seconds_total{container!=\"\",container!=\"POD\"}[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.005650744,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.438476611Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:memory_usage:ratio',\n",
       "       'query': 'sum by(cluster) (container_memory_usage_bytes{container=\"\",pod!=\"\"}) / sum by(cluster) (machine_memory_bytes)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.004337382,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.444129539Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:container_spec_cpu_shares:ratio',\n",
       "       'query': 'sum(container_spec_cpu_shares{container=\"\",pod!=\"\"}) / 1000 / sum(machine_cpu_cores)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00383154,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.448468143Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:container_cpu_usage:ratio',\n",
       "       'query': 'sum(rate(container_cpu_usage_seconds_total{container=\"\",pod!=\"\"}[5m])) / sum(machine_cpu_cores)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.004823406,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.452302559Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:master_nodes',\n",
       "       'query': 'max without(endpoint, instance, job, pod, service) (kube_node_labels and on(node) kube_node_role{role=\"master\"})',\n",
       "       'labels': {'label_node_role_kubernetes_io': 'master',\n",
       "        'label_node_role_kubernetes_io_master': 'true'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000188916,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.457127588Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:infra_nodes',\n",
       "       'query': 'max without(endpoint, instance, job, pod, service) (kube_node_labels and on(node) kube_node_role{role=\"infra\"})',\n",
       "       'labels': {'label_node_role_kubernetes_io_infra': 'true'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000211759,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.457317556Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:master_infra_nodes',\n",
       "       'query': 'max without(endpoint, instance, job, pod, service) (cluster:master_nodes and on(node) cluster:infra_nodes)',\n",
       "       'labels': {'label_node_role_kubernetes_io_infra': 'true',\n",
       "        'label_node_role_kubernetes_io_master': 'true'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000189757,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.45753186Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:nodes_roles',\n",
       "       'query': 'cluster:master_infra_nodes or on(node) cluster:master_nodes or on(node) cluster:infra_nodes or on(node) max without(endpoint, instance, job, pod, service) (kube_node_labels)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000299143,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.457723791Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:hyperthread_enabled_nodes',\n",
       "       'query': 'kube_node_labels and on(node) (sum by(node, package, core) (label_replace(node_cpu_info, \"node\", \"$1\", \"instance\", \"(.*)\")) == 2)',\n",
       "       'labels': {'label_node_hyperthread_enabled': 'true'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000920151,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.458025338Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:virt_platform_nodes:sum',\n",
       "       'query': 'count by(type, system_manufacturer, system_product_name, baseboard_manufacturer, baseboard_product_name) (sum by(instance, type, system_manufacturer, system_product_name, baseboard_manufacturer, baseboard_product_name) (virt_platform))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000262114,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.458947222Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:capacity_cpu_cores:sum',\n",
       "       'query': 'sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id) ((cluster:master_nodes * on(node) group_left() max by(node) (kube_node_status_capacity{resource=\"cpu\",unit=\"core\"})) or on(node) (max without(endpoint, instance, job, pod, service) (kube_node_labels) * on(node) group_left() max by(node) (kube_node_status_capacity{resource=\"cpu\",unit=\"core\"})))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000455548,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.459210989Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:cpu_core_hyperthreading',\n",
       "       'query': 'clamp_max(label_replace(sum by(instance, package, core) (node_cpu_info{core!=\"\",package!=\"\"} or label_replace(label_join(node_cpu_info{core=\"\",package=\"\"}, \"core\", \"\", \"cpu\"), \"package\", \"0\", \"package\", \"\")) > 1, \"label_node_hyperthread_enabled\", \"true\", \"instance\", \"(.*)\") or on(instance, package) label_replace(sum by(instance, package, core) (label_replace(node_cpu_info{core!=\"\",package!=\"\"} or label_join(node_cpu_info{core=\"\",package=\"\"}, \"core\", \"\", \"cpu\"), \"package\", \"0\", \"package\", \"\")) <= 1, \"label_node_hyperthread_enabled\", \"false\", \"instance\", \"(.*)\"), 1)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001152368,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.4596682Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:cpu_core_node_labels',\n",
       "       'query': 'topk by(node) (1, cluster:nodes_roles) * on(node) group_right(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_node_openshift_io_os_id, label_kubernetes_io_arch, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) label_replace(cluster:cpu_core_hyperthreading, \"node\", \"$1\", \"instance\", \"(.*)\")',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000628393,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.460821961Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:capacity_cpu_cores_hyperthread_enabled:sum',\n",
       "       'query': 'count by(label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled) (cluster:cpu_core_node_labels)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000253497,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.461451656Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:capacity_memory_bytes:sum',\n",
       "       'query': 'sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io) ((cluster:master_nodes * on(node) group_left() max by(node) (kube_node_status_capacity{resource=\"memory\",unit=\"byte\"})) or on(node) (max without(endpoint, instance, job, pod, service) (kube_node_labels) * on(node) group_left() max by(node) (kube_node_status_capacity{resource=\"memory\",unit=\"byte\"})))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000411585,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.461706736Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:cpu_usage_cores:sum',\n",
       "       'query': 'sum(1 - rate(node_cpu_seconds_total{mode=\"idle\"}[2m]) * on(namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:{pod=~\"node-exporter.+\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000736686,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.462119743Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:memory_usage_bytes:sum',\n",
       "       'query': 'sum(node_memory_MemTotal_bytes{job=\"node-exporter\"} - node_memory_MemAvailable_bytes{job=\"node-exporter\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000306767,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.462857842Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'workload:cpu_usage_cores:sum',\n",
       "       'query': 'sum(rate(container_cpu_usage_seconds_total{container=\"\",namespace!~\"openshift-.+\",pod!=\"\"}[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003597871,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.463166212Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'openshift:cpu_usage_cores:sum',\n",
       "       'query': 'cluster:cpu_usage_cores:sum - workload:cpu_usage_cores:sum',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000174108,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.466766357Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'workload:memory_usage_bytes:sum',\n",
       "       'query': 'sum(container_memory_working_set_bytes{container=\"\",namespace!~\"openshift-.+\",pod!=\"\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002440533,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.466941797Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'openshift:memory_usage_bytes:sum',\n",
       "       'query': 'cluster:memory_usage_bytes:sum - workload:memory_usage_bytes:sum',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000141346,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.469383512Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:node_instance_type_count:sum',\n",
       "       'query': 'sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id) (cluster:master_nodes or on(node) kube_node_labels)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00024388,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.469526861Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum',\n",
       "       'query': 'sum by(provisioner) (topk by(namespace, persistentvolumeclaim) (1, kube_persistentvolumeclaim_resource_requests_storage_bytes) * on(namespace, persistentvolumeclaim) group_right() topk by(namespace, persistentvolumeclaim) (1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000396145,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.469772544Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'workload:capacity_physical_cpu_cores:sum',\n",
       "       'query': '(sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_infra=\"\",label_node_role_kubernetes_io_master=\"\"} or absent(__does_not_exist__) * 0)) + ((sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master=\"true\"} or absent(__does_not_exist__) * 0) * ((max(cluster_master_schedulable == 1) * 0 + 1) or (absent(cluster_master_schedulable == 1) * 0))))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000434739,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.470170212Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:workload:capacity_physical_cpu_cores:min:5m',\n",
       "       'query': 'min_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000121088,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.470606012Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:workload:capacity_physical_cpu_cores:max:5m',\n",
       "       'query': 'max_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000105738,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.470728684Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:kubelet_volume_stats_used_bytes:provisioner:sum',\n",
       "       'query': 'sum by(provisioner) (topk by(namespace, persistentvolumeclaim) (1, kubelet_volume_stats_used_bytes) * on(namespace, persistentvolumeclaim) group_right() topk by(namespace, persistentvolumeclaim) (1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000228711,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.470835644Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:etcd_object_counts:sum',\n",
       "       'query': 'sum by(instance) (apiserver_storage_objects)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.005073938,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.471065578Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:resources:sum',\n",
       "       'query': 'topk(500, max by(resource) (apiserver_storage_objects))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.005917767,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.476141289Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:pods:terminal:workload:sum',\n",
       "       'query': 'count(count by(namespace, pod) (kube_pod_restart_policy{namespace!~\"openshift-.+\",type!=\"Always\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000210506,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.482062332Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:containers:sum',\n",
       "       'query': 'sum(max by(instance) (kubelet_containers_per_pod_count_sum))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000137559,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.482273759Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_role_os_version_machine:cpu_capacity_cores:sum',\n",
       "       'query': 'count by(label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) (cluster:cpu_core_node_labels)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000273845,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.48241236Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:capacity_cpu_sockets_hyperthread_enabled:sum',\n",
       "       'query': 'count by(label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled, label_node_role_kubernetes_io) (max by(node, package, label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled, label_node_role_kubernetes_io) (cluster:cpu_core_node_labels))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000371149,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.482687688Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_role_os_version_machine:cpu_capacity_sockets:sum',\n",
       "       'query': 'count by(label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) (max by(node, package, label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id, label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra) (cluster:cpu_core_node_labels))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000336904,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.48306058Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:alertmanager_integrations:max',\n",
       "       'query': 'max(alertmanager_integrations{namespace=\"openshift-monitoring\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000104416,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.483398917Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:kube_persistentvolume_plugin_type_counts:sum',\n",
       "       'query': 'sum by(plugin_name, volume_mode) (pv_collector_total_pv_count)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000213873,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.483504295Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:vsphere_vcenter_info:sum',\n",
       "       'query': 'sum by(version) (vsphere_vcenter_info)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000120186,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.483720512Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:vsphere_esxi_version_total:sum',\n",
       "       'query': 'sum by(version) (vsphere_esxi_version_total)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.0023e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.483841679Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:vsphere_node_hw_version_total:sum',\n",
       "       'query': 'sum by(hw_version) (vsphere_node_hw_version_total)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.0633e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.483913054Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:control_plane:all_nodes_ready',\n",
       "       'query': 'sum(min by(node) (kube_node_status_condition{condition=\"Ready\",status=\"true\"}) and max by(node) (kube_node_role{role=\"master\"})) == bool sum(kube_node_role{role=\"master\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000274016,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.483984799Z',\n",
       "       'type': 'recording'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ClusterMonitoringOperatorReconciliationErrors',\n",
       "       'query': 'max_over_time(cluster_monitoring_operator_last_reconciliation_successful[5m]) == 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Errors are occurring during reconciliation cycles. Inspect the cluster-monitoring-operator log for potential root causes.',\n",
       "        'summary': 'Cluster Monitoring Operator is experiencing unexpected reconciliation errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 8.9608e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.484260217Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'firing',\n",
       "       'name': 'AlertmanagerReceiversNotConfigured',\n",
       "       'query': 'cluster:alertmanager_integrations:max == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'namespace': 'openshift-monitoring', 'severity': 'warning'},\n",
       "       'annotations': {'description': 'Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager.',\n",
       "        'summary': 'Receivers (notification integrations) are not configured on Alertmanager'},\n",
       "       'alerts': [{'labels': {'alertname': 'AlertmanagerReceiversNotConfigured',\n",
       "          'namespace': 'openshift-monitoring',\n",
       "          'severity': 'warning'},\n",
       "         'annotations': {'description': 'Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager.',\n",
       "          'summary': 'Receivers (notification integrations) are not configured on Alertmanager'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2022-12-11T04:20:52.418422385Z',\n",
       "         'value': '0e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000354889,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.484350837Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeDeploymentReplicasMismatch',\n",
       "       'query': '(((kube_deployment_spec_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > kube_deployment_status_replicas_available{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) and (changes(kube_deployment_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[5m]) == 0)) * on() group_left() cluster:control_plane:all_nodes_ready) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes. This indicates that cluster infrastructure is unable to start or restart the necessary components. This most often occurs when one or more nodes are down or partioned from the cluster, or a fault occurs on the node that prevents the workload from starting. In rare cases this may indicate a new version of a cluster component cannot start due to a bug or configuration error. Assess the pods for this deployment to verify they are running on healthy nodes and then contact support.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeDeploymentReplicasMismatch.md',\n",
       "        'summary': 'Deployment has not matched the expected number of replicas'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002376241,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.48470775Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'MultipleContainersOOMKilled',\n",
       "       'query': 'sum(max by(namespace, container, pod) (increase(kube_pod_container_status_restarts_total[12m])) and max by(namespace, container, pod) (kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}) == 1) > 5',\n",
       "       'duration': 900,\n",
       "       'labels': {'namespace': 'kube-system', 'severity': 'info'},\n",
       "       'annotations': {'description': 'Multiple containers were out of memory killed within the past 15 minutes. There are many potential causes of OOM errors, however issues on a specific node or containers breaching their limits is common.',\n",
       "        'summary': 'Containers are being killed due to OOM'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00362446,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.487085284Z',\n",
       "       'type': 'alerting'},\n",
       "      {'name': 'cluster:usage:kube_schedulable_node_ready_reachable:avg5m',\n",
       "       'query': 'avg_over_time((((count((max by(node) (up{job=\"kubelet\",metrics_path=\"/metrics\"} == 1) and max by(node) (kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 1) and min by(node) (kube_node_spec_unschedulable == 0))) / scalar(count(min by(node) (kube_node_spec_unschedulable == 0))))))[5m:1s])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.010666362,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.490711618Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:kube_node_ready:avg5m',\n",
       "       'query': 'avg_over_time((count(max by(node) (kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 1)) / scalar(count(max by(node) (kube_node_status_condition{condition=\"Ready\",status=\"true\"}))))[5m:1s])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.004473809,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.501379884Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'kube_running_pod_ready',\n",
       "       'query': '(max without(condition, container, endpoint, instance, job, service) (((kube_pod_status_ready{condition=\"false\"} == 1) * 0 or (kube_pod_status_ready{condition=\"true\"} == 1)) * on(pod, namespace) group_left() group by(pod, namespace) (kube_pod_status_phase{phase=~\"Running|Unknown|Pending\"} == 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.011406355,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.505855686Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:openshift:kube_running_pod_ready:avg',\n",
       "       'query': 'avg(kube_running_pod_ready{namespace=~\"openshift-.*\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001898603,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.517265668Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:usage:workload:kube_running_pod_ready:avg',\n",
       "       'query': 'avg(kube_running_pod_ready{namespace!~\"openshift-.*\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000404211,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.519166014Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.100126359,\n",
       "     'lastEvaluation': '2022-12-14T21:00:22.419445878Z'},\n",
       "    {'name': 'openshift-monitoring.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-bcb44649-067e-47ea-ac05-f4300b55f04c.yaml',\n",
       "     'rules': [{'name': 'openshift:prometheus_tsdb_head_series:sum',\n",
       "       'query': 'sum by(job, namespace) (max without(instance) (prometheus_tsdb_head_series{namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000392919,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.192000066Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'openshift:prometheus_tsdb_head_samples_appended_total:sum',\n",
       "       'query': 'sum by(job, namespace) (max without(instance) (rate(prometheus_tsdb_head_samples_appended_total{namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[2m])))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000173055,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.192394899Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'monitoring:container_memory_working_set_bytes:sum',\n",
       "       'query': 'sum by(namespace) (max without(instance) (container_memory_working_set_bytes{container=\"\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000805325,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.192569056Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_job:scrape_series_added:topk3_sum1h',\n",
       "       'query': 'topk(3, sum by(namespace, job) (sum_over_time(scrape_series_added[1h])))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.0023492,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.193375695Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_job:scrape_samples_post_metric_relabeling:topk3',\n",
       "       'query': 'topk(3, max by(namespace, job) (topk by(namespace, job) (1, scrape_samples_post_metric_relabeling)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001440431,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.195726558Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'monitoring:haproxy_server_http_responses_total:sum',\n",
       "       'query': 'sum by(exported_service) (rate(haproxy_server_http_responses_total{exported_namespace=\"openshift-monitoring\",exported_service=~\"alertmanager-main|prometheus-k8s\"}[5m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000895805,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.197168422Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_workload_pod:kube_pod_owner:relabel',\n",
       "       'query': 'max by(cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"ReplicationController\"}, \"replicationcontroller\", \"$1\", \"owner_name\", \"(.*)\") * on(replicationcontroller, namespace) group_left(owner_name) topk by(replicationcontroller, namespace) (1, max by(replicationcontroller, namespace, owner_name) (kube_replicationcontroller_owner{job=\"kube-state-metrics\"})), \"workload\", \"$1\", \"owner_name\", \"(.*)\"))',\n",
       "       'labels': {'workload_type': 'deploymentconfig'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000469123,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.19806582Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.006561978,\n",
       "     'lastEvaluation': '2022-12-14T21:00:22.191977674Z'},\n",
       "    {'name': 'openshift-sre.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-cluster-monitoring-operator-prometheus-rules-bcb44649-067e-47ea-ac05-f4300b55f04c.yaml',\n",
       "     'rules': [{'name': 'code:apiserver_request_total:rate:sum',\n",
       "       'query': 'sum by(code) (rate(apiserver_request_total{job=\"apiserver\"}[10m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.0329416,\n",
       "       'lastEvaluation': '2022-12-14T21:00:18.452934482Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.032950547,\n",
       "     'lastEvaluation': '2022-12-14T21:00:18.452930414Z'},\n",
       "    {'name': 'kube-state-metrics',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kube-state-metrics-rules-d74c0aa2-ee53-4fa7-9f12-6cd0d7428787.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeStateMetricsListErrors',\n",
       "       'query': '(sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\"}[5m]))) > 0.01',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.',\n",
       "        'summary': 'kube-state-metrics is experiencing errors in list operations.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000656085,\n",
       "       'lastEvaluation': '2022-12-14T21:00:18.07671301Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeStateMetricsWatchErrors',\n",
       "       'query': '(sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) / sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\"}[5m]))) > 0.01',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.',\n",
       "        'summary': 'kube-state-metrics is experiencing errors in watch operations.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000477679,\n",
       "       'lastEvaluation': '2022-12-14T21:00:18.077370608Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001177135,\n",
       "     'lastEvaluation': '2022-12-14T21:00:18.076674117Z'},\n",
       "    {'name': 'k8s.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-409c21eb-e7a4-4fde-bcff-171d4bd9380b.yaml',\n",
       "     'rules': [{'name': 'node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate',\n",
       "       'query': 'sum by(cluster, namespace, pod, container) (irate(container_cpu_usage_seconds_total{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"}[5m])) * on(cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=\"\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.010155121,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.818061236Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_namespace_pod_container:container_memory_working_set_bytes',\n",
       "       'query': 'container_memory_working_set_bytes{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace, pod, node) (kube_pod_info{node!=\"\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.009162442,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.828219993Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_namespace_pod_container:container_memory_rss',\n",
       "       'query': 'container_memory_rss{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace, pod, node) (kube_pod_info{node!=\"\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.009292297,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.837385811Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_namespace_pod_container:container_memory_cache',\n",
       "       'query': 'container_memory_cache{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace, pod, node) (kube_pod_info{node!=\"\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.009009674,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.846682306Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_namespace_pod_container:container_memory_swap',\n",
       "       'query': 'container_memory_swap{image!=\"\",job=\"kubelet\",metrics_path=\"/metrics/cadvisor\"} * on(namespace, pod) group_left(node) topk by(namespace, pod) (1, max by(namespace, pod, node) (kube_pod_info{node!=\"\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.008694051,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.855696909Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:namespace:pod_memory:active:kube_pod_container_resource_requests',\n",
       "       'query': 'kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"memory\"} * on(namespace, pod, cluster) group_left() max by(namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.008540923,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.864395659Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_memory:kube_pod_container_resource_requests:sum',\n",
       "       'query': 'sum by(namespace, cluster) (sum by(namespace, pod, cluster) (max by(namespace, pod, container, cluster) (kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"memory\"}) * on(namespace, pod, cluster) group_left() max by(namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00703541,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.872940539Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests',\n",
       "       'query': 'kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"cpu\"} * on(namespace, pod, cluster) group_left() max by(namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.007638193,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.879977923Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_cpu:kube_pod_container_resource_requests:sum',\n",
       "       'query': 'sum by(namespace, cluster) (sum by(namespace, pod, cluster) (max by(namespace, pod, container, cluster) (kube_pod_container_resource_requests{job=\"kube-state-metrics\",resource=\"cpu\"}) * on(namespace, pod, cluster) group_left() max by(namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.007367305,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.887620023Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:namespace:pod_memory:active:kube_pod_container_resource_limits',\n",
       "       'query': 'kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"memory\"} * on(namespace, pod, cluster) group_left() max by(namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00389557,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.894989392Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_memory:kube_pod_container_resource_limits:sum',\n",
       "       'query': 'sum by(namespace, cluster) (sum by(namespace, pod, cluster) (max by(namespace, pod, container, cluster) (kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"memory\"}) * on(namespace, pod, cluster) group_left() max by(namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003796385,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.898886374Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits',\n",
       "       'query': 'kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"cpu\"} * on(namespace, pod, cluster) group_left() max by(namespace, pod, cluster) ((kube_pod_status_phase{phase=~\"Pending|Running\"} == 1))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003763633,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.902684642Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_cpu:kube_pod_container_resource_limits:sum',\n",
       "       'query': 'sum by(namespace, cluster) (sum by(namespace, pod, cluster) (max by(namespace, pod, container, cluster) (kube_pod_container_resource_limits{job=\"kube-state-metrics\",resource=\"cpu\"}) * on(namespace, pod, cluster) group_left() max by(namespace, pod, cluster) (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00398007,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.906450699Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_workload_pod:kube_pod_owner:relabel',\n",
       "       'query': 'max by(cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"ReplicaSet\"}, \"replicaset\", \"$1\", \"owner_name\", \"(.*)\") * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (1, max by(replicaset, namespace, owner_name) (kube_replicaset_owner{job=\"kube-state-metrics\"})), \"workload\", \"$1\", \"owner_name\", \"(.*)\"))',\n",
       "       'labels': {'workload_type': 'deployment'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002702667,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.910432021Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_workload_pod:kube_pod_owner:relabel',\n",
       "       'query': 'max by(cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"DaemonSet\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))',\n",
       "       'labels': {'workload_type': 'daemonset'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001043784,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.913136922Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_workload_pod:kube_pod_owner:relabel',\n",
       "       'query': 'max by(cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"StatefulSet\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))',\n",
       "       'labels': {'workload_type': 'statefulset'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000342465,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.914181998Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'namespace_workload_pod:kube_pod_owner:relabel',\n",
       "       'query': 'max by(cluster, namespace, workload, pod) (label_replace(kube_pod_owner{job=\"kube-state-metrics\",owner_kind=\"Job\"}, \"workload\", \"$1\", \"owner_name\", \"(.*)\"))',\n",
       "       'labels': {'workload_type': 'job'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00049956,\n",
       "       'lastEvaluation': '2022-12-14T21:00:12.914527358Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.096976542,\n",
       "     'lastEvaluation': '2022-12-14T21:00:12.81805253Z'},\n",
       "    {'name': 'kube-scheduler.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-409c21eb-e7a4-4fde-bcff-171d4bd9380b.yaml',\n",
       "     'rules': [{'name': 'cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum without(instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000238519,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.787000071Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum without(instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000525679,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.787240153Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum without(instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 8.6813e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.787767385Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.9, sum without(instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.9'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.6454e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.78785528Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.9, sum without(instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.9'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000449687,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.787932585Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.9, sum without(instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.9'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 6.4612e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.788383344Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.5, sum without(instance, pod) (rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.5'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 5.4873e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.788448647Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.5, sum without(instance, pod) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.5'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000437363,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.788504182Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.5, sum without(instance, pod) (rate(scheduler_binding_duration_seconds_bucket{job=\"scheduler\"}[5m])))',\n",
       "       'labels': {'quantile': '0.5'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 6.6404e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.788942938Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.002016735,\n",
       "     'lastEvaluation': '2022-12-14T21:00:03.786994501Z'},\n",
       "    {'name': 'kubelet.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-409c21eb-e7a4-4fde-bcff-171d4bd9380b.yaml',\n",
       "     'rules': [{'name': 'node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.99, sum by(cluster, instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on(cluster, instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})',\n",
       "       'labels': {'quantile': '0.99'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001476499,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.035568502Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.9, sum by(cluster, instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on(cluster, instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})',\n",
       "       'labels': {'quantile': '0.9'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000967721,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.037046854Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile',\n",
       "       'query': 'histogram_quantile(0.5, sum by(cluster, instance, le) (rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) * on(cluster, instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"})',\n",
       "       'labels': {'quantile': '0.5'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000950018,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.038015818Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.003403696,\n",
       "     'lastEvaluation': '2022-12-14T21:00:04.035564134Z'},\n",
       "    {'name': 'kubernetes-apps',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-409c21eb-e7a4-4fde-bcff-171d4bd9380b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubePodCrashLooping',\n",
       "       'query': 'max_over_time(kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",reason=\"CrashLoopBackOff\"}[5m]) >= 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: \"CrashLoopBackOff\").',\n",
       "        'summary': 'Pod is crash looping.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000416164,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.11528886Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'firing',\n",
       "       'name': 'KubePodNotReady',\n",
       "       'query': 'sum by(namespace, pod, cluster) (max by(namespace, pod, cluster) (kube_pod_status_phase{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",phase=~\"Pending|Unknown\"}) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!=\"Job\"}))) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePodNotReady.md',\n",
       "        'summary': 'Pod has been in a non-ready state for more than 15 minutes.'},\n",
       "       'alerts': [{'labels': {'alertname': 'KubePodNotReady',\n",
       "          'namespace': 'openshift-storage',\n",
       "          'pod': 'noobaa-core-0',\n",
       "          'severity': 'warning'},\n",
       "         'annotations': {'description': 'Pod openshift-storage/noobaa-core-0 has been in a non-ready state for longer than 15 minutes.',\n",
       "          'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePodNotReady.md',\n",
       "          'summary': 'Pod has been in a non-ready state for more than 15 minutes.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2022-12-14T11:19:23.114812544Z',\n",
       "         'value': '1e+00'},\n",
       "        {'labels': {'alertname': 'KubePodNotReady',\n",
       "          'namespace': 'openshift-storage',\n",
       "          'pod': 'noobaa-db-pg-0',\n",
       "          'severity': 'warning'},\n",
       "         'annotations': {'description': 'Pod openshift-storage/noobaa-db-pg-0 has been in a non-ready state for longer than 15 minutes.',\n",
       "          'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePodNotReady.md',\n",
       "          'summary': 'Pod has been in a non-ready state for more than 15 minutes.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2022-12-14T11:19:23.114812544Z',\n",
       "         'value': '1e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.007638454,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.115706397Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeDeploymentGenerationMismatch',\n",
       "       'query': 'kube_deployment_status_observed_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_deployment_metadata_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.',\n",
       "        'summary': 'Deployment generation mismatch due to possible roll-back'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001323521,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.123347345Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'firing',\n",
       "       'name': 'KubeStatefulSetReplicasMismatch',\n",
       "       'query': '(kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_statefulset_status_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) and (changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[10m]) == 0)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.',\n",
       "        'summary': 'Deployment has not matched the expected number of replicas.'},\n",
       "       'alerts': [{'labels': {'alertname': 'KubeStatefulSetReplicasMismatch',\n",
       "          'container': 'kube-rbac-proxy-main',\n",
       "          'endpoint': 'https-main',\n",
       "          'job': 'kube-state-metrics',\n",
       "          'namespace': 'openshift-storage',\n",
       "          'service': 'kube-state-metrics',\n",
       "          'severity': 'warning',\n",
       "          'statefulset': 'noobaa-core'},\n",
       "         'annotations': {'description': 'StatefulSet openshift-storage/noobaa-core has not matched the expected number of replicas for longer than 15 minutes.',\n",
       "          'summary': 'Deployment has not matched the expected number of replicas.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2022-12-14T11:19:23.114812544Z',\n",
       "         'value': '0e+00'},\n",
       "        {'labels': {'alertname': 'KubeStatefulSetReplicasMismatch',\n",
       "          'container': 'kube-rbac-proxy-main',\n",
       "          'endpoint': 'https-main',\n",
       "          'job': 'kube-state-metrics',\n",
       "          'namespace': 'openshift-storage',\n",
       "          'service': 'kube-state-metrics',\n",
       "          'severity': 'warning',\n",
       "          'statefulset': 'noobaa-db-pg'},\n",
       "         'annotations': {'description': 'StatefulSet openshift-storage/noobaa-db-pg has not matched the expected number of replicas for longer than 15 minutes.',\n",
       "          'summary': 'Deployment has not matched the expected number of replicas.'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2022-12-14T11:19:23.114812544Z',\n",
       "         'value': '0e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00088198,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.124672158Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeStatefulSetGenerationMismatch',\n",
       "       'query': 'kube_statefulset_status_observed_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_statefulset_metadata_generation{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.',\n",
       "        'summary': 'StatefulSet generation mismatch due to possible roll-back'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000358275,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.125555821Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeStatefulSetUpdateNotRolledOut',\n",
       "       'query': '(max without(revision) (kube_statefulset_status_current_revision{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} unless kube_statefulset_status_update_revision{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) * (kube_statefulset_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"})) and (changes(kube_statefulset_status_replicas_updated{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[5m]) == 0)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.',\n",
       "        'summary': 'StatefulSet update has not been rolled out.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00080276,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.125915368Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeDaemonSetRolloutStuck',\n",
       "       'query': '((kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) or (kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != 0) or (kube_daemonset_status_updated_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) or (kube_daemonset_status_number_available{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"})) and (changes(kube_daemonset_status_updated_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[5m]) == 0)',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 30 minutes.',\n",
       "        'summary': 'DaemonSet rollout is stuck.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003402383,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.12671904Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeContainerWaiting',\n",
       "       'query': 'sum by(namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour.',\n",
       "        'summary': 'Pod container waiting longer than 1 hour'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000198814,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.130122835Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeDaemonSetNotScheduled',\n",
       "       'query': 'kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} - kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.',\n",
       "        'summary': 'DaemonSet pods are not scheduled.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000520801,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.130322541Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeDaemonSetMisScheduled',\n",
       "       'query': 'kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.',\n",
       "        'summary': 'DaemonSet pods are misscheduled.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000246513,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.130844304Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeJobNotCompleted',\n",
       "       'query': 'time() - max by(namespace, job_name, cluster) (kube_job_status_start_time{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} and kube_job_status_active{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0) > 43200',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than {{ \"43200\" | humanizeDuration }} to complete.',\n",
       "        'summary': 'Job did not complete in time'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000693284,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.131091669Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeJobFailed',\n",
       "       'query': 'kube_job_failed{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeJobFailed.md',\n",
       "        'summary': 'Job failed to complete.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000142438,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.131785815Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeHpaReplicasMismatch',\n",
       "       'query': '(kube_horizontalpodautoscaler_status_desired_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} != kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) and (kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > kube_horizontalpodautoscaler_spec_min_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) and (kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} < kube_horizontalpodautoscaler_spec_max_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) and changes(kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[15m]) == 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes.',\n",
       "        'summary': 'HPA has not matched descired number of replicas.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001188005,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.131929114Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeHpaMaxedOut',\n",
       "       'query': 'kube_horizontalpodautoscaler_status_current_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} == kube_horizontalpodautoscaler_spec_max_replicas{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes.',\n",
       "        'summary': 'HPA is running at max replicas'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000292851,\n",
       "       'lastEvaluation': '2022-12-14T21:00:23.133119224Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.018132612,\n",
       "     'lastEvaluation': '2022-12-14T21:00:23.115281607Z'},\n",
       "    {'name': 'kubernetes-resources',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-409c21eb-e7a4-4fde-bcff-171d4bd9380b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeCPUOvercommit',\n",
       "       'query': 'sum(namespace_cpu:kube_pod_container_resource_requests:sum) - (sum(kube_node_status_allocatable{resource=\"cpu\"}) - max(kube_node_status_allocatable{resource=\"cpu\"})) > 0 and (sum(kube_node_status_allocatable{resource=\"cpu\"}) - max(kube_node_status_allocatable{resource=\"cpu\"})) > 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'namespace': 'kube-system', 'severity': 'warning'},\n",
       "       'annotations': {'description': 'Cluster has overcommitted CPU resource requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.',\n",
       "        'summary': 'Cluster has overcommitted CPU resource requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000862202,\n",
       "       'lastEvaluation': '2022-12-14T20:59:55.091123985Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeMemoryOvercommit',\n",
       "       'query': 'sum(namespace_memory:kube_pod_container_resource_requests:sum) - (sum(kube_node_status_allocatable{resource=\"memory\"}) - max(kube_node_status_allocatable{resource=\"memory\"})) > 0 and (sum(kube_node_status_allocatable{resource=\"memory\"}) - max(kube_node_status_allocatable{resource=\"memory\"})) > 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'namespace': 'kube-system', 'severity': 'warning'},\n",
       "       'annotations': {'description': 'Cluster has overcommitted memory resource requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node failure.',\n",
       "        'summary': 'Cluster has overcommitted memory resource requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000577958,\n",
       "       'lastEvaluation': '2022-12-14T20:59:55.09198764Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeCPUQuotaOvercommit',\n",
       "       'query': 'sum(min without(resource) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",resource=~\"(cpu|requests.cpu)\",type=\"hard\"})) / sum(kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"cpu\"}) > 1.5',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Cluster has overcommitted CPU resource requests for Namespaces.',\n",
       "        'summary': 'Cluster has overcommitted CPU resource requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000608856,\n",
       "       'lastEvaluation': '2022-12-14T20:59:55.092566489Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeMemoryQuotaOvercommit',\n",
       "       'query': 'sum(min without(resource) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",resource=~\"(memory|requests.memory)\",type=\"hard\"})) / sum(kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"memory\"}) > 1.5',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Cluster has overcommitted memory resource requests for Namespaces.',\n",
       "        'summary': 'Cluster has overcommitted memory resource requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000378352,\n",
       "       'lastEvaluation': '2022-12-14T20:59:55.093176287Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeQuotaAlmostFull',\n",
       "       'query': 'kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",type=\"used\"} / ignoring(instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",type=\"hard\"} > 0) > 0.9 < 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.',\n",
       "        'summary': 'Namespace quota is going to be full.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000537031,\n",
       "       'lastEvaluation': '2022-12-14T20:59:55.09355543Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeQuotaFullyUsed',\n",
       "       'query': 'kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",type=\"used\"} / ignoring(instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",type=\"hard\"} > 0) == 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.',\n",
       "        'summary': 'Namespace quota is fully used.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000446901,\n",
       "       'lastEvaluation': '2022-12-14T20:59:55.094093422Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeQuotaExceeded',\n",
       "       'query': 'kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",type=\"used\"} / ignoring(instance, job, type) (kube_resourcequota{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",type=\"hard\"} > 0) > 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.',\n",
       "        'summary': 'Namespace quota has exceeded the limits.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000464765,\n",
       "       'lastEvaluation': '2022-12-14T20:59:55.094541045Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.003893848,\n",
       "     'lastEvaluation': '2022-12-14T20:59:55.091115308Z'},\n",
       "    {'name': 'kubernetes-storage',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-409c21eb-e7a4-4fde-bcff-171d4bd9380b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubePersistentVolumeFillingUp',\n",
       "       'query': '(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) < 0.03 and kubelet_volume_stats_used_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0 unless on(namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1 unless on(namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_alerts_k8s_io_kube_persistent_volume_filling_up=\"disabled\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1',\n",
       "       'duration': 60,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md',\n",
       "        'summary': 'PersistentVolume is filling up.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001324482,\n",
       "       'lastEvaluation': '2022-12-14T21:00:10.507818312Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubePersistentVolumeFillingUp',\n",
       "       'query': '(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} / kubelet_volume_stats_capacity_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) < 0.15 and kubelet_volume_stats_used_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0 and predict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[6h], 4 * 24 * 3600) < 0 unless on(namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1 unless on(namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_alerts_k8s_io_kube_persistent_volume_filling_up=\"disabled\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md',\n",
       "        'summary': 'PersistentVolume is filling up.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.004609434,\n",
       "       'lastEvaluation': '2022-12-14T21:00:10.509144387Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubePersistentVolumeInodesFillingUp',\n",
       "       'query': '(kubelet_volume_stats_inodes_free{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} / kubelet_volume_stats_inodes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) < 0.03 and kubelet_volume_stats_inodes_used{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0 unless on(namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1 unless on(namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_alerts_k8s_io_kube_persistent_volume_filling_up=\"disabled\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1',\n",
       "       'duration': 60,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} only has {{ $value | humanizePercentage }} free inodes.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md',\n",
       "        'summary': 'PersistentVolumeInodes are filling up.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000759158,\n",
       "       'lastEvaluation': '2022-12-14T21:00:10.513756166Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubePersistentVolumeInodesFillingUp',\n",
       "       'query': '(kubelet_volume_stats_inodes_free{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} / kubelet_volume_stats_inodes{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}) < 0.15 and kubelet_volume_stats_inodes_used{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"} > 0 and predict_linear(kubelet_volume_stats_inodes_free{job=\"kubelet\",metrics_path=\"/metrics\",namespace=~\"(openshift-.*|kube-.*|default)\"}[6h], 4 * 24 * 3600) < 0 unless on(namespace, persistentvolumeclaim) kube_persistentvolumeclaim_access_mode{access_mode=\"ReadOnlyMany\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1 unless on(namespace, persistentvolumeclaim) kube_persistentvolumeclaim_labels{label_alerts_k8s_io_kube_persistent_volume_filling_up=\"disabled\",namespace=~\"(openshift-.*|kube-.*|default)\"} == 1',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to run out of inodes within four days. Currently {{ $value | humanizePercentage }} of its inodes are free.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md',\n",
       "        'summary': 'PersistentVolumeInodes are filling up.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003543909,\n",
       "       'lastEvaluation': '2022-12-14T21:00:10.514516416Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubePersistentVolumeErrors',\n",
       "       'query': 'kube_persistentvolume_status_phase{job=\"kube-state-metrics\",namespace=~\"(openshift-.*|kube-.*|default)\",phase=~\"Failed|Pending\"} > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.',\n",
       "        'summary': 'PersistentVolume is having issues with provisioning.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000284685,\n",
       "       'lastEvaluation': '2022-12-14T21:00:10.518061638Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.010537358,\n",
       "     'lastEvaluation': '2022-12-14T21:00:10.507810697Z'},\n",
       "    {'name': 'kubernetes-system',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-409c21eb-e7a4-4fde-bcff-171d4bd9380b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeClientErrors',\n",
       "       'query': '(sum by(cluster, instance, job, namespace) (rate(rest_client_requests_total{code=~\"5..\"}[5m])) / sum by(cluster, instance, job, namespace) (rate(rest_client_requests_total[5m]))) > 0.01',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'\",\n",
       "        'summary': 'Kubernetes API server client is experiencing errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00421362,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.887699221Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.004229119,\n",
       "     'lastEvaluation': '2022-12-14T20:59:58.887688481Z'},\n",
       "    {'name': 'kubernetes-system-apiserver',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-409c21eb-e7a4-4fde-bcff-171d4bd9380b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeAggregatedAPIErrors',\n",
       "       'query': 'sum by(name, namespace, cluster) (increase(aggregator_unavailable_apiservice_total[10m])) > 4',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. It has appeared unavailable {{ $value | humanize }} times averaged over the past 10m.',\n",
       "        'summary': 'Kubernetes aggregated API has reported errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000350048,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.746657747Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeAggregatedAPIDown',\n",
       "       'query': '(1 - max by(name, namespace, cluster) (avg_over_time(aggregator_unavailable_apiservice[10m]))) * 100 < 85',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only {{ $value | humanize }}% available over the last 10m.',\n",
       "        'summary': 'Kubernetes aggregated API is down.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003503683,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.747009699Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeAPIDown',\n",
       "       'query': 'absent(up{job=\"apiserver\"} == 1)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'KubeAPI has disappeared from Prometheus target discovery.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeAPIDown.md',\n",
       "        'summary': 'Target disappeared from Prometheus target discovery.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00028683,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.750516608Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeAPITerminatedRequests',\n",
       "       'query': 'sum(rate(apiserver_request_terminations_total{job=\"apiserver\"}[10m])) / (sum(rate(apiserver_request_total{job=\"apiserver\"}[10m])) + sum(rate(apiserver_request_terminations_total{job=\"apiserver\"}[10m]))) > 0.2',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.',\n",
       "        'summary': 'The kubernetes apiserver has terminated {{ $value | humanizePercentage }} of its incoming requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.031852741,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.75080436Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.036021105,\n",
       "     'lastEvaluation': '2022-12-14T20:59:58.74664338Z'},\n",
       "    {'name': 'kubernetes-system-kubelet',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-409c21eb-e7a4-4fde-bcff-171d4bd9380b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'KubeNodeNotReady',\n",
       "       'query': 'kube_node_status_condition{condition=\"Ready\",job=\"kube-state-metrics\",status=\"true\"} == 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $labels.node }} has been unready for more than 15 minutes.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeNodeNotReady.md',\n",
       "        'summary': 'Node is not ready.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000887209,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.034117361Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeNodeUnreachable',\n",
       "       'query': '(kube_node_spec_taint{effect=\"NoSchedule\",job=\"kube-state-metrics\",key=\"node.kubernetes.io/unreachable\"} unless ignoring(key, value) kube_node_spec_taint{job=\"kube-state-metrics\",key=~\"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn\"}) == 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $labels.node }} is unreachable and some workloads may be rescheduled.',\n",
       "        'summary': 'Node is unreachable.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000177715,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.035005892Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeletTooManyPods',\n",
       "       'query': 'count by(cluster, node) ((kube_pod_status_phase{job=\"kube-state-metrics\",phase=\"Running\"} == 1) * on(instance, pod, namespace, cluster) group_left(node) topk by(instance, pod, namespace, cluster) (1, kube_pod_info{job=\"kube-state-metrics\"})) / max by(cluster, node) (kube_node_status_capacity{job=\"kube-state-metrics\",resource=\"pods\"} != 1) > 0.95',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': \"Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity.\",\n",
       "        'summary': 'Kubelet is running at capacity.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00504259,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.035184388Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeNodeReadinessFlapping',\n",
       "       'query': 'sum by(cluster, node) (changes(kube_node_status_condition{condition=\"Ready\",status=\"true\"}[15m])) > 2',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes.',\n",
       "        'summary': 'Node readiness status is flapping.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000130796,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.04022826Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeletPlegDurationHigh',\n",
       "       'query': 'node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile=\"0.99\"} >= 10',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}.',\n",
       "        'summary': 'Kubelet Pod Lifecycle Event Generator is taking too long to relist.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 8.9729e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.040359907Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeletPodStartUpLatencyHigh',\n",
       "       'query': 'histogram_quantile(0.99, sum by(cluster, instance, le) (rate(kubelet_pod_worker_duration_seconds_bucket{job=\"kubelet\",metrics_path=\"/metrics\"}[5m]))) * on(cluster, instance) group_left(node) kubelet_node_name{job=\"kubelet\",metrics_path=\"/metrics\"} > 60',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}.',\n",
       "        'summary': 'Kubelet Pod startup latency is too high.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001946523,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.040450368Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeletClientCertificateRenewalErrors',\n",
       "       'query': 'increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $value | humanize }} errors in the last 5 minutes).',\n",
       "        'summary': 'Kubelet has failed to renew its client certificate.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000100809,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.042397803Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeletServerCertificateRenewalErrors',\n",
       "       'query': 'increase(kubelet_server_expiration_renew_errors[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $value | humanize }} errors in the last 5 minutes).',\n",
       "        'summary': 'Kubelet has failed to renew its server certificate.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.0751e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.042499303Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'KubeletDown',\n",
       "       'query': 'absent(up{job=\"kubelet\",metrics_path=\"/metrics\"} == 1)',\n",
       "       'duration': 900,\n",
       "       'labels': {'namespace': 'kube-system', 'severity': 'critical'},\n",
       "       'annotations': {'description': 'Kubelet has disappeared from Prometheus target discovery.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeletDown.md',\n",
       "        'summary': 'Target disappeared from Prometheus target discovery.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000124033,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.042590806Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.008605083,\n",
       "     'lastEvaluation': '2022-12-14T21:00:03.034111349Z'},\n",
       "    {'name': 'node.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-kubernetes-monitoring-rules-409c21eb-e7a4-4fde-bcff-171d4bd9380b.yaml',\n",
       "     'rules': [{'name': 'node_namespace_pod:kube_pod_info:',\n",
       "       'query': 'topk by(cluster, namespace, pod) (1, max by(cluster, node, namespace, pod) (label_replace(kube_pod_info{job=\"kube-state-metrics\",node!=\"\"}, \"pod\", \"$1\", \"pod\", \"(.*)\")))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.005361881,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.045121691Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': ':node_memory_MemAvailable_bytes:sum',\n",
       "       'query': 'sum by(cluster) (node_memory_MemAvailable_bytes{job=\"node-exporter\"} or (node_memory_Buffers_bytes{job=\"node-exporter\"} + node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_MemFree_bytes{job=\"node-exporter\"} + node_memory_Slab_bytes{job=\"node-exporter\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000645335,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.050491907Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:node_cpu:ratio_rate5m',\n",
       "       'query': 'sum(rate(node_cpu_seconds_total{job=\"node-exporter\",mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[5m])) / count(sum by(cluster, instance, cpu) (node_cpu_seconds_total{job=\"node-exporter\"}))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002878917,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.051138624Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.008909015,\n",
       "     'lastEvaluation': '2022-12-14T20:59:58.045111913Z'},\n",
       "    {'name': 'node-exporter',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-node-exporter-rules-c32f5df6-f499-4e68-82ff-6aca7875f0a2.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'NodeFilesystemSpaceFillingUp',\n",
       "       'query': '(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 15 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md',\n",
       "        'summary': 'Filesystem is predicted to run out of space within the next 24 hours.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.011598907,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.80012541Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemSpaceFillingUp',\n",
       "       'query': '(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 10 and predict_linear(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md',\n",
       "        'summary': 'Filesystem is predicted to run out of space within the next 4 hours.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.011380086,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.811727573Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemAlmostOutOfSpace',\n",
       "       'query': '(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 5 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md',\n",
       "        'summary': 'Filesystem has less than 5% space left.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001682115,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.823109953Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemAlmostOutOfSpace',\n",
       "       'query': '(node_filesystem_avail_bytes{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_size_bytes{fstype!=\"\",job=\"node-exporter\"} * 100 < 3 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)',\n",
       "       'duration': 1800,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md',\n",
       "        'summary': 'Filesystem has less than 3% space left.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00239054,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.824793891Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemFilesFillingUp',\n",
       "       'query': '(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 40 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"}[6h], 24 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md',\n",
       "        'summary': 'Filesystem is predicted to run out of inodes within the next 24 hours.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.012256714,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.827187065Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemFilesFillingUp',\n",
       "       'query': '(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 20 and predict_linear(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md',\n",
       "        'summary': 'Filesystem is predicted to run out of inodes within the next 4 hours.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.011418618,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.839447617Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemAlmostOutOfFiles',\n",
       "       'query': '(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 5 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md',\n",
       "        'summary': 'Filesystem has less than 5% inodes left.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001759271,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.850867938Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFilesystemAlmostOutOfFiles',\n",
       "       'query': '(node_filesystem_files_free{fstype!=\"\",job=\"node-exporter\"} / node_filesystem_files{fstype!=\"\",job=\"node-exporter\"} * 100 < 3 and node_filesystem_readonly{fstype!=\"\",job=\"node-exporter\"} == 0)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md',\n",
       "        'summary': 'Filesystem has less than 3% inodes left.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001732161,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.852628491Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeNetworkReceiveErrs',\n",
       "       'query': 'rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.',\n",
       "        'summary': 'Network interface is reporting many receive errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000903048,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.854361703Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeNetworkTransmitErrs',\n",
       "       'query': 'rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.',\n",
       "        'summary': 'Network interface is reporting many transmit errors.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000876961,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.855265664Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeHighNumberConntrackEntriesUsed',\n",
       "       'query': '(node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $value | humanizePercentage }} of conntrack entries are used.',\n",
       "        'summary': 'Number of conntrack are getting close to the limit.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000443213,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.856143617Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeTextFileCollectorScrapeError',\n",
       "       'query': 'node_textfile_scrape_error{job=\"node-exporter\"} == 1',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Node Exporter text file collector failed to scrape.',\n",
       "        'summary': 'Node Exporter text file collector failed to scrape.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000242427,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.856591409Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeClockSkewDetected',\n",
       "       'query': '(node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0)',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Clock on {{ $labels.instance }} is out of sync by more than 300s. Ensure NTP is configured correctly on this host.',\n",
       "        'summary': 'Clock skew detected.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000490063,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.856834797Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeClockNotSynchronising',\n",
       "       'query': 'min_over_time(node_timex_sync_status[5m]) == 0 and node_timex_maxerror_seconds >= 16',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Clock on {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeClockNotSynchronising.md',\n",
       "        'summary': 'Clock not synchronising.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000272852,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.857325841Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeRAIDDegraded',\n",
       "       'query': 'node_md_disks_required - ignoring(state) (node_md_disks{state=\"active\"}) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': \"RAID array '{{ $labels.device }}' on {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.\",\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeRAIDDegraded.md',\n",
       "        'summary': 'RAID Array is degraded'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000126428,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.857600077Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeRAIDDiskFailure',\n",
       "       'query': 'node_md_disks{state=\"failed\"} > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"At least one device in RAID array on {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.\",\n",
       "        'summary': 'Failed device in RAID array'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.8348e-05,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.857727928Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFileDescriptorLimit',\n",
       "       'query': '(node_filefd_allocated{job=\"node-exporter\"} * 100 / node_filefd_maximum{job=\"node-exporter\"} > 70)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'File descriptors limit at {{ $labels.instance }} is currently at {{ printf \"%.2f\" $value }}%.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md',\n",
       "        'summary': 'Kernel is predicted to exhaust file descriptors limit soon.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000346582,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.857806997Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NodeFileDescriptorLimit',\n",
       "       'query': '(node_filefd_allocated{job=\"node-exporter\"} * 100 / node_filefd_maximum{job=\"node-exporter\"} > 90)',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'File descriptors limit at {{ $labels.instance }} is currently at {{ printf \"%.2f\" $value }}%.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md',\n",
       "        'summary': 'Kernel is predicted to exhaust file descriptors limit soon.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00033998,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.858155002Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.058379651,\n",
       "     'lastEvaluation': '2022-12-14T20:59:56.800118507Z'},\n",
       "    {'name': 'node-exporter.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-node-exporter-rules-c32f5df6-f499-4e68-82ff-6aca7875f0a2.yaml',\n",
       "     'rules': [{'name': 'instance:node_num_cpu:sum',\n",
       "       'query': 'count without(cpu, mode) (node_cpu_seconds_total{job=\"node-exporter\",mode=\"idle\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00062193,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.706702544Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_cpu_utilisation:rate1m',\n",
       "       'query': '1 - avg without(cpu) (sum without(mode) (rate(node_cpu_seconds_total{job=\"node-exporter\",mode=~\"idle|iowait|steal\"}[1m])))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001062048,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.707326479Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_load1_per_cpu:ratio',\n",
       "       'query': '(node_load1{job=\"node-exporter\"} / instance:node_num_cpu:sum{job=\"node-exporter\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00022886,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.70839001Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_memory_utilisation:ratio',\n",
       "       'query': '1 - ((node_memory_MemAvailable_bytes{job=\"node-exporter\"} or (node_memory_Buffers_bytes{job=\"node-exporter\"} + node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_MemFree_bytes{job=\"node-exporter\"} + node_memory_Slab_bytes{job=\"node-exporter\"})) / node_memory_MemTotal_bytes{job=\"node-exporter\"})',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000603827,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.708619872Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_vmstat_pgmajfault:rate1m',\n",
       "       'query': 'rate(node_vmstat_pgmajfault{job=\"node-exporter\"}[1m])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000162817,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.70922471Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance_device:node_disk_io_time_seconds:rate1m',\n",
       "       'query': 'rate(node_disk_io_time_seconds_total{device=~\"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\",job=\"node-exporter\"}[1m])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00021833,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.709388569Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance_device:node_disk_io_time_weighted_seconds:rate1m',\n",
       "       'query': 'rate(node_disk_io_time_weighted_seconds_total{device=~\"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\",job=\"node-exporter\"}[1m])',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000207552,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.709607931Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_network_receive_bytes_excluding_lo:rate1m',\n",
       "       'query': 'sum without(device) (rate(node_network_receive_bytes_total{device!=\"lo\",job=\"node-exporter\"}[1m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000538753,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.709816605Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_network_transmit_bytes_excluding_lo:rate1m',\n",
       "       'query': 'sum without(device) (rate(node_network_transmit_bytes_total{device!=\"lo\",job=\"node-exporter\"}[1m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000508287,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.71035648Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_network_receive_drop_excluding_lo:rate1m',\n",
       "       'query': 'sum without(device) (rate(node_network_receive_drop_total{device!=\"lo\",job=\"node-exporter\"}[1m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00046745,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.710866009Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'instance:node_network_transmit_drop_excluding_lo:rate1m',\n",
       "       'query': 'sum without(device) (rate(node_network_transmit_drop_total{device!=\"lo\",job=\"node-exporter\"}[1m]))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000457201,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.711334551Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.005099276,\n",
       "     'lastEvaluation': '2022-12-14T21:00:22.70669463Z'},\n",
       "    {'name': 'prometheus',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-prometheus-rules-0958093c-f47f-4e28-a3b2-038c33e93303.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'PrometheusBadConfig',\n",
       "       'query': 'max_over_time(prometheus_config_last_reload_successful{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.',\n",
       "        'summary': 'Failed Prometheus configuration reload.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000399522,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.507827143Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusNotificationQueueRunningFull',\n",
       "       'query': '(predict_linear(prometheus_notifications_queue_length{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m], 60 * 30) > min_over_time(prometheus_notifications_queue_capacity{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]))',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.',\n",
       "        'summary': 'Prometheus alert notification queue predicted to run full in less than 30m.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000412566,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.508228238Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusErrorSendingAlertsToSomeAlertmanagers',\n",
       "       'query': '(rate(prometheus_notifications_errors_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) / rate(prometheus_notifications_sent_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])) * 100 > 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ printf \"%.1f\" $value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.',\n",
       "        'summary': 'Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000396958,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.508642307Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusNotConnectedToAlertmanagers',\n",
       "       'query': 'max_over_time(prometheus_notifications_alertmanagers_discovered{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) < 1',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.',\n",
       "        'summary': 'Prometheus is not connected to any Alertmanagers.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000189246,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.509040537Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusTSDBReloadsFailing',\n",
       "       'query': 'increase(prometheus_tsdb_reloads_failures_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[3h]) > 0',\n",
       "       'duration': 14400,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.',\n",
       "        'summary': 'Prometheus has issues reloading blocks from disk.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000257263,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.509230765Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusTSDBCompactionsFailing',\n",
       "       'query': 'increase(prometheus_tsdb_compactions_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[3h]) > 0',\n",
       "       'duration': 14400,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.',\n",
       "        'summary': 'Prometheus has issues compacting blocks.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000205547,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.509489191Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusNotIngestingSamples',\n",
       "       'query': '(rate(prometheus_tsdb_head_samples_appended_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) <= 0 and (sum without(scrape_job) (prometheus_target_metadata_cache_entries{job=~\"prometheus-k8s|prometheus-user-workload\"}) > 0 or sum without(rule_group) (prometheus_rule_group_rules{job=~\"prometheus-k8s|prometheus-user-workload\"}) > 0))',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.',\n",
       "        'summary': 'Prometheus is not ingesting samples.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003026726,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.509696Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusDuplicateTimestamps',\n",
       "       'query': 'rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with different values but duplicated timestamp.',\n",
       "        'summary': 'Prometheus is dropping samples with duplicate timestamps.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00017559,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.512727375Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOutOfOrderTimestamps',\n",
       "       'query': 'rate(prometheus_target_scrapes_sample_out_of_order_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with timestamps arriving out of order.',\n",
       "        'summary': 'Prometheus drops samples with out-of-order timestamps.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00014855,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.512903987Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusRemoteStorageFailures',\n",
       "       'query': '((rate(prometheus_remote_storage_failed_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])) / ((rate(prometheus_remote_storage_failed_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])) + (rate(prometheus_remote_storage_succeeded_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) or rate(prometheus_remote_storage_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])))) * 100 > 1',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf \"%.1f\" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}',\n",
       "        'summary': 'Prometheus fails to send samples to remote storage.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000585371,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.513053528Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusRemoteWriteBehind',\n",
       "       'query': '(max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) - ignoring(remote_name, url) group_right() max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])) > 120',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'info'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf \"%.1f\" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.',\n",
       "        'summary': 'Prometheus remote write is behind.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00027049,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.513643037Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusRemoteWriteDesiredShards',\n",
       "       'query': '(max_over_time(prometheus_remote_storage_shards_desired{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > max_over_time(prometheus_remote_storage_shards_max{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]))',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance=\"%s\",job=~\"prometheus-k8s|prometheus-user-workload\"}` $labels.instance | query | first | value }}.',\n",
       "        'summary': 'Prometheus remote write desired shards calculation wants to run more than configured max shards.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000222298,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.513914669Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusRuleFailures',\n",
       "       'query': 'increase(prometheus_rule_evaluation_failures_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf \"%.0f\" $value }} rules in the last 5m.',\n",
       "        'summary': 'Prometheus is failing rule evaluations.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001713204,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.514137999Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusMissingRuleEvaluations',\n",
       "       'query': 'increase(prometheus_rule_group_iterations_missed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf \"%.0f\" $value }} rule group evaluations in the last 5m.',\n",
       "        'summary': 'Prometheus is missing rule evaluations due to slow rule group evaluation.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001917628,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.515852496Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusTargetLimitHit',\n",
       "       'query': 'increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf \"%.0f\" $value }} targets because the number of targets exceeded the configured target_limit.',\n",
       "        'summary': 'Prometheus has dropped targets because some scrape configs have exceeded the targets limit.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000158618,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.517771296Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusLabelLimitHit',\n",
       "       'query': 'increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf \"%.0f\" $value }} targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.',\n",
       "        'summary': 'Prometheus has dropped targets because some scrape configs have exceeded the labels limit.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000173548,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.517930876Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusScrapeBodySizeLimitHit',\n",
       "       'query': 'increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf \"%.0f\" $value }} scrapes in the last 5m because some targets exceeded the configured body_size_limit.',\n",
       "        'summary': 'Prometheus has dropped some targets that exceeded body size limit.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000337555,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.518105486Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusScrapeSampleLimitHit',\n",
       "       'query': 'increase(prometheus_target_scrapes_exceeded_sample_limit_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) > 0',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf \"%.0f\" $value }} scrapes in the last 5m because some targets exceeded the configured sample_limit.',\n",
       "        'summary': 'Prometheus has failed scrapes that have exceeded the configured sample limit.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000150132,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.518444824Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusTargetSyncFailure',\n",
       "       'query': 'increase(prometheus_target_sync_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[30m]) > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': '{{ printf \"%.0f\" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}} have failed to sync because invalid configuration was supplied.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusTargetSyncFailure.md',\n",
       "        'summary': 'Prometheus has failed to sync targets.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001759861,\n",
       "       'lastEvaluation': '2022-12-14T21:00:00.518596009Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.012539077,\n",
       "     'lastEvaluation': '2022-12-14T21:00:00.507818857Z'},\n",
       "    {'name': 'thanos-sidecar',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-k8s-thanos-sidecar-rules-9f386d85-3ef0-4b39-8706-aaccba4fd969.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ThanosSidecarBucketOperationsFailed',\n",
       "       'query': 'sum by(namespace, job, instance) (rate(thanos_objstore_bucket_operation_failures_total{job=~\"prometheus-(k8s|user-workload)-thanos-sidecar\"}[5m])) > 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}} bucket operations are failing',\n",
       "        'summary': 'Thanos Sidecar bucket operations are failing'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00048429,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.681274678Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ThanosSidecarNoConnectionToStartedPrometheus',\n",
       "       'query': 'thanos_sidecar_prometheus_up{job=~\"prometheus-(k8s|user-workload)-thanos-sidecar\"} == 0 and on(namespace, pod) prometheus_tsdb_data_replay_duration_seconds != 0',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}} is unhealthy.',\n",
       "        'summary': 'Thanos Sidecar cannot access Prometheus, even though Prometheus seems healthy and has reloaded WAL.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000399853,\n",
       "       'lastEvaluation': '2022-12-14T20:59:58.681761323Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000907338,\n",
       "     'lastEvaluation': '2022-12-14T20:59:58.68126041Z'},\n",
       "    {'name': 'config-reloaders',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-operator-rules-af8bfcad-6276-4c2c-804f-2d09910894a8.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ConfigReloaderSidecarErrors',\n",
       "       'query': 'max_over_time(reloader_last_reload_successful{namespace=~\".+\"}[5m]) == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.\\nAs a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.',\n",
       "        'summary': 'config-reloader sidecar has not had a successful reload for 10m'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000426743,\n",
       "       'lastEvaluation': '2022-12-14T21:00:04.447265703Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000456769,\n",
       "     'lastEvaluation': '2022-12-14T21:00:04.447239033Z'},\n",
       "    {'name': 'prometheus-operator',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-prometheus-operator-rules-af8bfcad-6276-4c2c-804f-2d09910894a8.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorListErrors',\n",
       "       'query': '(sum by(controller, namespace) (rate(prometheus_operator_list_operations_failed_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[10m])) / sum by(controller, namespace) (rate(prometheus_operator_list_operations_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[10m]))) > 0.4',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.',\n",
       "        'summary': 'Errors while performing list operations in controller.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00086102,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.168449604Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorWatchErrors',\n",
       "       'query': '(sum by(controller, namespace) (rate(prometheus_operator_watch_operations_failed_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m])) / sum by(controller, namespace) (rate(prometheus_operator_watch_operations_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]))) > 0.4',\n",
       "       'duration': 900,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Errors while performing watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.',\n",
       "        'summary': 'Errors while performing watch operations in controller.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000466338,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.169314371Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorSyncFailed',\n",
       "       'query': 'min_over_time(prometheus_operator_syncs{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\",status=\"failed\"}[5m]) > 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $value }} objects.',\n",
       "        'summary': 'Last controller reconciliation failed'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000143059,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.169782051Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorReconcileErrors',\n",
       "       'query': '(sum by(controller, namespace) (rate(prometheus_operator_reconcile_errors_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]))) / (sum by(controller, namespace) (rate(prometheus_operator_reconcile_operations_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]))) > 0.1',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': '{{ $value | humanizePercentage }} of reconciling operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.',\n",
       "        'summary': 'Errors while reconciling controller.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000282341,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.169926102Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorNodeLookupErrors',\n",
       "       'query': 'rate(prometheus_operator_node_address_lookup_errors_total{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]) > 0.1',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.',\n",
       "        'summary': 'Errors while reconciling Prometheus.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000120547,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.170209766Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorNotReady',\n",
       "       'query': 'min by(controller, namespace) (max_over_time(prometheus_operator_ready{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]) == 0)',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"Prometheus operator in {{ $labels.namespace }} namespace isn't ready to reconcile {{ $labels.controller }} resources.\",\n",
       "        'summary': 'Prometheus operator not ready'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000164951,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.170331124Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PrometheusOperatorRejectedResources',\n",
       "       'query': 'min_over_time(prometheus_operator_managed_resources{job=\"prometheus-operator\",namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\",state=\"rejected\"}[5m]) > 0',\n",
       "       'duration': 300,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf \"%0.0f\" $value }} {{ $labels.controller }}/{{ $labels.resource }} resources.',\n",
       "        'summary': 'Resources rejected by Prometheus operator'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00014314,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.170496916Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.002208676,\n",
       "     'lastEvaluation': '2022-12-14T20:59:59.168435047Z'},\n",
       "    {'name': 'telemeter.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-telemetry-df32625c-2784-46fa-b4d8-dea312038f16.yaml',\n",
       "     'rules': [{'name': 'cluster:telemetry_selected_series:count',\n",
       "       'query': 'max(federate_samples - federate_filtered_samples)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000234231,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.073779888Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000242457,\n",
       "     'lastEvaluation': '2022-12-14T21:00:03.073773917Z'},\n",
       "    {'name': 'thanos-query',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-monitoring-thanos-querier-79b9c548-01b9-4402-810d-139c04bb2109.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ThanosQueryHttpRequestQueryErrorRateHigh',\n",
       "       'query': '(sum by(namespace, job) (rate(http_requests_total{code=~\"5..\",handler=\"query\",job=\"thanos-querier\"}[5m])) / sum by(namespace, job) (rate(http_requests_total{handler=\"query\",job=\"thanos-querier\"}[5m]))) * 100 > 5',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to handle {{$value | humanize}}% of \"query\" requests.',\n",
       "        'summary': 'Thanos Query is failing to handle requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000725004,\n",
       "       'lastEvaluation': '2022-12-14T20:59:57.562408511Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ThanosQueryHttpRequestQueryRangeErrorRateHigh',\n",
       "       'query': '(sum by(namespace, job) (rate(http_requests_total{code=~\"5..\",handler=\"query_range\",job=\"thanos-querier\"}[5m])) / sum by(namespace, job) (rate(http_requests_total{handler=\"query_range\",job=\"thanos-querier\"}[5m]))) * 100 > 5',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to handle {{$value | humanize}}% of \"query_range\" requests.',\n",
       "        'summary': 'Thanos Query is failing to handle requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000357183,\n",
       "       'lastEvaluation': '2022-12-14T20:59:57.56313629Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ThanosQueryGrpcServerErrorRate',\n",
       "       'query': '(sum by(namespace, job) (rate(grpc_server_handled_total{grpc_code=~\"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\",job=\"thanos-querier\"}[5m])) / sum by(namespace, job) (rate(grpc_server_started_total{job=\"thanos-querier\"}[5m])) * 100 > 5)',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to handle {{$value | humanize}}% of requests.',\n",
       "        'summary': 'Thanos Query is failing to handle requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001911887,\n",
       "       'lastEvaluation': '2022-12-14T20:59:57.563495336Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ThanosQueryGrpcClientErrorRate',\n",
       "       'query': '(sum by(namespace, job) (rate(grpc_client_handled_total{grpc_code!=\"OK\",job=\"thanos-querier\"}[5m])) / sum by(namespace, job) (rate(grpc_client_started_total{job=\"thanos-querier\"}[5m]))) * 100 > 5',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to send {{$value | humanize}}% of requests.',\n",
       "        'summary': 'Thanos Query is failing to send requests.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000381338,\n",
       "       'lastEvaluation': '2022-12-14T20:59:57.565408326Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ThanosQueryHighDNSFailures',\n",
       "       'query': '(sum by(namespace, job) (rate(thanos_query_store_apis_dns_failures_total{job=\"thanos-querier\"}[5m])) / sum by(namespace, job) (rate(thanos_query_store_apis_dns_lookups_total{job=\"thanos-querier\"}[5m]))) * 100 > 1',\n",
       "       'duration': 3600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Thanos Query {{$labels.job}} in {{$labels.namespace}} have {{$value | humanize}}% of failing DNS queries for store endpoints.',\n",
       "        'summary': 'Thanos Query is having high number of DNS failures.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00021284,\n",
       "       'lastEvaluation': '2022-12-14T20:59:57.565790766Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.003607569,\n",
       "     'lastEvaluation': '2022-12-14T20:59:57.562398342Z'},\n",
       "    {'name': 'multus-admission-controller-monitor-service.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-multus-prometheus-k8s-rules-e27e045e-c087-4bbe-88d0-595defba599d.yaml',\n",
       "     'rules': [{'name': 'cluster:network_attachment_definition_enabled_instance_up:max',\n",
       "       'query': 'max by(networks) (network_attachment_definition_enabled_instance_up)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000354607,\n",
       "       'lastEvaluation': '2022-12-14T21:00:06.757684669Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:network_attachment_definition_instances:max',\n",
       "       'query': 'max by(networks) (network_attachment_definition_instances)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000129975,\n",
       "       'lastEvaluation': '2022-12-14T21:00:06.75804109Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000495703,\n",
       "     'lastEvaluation': '2022-12-14T21:00:06.757677806Z'},\n",
       "    {'name': 'olm.csv_abnormal.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-operator-lifecycle-manager-olm-alert-rules-8794285b-483b-481d-b241-1376dcbd5850.yaml',\n",
       "     'rules': [{'state': 'firing',\n",
       "       'name': 'CsvAbnormalFailedOver2Min',\n",
       "       'query': 'csv_abnormal{phase=~\"^Failed$\"}',\n",
       "       'duration': 120,\n",
       "       'labels': {'namespace': '{{ $labels.namespace }}',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'Fires whenever a CSV has been in the failed phase for more than 2 minutes.',\n",
       "        'message': 'Failed to install Operator {{ $labels.name }} version {{ $labels.version }}. Reason-{{ $labels.reason }}',\n",
       "        'summary': 'CSV failed for over 2 minutes'},\n",
       "       'alerts': [{'labels': {'alertname': 'CsvAbnormalFailedOver2Min',\n",
       "          'container': 'olm-operator',\n",
       "          'endpoint': 'https-metrics',\n",
       "          'exported_namespace': 'ack-system',\n",
       "          'instance': '10.130.0.23:8443',\n",
       "          'job': 'olm-operator-metrics',\n",
       "          'name': 'ack-ecr-controller.v0.1.7',\n",
       "          'namespace': 'openshift-operator-lifecycle-manager',\n",
       "          'phase': 'Failed',\n",
       "          'pod': 'olm-operator-77bfb67b99-nx5cq',\n",
       "          'reason': 'InstallCheckFailed',\n",
       "          'service': 'olm-operator-metrics',\n",
       "          'severity': 'warning',\n",
       "          'version': '0.1.7'},\n",
       "         'annotations': {'description': 'Fires whenever a CSV has been in the failed phase for more than 2 minutes.',\n",
       "          'message': 'Failed to install Operator ack-ecr-controller.v0.1.7 version 0.1.7. Reason-InstallCheckFailed',\n",
       "          'summary': 'CSV failed for over 2 minutes'},\n",
       "         'state': 'firing',\n",
       "         'activeAt': '2022-12-13T10:02:56.878259014Z',\n",
       "         'value': '1e+00'}],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000623132,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.879593455Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'CsvAbnormalOver30Min',\n",
       "       'query': 'csv_abnormal{phase=~\"(^Replacing$|^Pending$|^Deleting$|^Unknown$)\"}',\n",
       "       'duration': 1800,\n",
       "       'labels': {'namespace': '{{ $labels.namespace }}',\n",
       "        'severity': 'warning'},\n",
       "       'annotations': {'description': 'Fires whenever a CSV is in the Replacing, Pending, Deleting, or Unkown phase for more than 30 minutes.',\n",
       "        'message': 'Failed to install Operator {{ $labels.name }} version {{ $labels.version }}. Phase-{{ $labels.phase }} Reason-{{ $labels.reason }}',\n",
       "        'summary': 'CSV abnormal for over 30 minutes'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000126948,\n",
       "       'lastEvaluation': '2022-12-14T20:59:56.880218772Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000764098,\n",
       "     'lastEvaluation': '2022-12-14T20:59:56.879584428Z'},\n",
       "    {'name': 'olm.installplan.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-operator-lifecycle-manager-olm-alert-rules-8794285b-483b-481d-b241-1376dcbd5850.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'InstallPlanStepAppliedWithWarnings',\n",
       "       'query': 'sum(increase(installplan_warnings_total[5m])) > 0',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Fires whenever the API server returns a warning when attempting to modify an operator.',\n",
       "        'message': 'The API server returned a warning during installation or upgrade of an operator. An Event with reason \"AppliedWithWarnings\" has been created with complete details, including a reference to the InstallPlan step that generated the warning.',\n",
       "        'summary': 'API returned a warning when modifying an operator'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000272754,\n",
       "       'lastEvaluation': '2022-12-14T21:00:21.07591357Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000296729,\n",
       "     'lastEvaluation': '2022-12-14T21:00:21.07589208Z'},\n",
       "    {'name': 'cluster-network-operator-master.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-ovn-kubernetes-master-rules-0965a1cc-b154-4189-9073-ff1533bc7c80.yaml',\n",
       "     'rules': [{'name': 'cluster:ovnkube_master_egress_routing_via_host:max',\n",
       "       'query': 'max(ovnkube_master_egress_routing_via_host)',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000264868,\n",
       "       'lastEvaluation': '2022-12-14T21:00:20.554872244Z',\n",
       "       'type': 'recording'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NoRunningOvnMaster',\n",
       "       'query': 'absent(up{job=\"ovnkube-master\",namespace=\"openshift-ovn-kubernetes\"} == 1)',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Networking control plane is degraded. Networking configuration updates applied to the cluster will not be\\nimplemented while there are no OVN Kubernetes pods.\\n',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NoRunningOvnMaster.md',\n",
       "        'summary': 'There is no running ovn-kubernetes master.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000131628,\n",
       "       'lastEvaluation': '2022-12-14T21:00:20.555139316Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NoOvnMasterLeader',\n",
       "       'query': 'max(ovnkube_master_leader) == 0',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Networking control plane is degraded. Networking configuration updates applied to the cluster will not be\\nimplemented while there is no OVN Kubernetes leader. Existing workloads should continue to have connectivity.\\nOVN-Kubernetes control plane is not functional.\\n',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NoOvnMasterLeader.md',\n",
       "        'summary': 'There is no ovn-kubernetes master leader.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.8318e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:20.555272026Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'NorthboundStale',\n",
       "       'query': 'time() - max(ovnkube_master_nb_e2e_timestamp) > 120',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Networking control plane is degraded. Networking configuration updates applied to the cluster will not be\\nimplemented. Existing workloads should continue to have connectivity. OVN-Kubernetes control plane and/or\\nOVN northbound database may not be functional.\\n',\n",
       "        'summary': 'ovn-kubernetes has not written anything to the northbound database for too long.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.2045e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:20.555351135Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'SouthboundStale',\n",
       "       'query': 'max(ovnkube_master_nb_e2e_timestamp) - max(ovnkube_master_sb_e2e_timestamp) > 120',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': \"Networking control plane is degraded. Networking configuration updates may not be applied to the cluster or\\ntaking a long time to apply. This usually means there is a large load on OVN component 'northd' or it is not\\nfunctioning.\\n\",\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/SouthboundStaleAlert.md',\n",
       "        'summary': 'ovn-northd has not successfully synced any changes to the southbound DB for too long.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.1442e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:20.555423872Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'V4SubnetAllocationThresholdExceeded',\n",
       "       'query': 'ovnkube_master_allocated_v4_host_subnets / ovnkube_master_num_v4_host_subnets * 100 > 80',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'More than 80% of IPv4 subnets are used. Insufficient IPv4 subnets could degrade provisioning of workloads.',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/V4SubnetAllocationThresholdExceeded.md',\n",
       "        'summary': 'More than 80% of v4 subnets available to assign to the nodes are allocated. Current v4 subnet allocation percentage is {{ $value }}.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 9.2483e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:20.555516066Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'V6SubnetAllocationThresholdExceeded',\n",
       "       'query': 'ovnkube_master_allocated_v6_host_subnets / ovnkube_master_num_v6_host_subnets * 100 > 80',\n",
       "       'duration': 600,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'More than 80% of IPv6 subnets are used. Insufficient IPv6 subnets could degrade provisioning of workloads.',\n",
       "        'summary': 'More than 80% of the v6 subnets available to assign to the nodes are allocated. Current v6 subnet allocation percentage is {{ $value }}.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.9521e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:20.55560928Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000826996,\n",
       "     'lastEvaluation': '2022-12-14T21:00:20.554863548Z'},\n",
       "    {'name': 'cluster-network-operator-ovn.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-ovn-kubernetes-networking-rules-b1ff35a6-1c89-4d0f-939c-3dc419532e0d.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'NodeWithoutOVNKubeNodePodRunning',\n",
       "       'query': '(kube_node_info unless on(node) (kube_pod_info{namespace=\"openshift-ovn-kubernetes\",pod=~\"ovnkube-node.*\"} or kube_node_labels{label_kubernetes_io_os=\"windows\"})) > 0',\n",
       "       'duration': 1200,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'Networking is degraded on nodes that do not have a functioning ovnkube-node pod. Existing workloads on the\\nnode may continue to have connectivity but any additional workloads will not be provisioned on the node. Any\\nnetwork policy changes will not be implemented on existing workloads on the node.\\n',\n",
       "        'runbook_url': 'https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NodeWithoutOVNKubeNodePodRunning.md',\n",
       "        'summary': 'All Linux nodes should be running an ovnkube-node pod, {{ $labels.node }} is not.'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.00063806,\n",
       "       'lastEvaluation': '2022-12-14T21:00:22.22815883Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000650344,\n",
       "     'lastEvaluation': '2022-12-14T21:00:22.228150815Z'},\n",
       "    {'name': 'ODF_standardized_metrics.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-storage-ocs-prometheus-rules-cb2ed59e-2ac1-4edc-a021-93b9ded4f24b.yaml',\n",
       "     'rules': [{'name': 'odf_system_health_status',\n",
       "       'query': 'ceph_health_status',\n",
       "       'labels': {'system_type': 'OCS', 'system_vendor': 'Red Hat'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000267983,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.120330101Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'odf_system_raw_capacity_total_bytes',\n",
       "       'query': 'ceph_cluster_total_bytes',\n",
       "       'labels': {'system_type': 'OCS', 'system_vendor': 'Red Hat'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 5.7589e-05,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.120600249Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'odf_system_raw_capacity_used_bytes',\n",
       "       'query': 'ceph_cluster_total_used_raw_bytes',\n",
       "       'labels': {'system_type': 'OCS', 'system_vendor': 'Red Hat'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 4.9893e-05,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.120658509Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'odf_system_iops_total_bytes',\n",
       "       'query': 'sum by(namespace, managedBy, job, service) (rate(ceph_pool_wr[1m]) + rate(ceph_pool_rd[1m]))',\n",
       "       'labels': {'system_type': 'OCS', 'system_vendor': 'Red Hat'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000405443,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.120709265Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'odf_system_throughput_total_bytes',\n",
       "       'query': 'sum by(namespace, managedBy, job, service) (rate(ceph_pool_wr_bytes[1m]) + rate(ceph_pool_rd_bytes[1m]))',\n",
       "       'labels': {'system_type': 'OCS', 'system_vendor': 'Red Hat'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000295095,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.12111582Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'odf_system_latency_seconds',\n",
       "       'query': 'sum by(namespace, managedBy, job, service) (topk by(ceph_daemon) (1, label_replace(label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\"}, \"instance\", \"$1\", \"exported_instance\", \"(.*)\"), \"device\", \"$1\", \"device\", \"/dev/(.*)\")) * on(instance, device) group_left() topk by(instance, device) (1, ((rate(node_disk_read_time_seconds_total[1m]) / (clamp_min(rate(node_disk_reads_completed_total[1m]), 1))) + (rate(node_disk_write_time_seconds_total[1m]) / (clamp_min(rate(node_disk_writes_completed_total[1m]), 1))))))',\n",
       "       'labels': {'system_type': 'OCS', 'system_vendor': 'Red Hat'},\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.003286765,\n",
       "       'lastEvaluation': '2022-12-14T20:59:59.121412598Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.004382687,\n",
       "     'lastEvaluation': '2022-12-14T20:59:59.120320053Z'},\n",
       "    {'name': 'external-cluster-services-alert.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-storage-ocs-prometheus-rules-cb2ed59e-2ac1-4edc-a021-93b9ded4f24b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ClusterObjectStoreState',\n",
       "       'query': 'ocs_rgw_health_status{job=\"ocs-metrics-exporter\"} > 1',\n",
       "       'duration': 15,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'Cluster Object Store is in unhealthy state for more than 15s. Please check Ceph cluster health or RGW connection.',\n",
       "        'message': 'Cluster Object Store is in unhealthy state. Please check Ceph cluster health or RGW connection.',\n",
       "        'severity_level': 'error',\n",
       "        'storage_type': 'RGW'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000267532,\n",
       "       'lastEvaluation': '2022-12-14T21:00:06.991961897Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000290526,\n",
       "     'lastEvaluation': '2022-12-14T21:00:06.991941359Z'},\n",
       "    {'name': 'ocs_performance.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-storage-ocs-prometheus-rules-cb2ed59e-2ac1-4edc-a021-93b9ded4f24b.yaml',\n",
       "     'rules': [{'name': 'cluster:ceph_disk_latency_read:join_ceph_node_disk_rate1m',\n",
       "       'query': 'sum by(namespace, managedBy) (topk by(ceph_daemon) (1, label_replace(label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\"}, \"instance\", \"$1\", \"exported_instance\", \"(.*)\"), \"device\", \"$1\", \"device\", \"/dev/(.*)\")) * on(instance, device) group_left() topk by(instance, device) (1, (rate(node_disk_read_time_seconds_total[1m]) / (clamp_min(rate(node_disk_reads_completed_total[1m]), 1)))))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.002572962,\n",
       "       'lastEvaluation': '2022-12-14T21:00:11.213952179Z',\n",
       "       'type': 'recording'},\n",
       "      {'name': 'cluster:ceph_disk_latency_write:join_ceph_node_disk_rate1m',\n",
       "       'query': 'sum by(namespace, managedBy) (topk by(ceph_daemon) (1, label_replace(label_replace(ceph_disk_occupation{job=\"rook-ceph-mgr\"}, \"instance\", \"$1\", \"exported_instance\", \"(.*)\"), \"device\", \"$1\", \"device\", \"/dev/(.*)\")) * on(instance, device) group_left() topk by(instance, device) (1, (rate(node_disk_write_time_seconds_total[1m]) / (clamp_min(rate(node_disk_writes_completed_total[1m]), 1)))))',\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.001734424,\n",
       "       'lastEvaluation': '2022-12-14T21:00:11.216528638Z',\n",
       "       'type': 'recording'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.004327094,\n",
       "     'lastEvaluation': '2022-12-14T21:00:11.213939114Z'},\n",
       "    {'name': 'odf-obc-quota-alert.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-storage-ocs-prometheus-rules-cb2ed59e-2ac1-4edc-a021-93b9ded4f24b.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'ObcQuotaBytesAlert',\n",
       "       'query': '(ocs_objectbucketclaim_info * on(namespace, objectbucket) group_left() (ocs_objectbucket_used_bytes / ocs_objectbucket_max_bytes)) > 0.8',\n",
       "       'duration': 10,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'ObjectBucketClaim {{$labels.objectbucketclaim}} has crossed 80% of the size limit set by the quota(bytes) and will become read-only on reaching the quota limit. Increase the quota in the {{$labels.objectbucketclaim}} OBC custom resource.',\n",
       "        'message': 'OBC has crossed 80% of the quota(bytes).',\n",
       "        'severity_level': 'warning',\n",
       "        'storage_type': 'RGW'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000205376,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.099115045Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ObcQuotaObjectsAlert',\n",
       "       'query': '(ocs_objectbucketclaim_info * on(namespace, objectbucket) group_left() (ocs_objectbucket_objects_total / ocs_objectbucket_max_objects)) > 0.8',\n",
       "       'duration': 10,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'ObjectBucketClaim {{$labels.objectbucketclaim}} has crossed 80% of the size limit set by the quota(objects) and will become read-only on reaching the quota limit. Increase the quota in the {{$labels.objectbucketclaim}} OBC custom resource.',\n",
       "        'message': 'OBC has crossed 80% of the quota(object).',\n",
       "        'severity_level': 'warning',\n",
       "        'storage_type': 'RGW'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.7486e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.099321623Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ObcQuotaBytesExhausedAlert',\n",
       "       'query': '(ocs_objectbucketclaim_info * on(namespace, objectbucket) group_left() (ocs_objectbucket_used_bytes / ocs_objectbucket_max_bytes)) >= 1',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'ObjectBucketClaim {{$labels.objectbucketclaim}} has crossed the limit set by the quota(bytes) and will be read-only now. Increase the quota in the {{$labels.objectbucketclaim}} OBC custom resource immediately.',\n",
       "        'message': 'OBC reached quota(bytes) limit.',\n",
       "        'severity_level': 'error',\n",
       "        'storage_type': 'RGW'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.2796e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.099399861Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'ObcQuotaObjectsExhausedAlert',\n",
       "       'query': '(ocs_objectbucketclaim_info * on(namespace, objectbucket) group_left() (ocs_objectbucket_objects_total / ocs_objectbucket_max_objects)) >= 1',\n",
       "       'duration': 0,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'ObjectBucketClaim {{$labels.objectbucketclaim}} has crossed the limit set by the quota(objects) and will be read-only now. Increase the quota in the {{$labels.objectbucketclaim}} OBC custom resource immediately.',\n",
       "        'message': 'OBC reached quota(object) limit.',\n",
       "        'severity_level': 'error',\n",
       "        'storage_type': 'RGW'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 7.0692e-05,\n",
       "       'lastEvaluation': '2022-12-14T21:00:03.099473349Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.000435168,\n",
       "     'lastEvaluation': '2022-12-14T21:00:03.099110456Z'},\n",
       "    {'name': 'persistent-volume-alert.rules',\n",
       "     'file': '/etc/prometheus/rules/prometheus-k8s-rulefiles-0/openshift-storage-prometheus-ceph-rules-external-2182a5c8-30bc-4998-b1cc-fa137cded9f7.yaml',\n",
       "     'rules': [{'state': 'inactive',\n",
       "       'name': 'PersistentVolumeUsageNearFull',\n",
       "       'query': '(kubelet_volume_stats_used_bytes * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) kube_storageclass_info{provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) / (kubelet_volume_stats_capacity_bytes * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) kube_storageclass_info{provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) > 0.75',\n",
       "       'duration': 5,\n",
       "       'labels': {'severity': 'warning'},\n",
       "       'annotations': {'description': 'PVC {{ $labels.persistentvolumeclaim }} utilization has crossed 75%. Free up some space or expand the PVC.',\n",
       "        'message': 'PVC {{ $labels.persistentvolumeclaim }} is nearing full. Data deletion or PVC expansion is required.',\n",
       "        'severity_level': 'warning',\n",
       "        'storage_type': 'ceph'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000731868,\n",
       "       'lastEvaluation': '2022-12-14T21:00:17.298041609Z',\n",
       "       'type': 'alerting'},\n",
       "      {'state': 'inactive',\n",
       "       'name': 'PersistentVolumeUsageCritical',\n",
       "       'query': '(kubelet_volume_stats_used_bytes * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) kube_storageclass_info{provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) / (kubelet_volume_stats_capacity_bytes * on(namespace, persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) kube_storageclass_info{provisioner=~\"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)\"})) > 0.85',\n",
       "       'duration': 5,\n",
       "       'labels': {'severity': 'critical'},\n",
       "       'annotations': {'description': 'PVC {{ $labels.persistentvolumeclaim }} utilization has crossed 85%. Free up some space or expand the PVC immediately.',\n",
       "        'message': 'PVC {{ $labels.persistentvolumeclaim }} is critically full. Data deletion or PVC expansion is required.',\n",
       "        'severity_level': 'error',\n",
       "        'storage_type': 'ceph'},\n",
       "       'alerts': [],\n",
       "       'health': 'ok',\n",
       "       'evaluationTime': 0.000458373,\n",
       "       'lastEvaluation': '2022-12-14T21:00:17.298781562Z',\n",
       "       'type': 'alerting'}],\n",
       "     'interval': 30,\n",
       "     'limit': 0,\n",
       "     'evaluationTime': 0.001209046,\n",
       "     'lastEvaluation': '2022-12-14T21:00:17.298033664Z'}]}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "r = requests.get(\n",
    "    f\"https://{prometheus_host}/v1/rules\",\n",
    "    verify=False,\n",
    "    headers=kubeConfig.api_key,\n",
    ")\n",
    "r.status_code, r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " {'status': 'success',\n",
       "  'data': ['From',\n",
       "   'Local',\n",
       "   'Remote',\n",
       "   'To',\n",
       "   '__name__',\n",
       "   'access_mode',\n",
       "   'action',\n",
       "   'addr',\n",
       "   'address',\n",
       "   'alertmanager',\n",
       "   'alertname',\n",
       "   'alertstate',\n",
       "   'apiserver',\n",
       "   'apiservice',\n",
       "   'approval',\n",
       "   'attempts',\n",
       "   'backend',\n",
       "   'bios_date',\n",
       "   'bios_vendor',\n",
       "   'bios_version',\n",
       "   'boot_id',\n",
       "   'bound',\n",
       "   'branch',\n",
       "   'bridge',\n",
       "   'broadcast',\n",
       "   'build',\n",
       "   'buildDate',\n",
       "   'build_date',\n",
       "   'build_phase',\n",
       "   'build_user',\n",
       "   'buildconfig',\n",
       "   'cachesize',\n",
       "   'call',\n",
       "   'cause',\n",
       "   'ceph_cluster',\n",
       "   'ceph_daemon',\n",
       "   'ceph_version',\n",
       "   'certificatesigningrequest',\n",
       "   'channel',\n",
       "   'chassis_vendor',\n",
       "   'chassis_version',\n",
       "   'check',\n",
       "   'checkName',\n",
       "   'cidr',\n",
       "   'claim_namespace',\n",
       "   'client',\n",
       "   'client_api_version',\n",
       "   'clocksource',\n",
       "   'cloud_type',\n",
       "   'cluster_addr',\n",
       "   'cluster_id',\n",
       "   'cluster_ip',\n",
       "   'cluster_version',\n",
       "   'code',\n",
       "   'collector',\n",
       "   'command',\n",
       "   'compiler',\n",
       "   'completion_mode',\n",
       "   'component',\n",
       "   'compression_mode',\n",
       "   'concurrency_policy',\n",
       "   'condition',\n",
       "   'config',\n",
       "   'configmap',\n",
       "   'connection_method',\n",
       "   'container',\n",
       "   'container_id',\n",
       "   'container_runtime_version',\n",
       "   'container_state',\n",
       "   'container_type',\n",
       "   'controller',\n",
       "   'core',\n",
       "   'cpu',\n",
       "   'crd',\n",
       "   'crd_name',\n",
       "   'created_by_kind',\n",
       "   'created_by_name',\n",
       "   'cronjob',\n",
       "   'csi_driver',\n",
       "   'csi_volume_handle',\n",
       "   'daemonset',\n",
       "   'data_pools',\n",
       "   'datapath',\n",
       "   'db_name',\n",
       "   'decision',\n",
       "   'deployment',\n",
       "   'deploymentconfig',\n",
       "   'description',\n",
       "   'device',\n",
       "   'device_class',\n",
       "   'device_ids',\n",
       "   'devices',\n",
       "   'dialer_name',\n",
       "   'dnsResolve',\n",
       "   'domainname',\n",
       "   'driver',\n",
       "   'driver_name',\n",
       "   'dry_run',\n",
       "   'duplex',\n",
       "   'effect',\n",
       "   'enabled',\n",
       "   'endpoint',\n",
       "   'engine',\n",
       "   'err',\n",
       "   'error',\n",
       "   'event',\n",
       "   'eventName',\n",
       "   'execute',\n",
       "   'exported_endpoint',\n",
       "   'exported_instance',\n",
       "   'exported_namespace',\n",
       "   'exported_pod',\n",
       "   'exported_service',\n",
       "   'extension_point',\n",
       "   'external_labels',\n",
       "   'external_name',\n",
       "   'family',\n",
       "   'field_validation',\n",
       "   'file',\n",
       "   'filter',\n",
       "   'flow_schema',\n",
       "   'from_version',\n",
       "   'frontend',\n",
       "   'fs_id',\n",
       "   'fstype',\n",
       "   'gitCommit',\n",
       "   'gitTreeState',\n",
       "   'gitVersion',\n",
       "   'git_commit',\n",
       "   'git_tree_state',\n",
       "   'git_version',\n",
       "   'go_version',\n",
       "   'goversion',\n",
       "   'group',\n",
       "   'grpc_code',\n",
       "   'grpc_method',\n",
       "   'grpc_service',\n",
       "   'grpc_status_code',\n",
       "   'grpc_type',\n",
       "   'handler',\n",
       "   'host',\n",
       "   'host_ip',\n",
       "   'host_network',\n",
       "   'hostname',\n",
       "   'id',\n",
       "   'id_like',\n",
       "   'image',\n",
       "   'image_id',\n",
       "   'image_spec',\n",
       "   'index',\n",
       "   'installed',\n",
       "   'instance',\n",
       "   'instance_id',\n",
       "   'integration',\n",
       "   'interface',\n",
       "   'internal_ip',\n",
       "   'interval',\n",
       "   'invoker',\n",
       "   'ip',\n",
       "   'ip_family',\n",
       "   'ipaddress',\n",
       "   'job',\n",
       "   'job_name',\n",
       "   'kernelVersion',\n",
       "   'kernel_version',\n",
       "   'key',\n",
       "   'kind',\n",
       "   'kubelet_version',\n",
       "   'kubeproxy_version',\n",
       "   'label_alertmanager',\n",
       "   'label_apiserver',\n",
       "   'label_app',\n",
       "   'label_app_kubernetes_io_component',\n",
       "   'label_app_kubernetes_io_instance',\n",
       "   'label_app_kubernetes_io_managed_by',\n",
       "   'label_app_kubernetes_io_name',\n",
       "   'label_app_kubernetes_io_part_of',\n",
       "   'label_app_kubernetes_io_version',\n",
       "   'label_app_openshift_io_runtime',\n",
       "   'label_app_openshift_io_runtime_namespace',\n",
       "   'label_app_openshift_io_runtime_version',\n",
       "   'label_beta_kubernetes_io_arch',\n",
       "   'label_beta_kubernetes_io_os',\n",
       "   'label_buildconfig',\n",
       "   'label_catalogsource_operators_coreos_com_update',\n",
       "   'label_cluster_name',\n",
       "   'label_com_company',\n",
       "   'label_component',\n",
       "   'label_contains',\n",
       "   'label_control_plane',\n",
       "   'label_controller_devfile_io_devworkspace_id',\n",
       "   'label_controller_devfile_io_devworkspace_name',\n",
       "   'label_controller_manager',\n",
       "   'label_controller_revision_hash',\n",
       "   'label_controller_tools_k8s_io',\n",
       "   'label_controller_uid',\n",
       "   'label_deployment',\n",
       "   'label_deploymentconfig',\n",
       "   'label_dns_operator_openshift_io_daemonset_dns',\n",
       "   'label_docker_registry',\n",
       "   'label_es_node_client',\n",
       "   'label_es_node_data',\n",
       "   'label_es_node_master',\n",
       "   'label_etcd',\n",
       "   'label_ingress_openshift_io_canary',\n",
       "   'label_ingresscanary_operator_openshift_io_daemonset_ingresscanary',\n",
       "   'label_ingresscontroller_operator_openshift_io_deployment_ingresscontroller',\n",
       "   'label_ingresscontroller_operator_openshift_io_hash',\n",
       "   'label_job_name',\n",
       "   'label_k8s_app',\n",
       "   'label_kube_controller_manager',\n",
       "   'label_kubernetes_io_arch',\n",
       "   'label_kubernetes_io_hostname',\n",
       "   'label_kubernetes_io_metadata_name',\n",
       "   'label_kubernetes_io_os',\n",
       "   'label_logging_infra',\n",
       "   'label_name',\n",
       "   'label_namespace',\n",
       "   'label_network_openshift_io_policy_group',\n",
       "   'label_node_hyperthread_enabled',\n",
       "   'label_node_name',\n",
       "   'label_node_openshift_io_os_id',\n",
       "   'label_node_role_kubernetes_io',\n",
       "   'label_node_role_kubernetes_io_master',\n",
       "   'label_noobaa_core',\n",
       "   'label_noobaa_db',\n",
       "   'label_noobaa_mgmt',\n",
       "   'label_noobaa_operator',\n",
       "   'label_oauth_apiserver_anti_affinity',\n",
       "   'label_oauth_openshift_anti_affinity',\n",
       "   'label_octopusexport',\n",
       "   'label_olm_catalog_source',\n",
       "   'label_olm_pod_spec_hash',\n",
       "   'label_openshift_apiserver_anti_affinity',\n",
       "   'label_openshift_app',\n",
       "   'label_openshift_io_build_config_name',\n",
       "   'label_openshift_io_build_name',\n",
       "   'label_openshift_io_build_start_policy',\n",
       "   'label_openshift_io_cluster_monitoring',\n",
       "   'label_openshift_io_component',\n",
       "   'label_openshift_io_deployer_pod_for_name',\n",
       "   'label_openshift_io_run_level',\n",
       "   'label_openshift_io_scc',\n",
       "   'label_operator_prometheus_io_name',\n",
       "   'label_operator_prometheus_io_shard',\n",
       "   'label_ovn_db_pod',\n",
       "   'label_pod_security_kubernetes_io_audit',\n",
       "   'label_pod_security_kubernetes_io_audit_version',\n",
       "   'label_pod_security_kubernetes_io_enforce',\n",
       "   'label_pod_security_kubernetes_io_warn',\n",
       "   'label_pod_security_kubernetes_io_warn_version',\n",
       "   'label_pod_template_generation',\n",
       "   'label_pod_template_hash',\n",
       "   'label_prometheus',\n",
       "   'label_provider',\n",
       "   'label_revision',\n",
       "   'label_rht_comp',\n",
       "   'label_rht_comp_t',\n",
       "   'label_rht_comp_ver',\n",
       "   'label_rht_prod_name',\n",
       "   'label_rht_prod_ver',\n",
       "   'label_run',\n",
       "   'label_scheduler',\n",
       "   'label_security_openshift_io_scc_pod_security_label_sync',\n",
       "   'label_service_ca',\n",
       "   'label_statefulset_kubernetes_io_pod_name',\n",
       "   'label_template',\n",
       "   'label_testlabel',\n",
       "   'label_type',\n",
       "   'latest_version',\n",
       "   'le',\n",
       "   'lease',\n",
       "   'level',\n",
       "   'listener_name',\n",
       "   'location',\n",
       "   'long_running',\n",
       "   'machine',\n",
       "   'machine_id',\n",
       "   'major',\n",
       "   'managedBy',\n",
       "   'mapping',\n",
       "   'mark',\n",
       "   'mediatype',\n",
       "   'member',\n",
       "   'metadata_pool',\n",
       "   'method',\n",
       "   'method_name',\n",
       "   'metric',\n",
       "   'metric_source',\n",
       "   'metrics_path',\n",
       "   'microcode',\n",
       "   'migrated',\n",
       "   'minor',\n",
       "   'mode',\n",
       "   'model',\n",
       "   'model_name',\n",
       "   'mountpoint',\n",
       "   'msg_type',\n",
       "   'mutatingwebhookconfiguration',\n",
       "   'name',\n",
       "   'namespace',\n",
       "   'nb_schema_version',\n",
       "   'network_name',\n",
       "   'networkpolicy',\n",
       "   'networks',\n",
       "   'nfs_path',\n",
       "   'nfs_server',\n",
       "   'node',\n",
       "   'nodename',\n",
       "   'objectstore',\n",
       "   'op',\n",
       "   'operation',\n",
       "   'operation_name',\n",
       "   'operation_type',\n",
       "   'operstate',\n",
       "   'os',\n",
       "   'osVersion',\n",
       "   'os_image',\n",
       "   'outcome',\n",
       "   'ovs_lib_version',\n",
       "   'owner_is_controller',\n",
       "   'owner_kind',\n",
       "   'owner_name',\n",
       "   'package',\n",
       "   'path',\n",
       "   'peer',\n",
       "   'persistentvolume',\n",
       "   'persistentvolumeclaim',\n",
       "   'phase',\n",
       "   'platform',\n",
       "   'plugin',\n",
       "   'plugin_name',\n",
       "   'pod',\n",
       "   'pod_ip',\n",
       "   'pod_uid',\n",
       "   'poddisruptionbudget',\n",
       "   'policy_level',\n",
       "   'policy_version',\n",
       "   'pool',\n",
       "   'pool_id',\n",
       "   'pool_name',\n",
       "   'port_name',\n",
       "   'port_number',\n",
       "   'port_protocol',\n",
       "   'pretty_name',\n",
       "   'priority',\n",
       "   'priority_class',\n",
       "   'priority_level',\n",
       "   'probe_type',\n",
       "   'product_family',\n",
       "   'product_name',\n",
       "   'product_version',\n",
       "   'profile',\n",
       "   'proto',\n",
       "   'protocol',\n",
       "   'provisioner',\n",
       "   'public_addr',\n",
       "   'quantile',\n",
       "   'queue',\n",
       "   'queue_name',\n",
       "   'rank',\n",
       "   'rcode',\n",
       "   'reason',\n",
       "   'reclaim_policy',\n",
       "   'registry',\n",
       "   'rejected',\n",
       "   'release',\n",
       "   'removed_release',\n",
       "   'replicaset',\n",
       "   'replicationcontroller',\n",
       "   'request_kind',\n",
       "   'request_operation',\n",
       "   'resource',\n",
       "   'resource_prefix',\n",
       "   'resource_type',\n",
       "   'resourcequota',\n",
       "   'result',\n",
       "   'revision',\n",
       "   'rgw_endpoint',\n",
       "   'role',\n",
       "   'route',\n",
       "   'router_name',\n",
       "   'rule_group',\n",
       "   'sb_schema_version',\n",
       "   'schedule',\n",
       "   'scheduled',\n",
       "   'scheduler',\n",
       "   'scope',\n",
       "   'scrape_job',\n",
       "   'secret',\n",
       "   'server',\n",
       "   'server_go_version',\n",
       "   'server_id',\n",
       "   'server_role',\n",
       "   'server_status',\n",
       "   'server_type',\n",
       "   'server_version',\n",
       "   'server_vote',\n",
       "   'service',\n",
       "   'service_kind',\n",
       "   'severity',\n",
       "   'shard_ordinal',\n",
       "   'signer_name',\n",
       "   'size',\n",
       "   'slice',\n",
       "   'socket_path',\n",
       "   'source',\n",
       "   'state',\n",
       "   'statefulset',\n",
       "   'status',\n",
       "   'status_code',\n",
       "   'stepping',\n",
       "   'storage',\n",
       "   'storage_class',\n",
       "   'storage_system',\n",
       "   'storageclass',\n",
       "   'store_type',\n",
       "   'strategy',\n",
       "   'subresource',\n",
       "   'succeeded',\n",
       "   'success',\n",
       "   'sysname',\n",
       "   'system_client',\n",
       "   'system_type',\n",
       "   'system_uuid',\n",
       "   'system_vendor',\n",
       "   'targetEndpoint',\n",
       "   'target_kind',\n",
       "   'target_name',\n",
       "   'target_namespace',\n",
       "   'tcpConnect',\n",
       "   'time_zone',\n",
       "   'tls_termination',\n",
       "   'to',\n",
       "   'to_kind',\n",
       "   'to_name',\n",
       "   'to_version',\n",
       "   'to_weight',\n",
       "   'topology',\n",
       "   'triggered_by',\n",
       "   'type',\n",
       "   'uid',\n",
       "   'ulimit',\n",
       "   'unit',\n",
       "   'upstream',\n",
       "   'url',\n",
       "   'usage',\n",
       "   'username',\n",
       "   'validatingwebhookconfiguration',\n",
       "   'vendor',\n",
       "   'verb',\n",
       "   'version',\n",
       "   'version_id',\n",
       "   'volume',\n",
       "   'volume_binding_mode',\n",
       "   'volume_mode',\n",
       "   'volume_plugin',\n",
       "   'volumename',\n",
       "   'webhook',\n",
       "   'work',\n",
       "   'workload',\n",
       "   'workload_type',\n",
       "   'zone',\n",
       "   'zones']})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "r = requests.get(\n",
    "    f\"https://{prometheus_host}/v1/labels\",\n",
    "    verify=False,\n",
    "    headers=kubeConfig.api_key,\n",
    ")\n",
    "r.status_code, r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': ['3639c944-358b-40f7-b2f1-912b62d2333c',\n",
       "  '651cac9e-d562-4040-a7c6-9692026388a8',\n",
       "  '96f3e336-c833-4c1f-885f-9c97979d8fd1',\n",
       "  '9ddbd942-0f1f-4ca8-948f-ce00f162f86a',\n",
       "  'a3acce37-9195-428a-8aca-89c548c266c0',\n",
       "  'd910f588-d8dc-44e7-86b7-68030155db51']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\n",
    "    f\"https://{prometheus_host}/v1/label/system_uuid/values\",\n",
    "    verify=False,\n",
    "    headers=kubeConfig.api_key,\n",
    ")\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'__name__': 'machine_cpu_cores',\n",
       "     'boot_id': '58f33e8d-1794-413c-843b-10288bb25356',\n",
       "     'endpoint': 'https-metrics',\n",
       "     'instance': '10.0.92.214:10250',\n",
       "     'job': 'kubelet',\n",
       "     'machine_id': 'd910f588d8dc44e786b768030155db51',\n",
       "     'metrics_path': '/metrics/cadvisor',\n",
       "     'namespace': 'kube-system',\n",
       "     'node': 'master-2.sharedocp4upi411ovn.lab.upshift.rdu2.redhat.com',\n",
       "     'service': 'kubelet',\n",
       "     'system_uuid': 'd910f588-d8dc-44e7-86b7-68030155db51'},\n",
       "    'value': [1671051626.318, '4']},\n",
       "   {'metric': {'__name__': 'machine_cpu_cores',\n",
       "     'boot_id': '87ada890-93b1-4036-b305-c79312f306e7',\n",
       "     'endpoint': 'https-metrics',\n",
       "     'instance': '10.0.92.177:10250',\n",
       "     'job': 'kubelet',\n",
       "     'machine_id': '651cac9ed5624040a7c69692026388a8',\n",
       "     'metrics_path': '/metrics/cadvisor',\n",
       "     'namespace': 'kube-system',\n",
       "     'node': 'master-1.sharedocp4upi411ovn.lab.upshift.rdu2.redhat.com',\n",
       "     'service': 'kubelet',\n",
       "     'system_uuid': '651cac9e-d562-4040-a7c6-9692026388a8'},\n",
       "    'value': [1671051626.318, '4']},\n",
       "   {'metric': {'__name__': 'machine_cpu_cores',\n",
       "     'boot_id': '98aa5a1e-9012-4e7c-a710-7876e8972726',\n",
       "     'endpoint': 'https-metrics',\n",
       "     'instance': '10.0.93.48:10250',\n",
       "     'job': 'kubelet',\n",
       "     'machine_id': '3639c944358b40f7b2f1912b62d2333c',\n",
       "     'metrics_path': '/metrics/cadvisor',\n",
       "     'namespace': 'kube-system',\n",
       "     'node': 'worker-2.sharedocp4upi411ovn.lab.upshift.rdu2.redhat.com',\n",
       "     'service': 'kubelet',\n",
       "     'system_uuid': '3639c944-358b-40f7-b2f1-912b62d2333c'},\n",
       "    'value': [1671051626.318, '4']},\n",
       "   {'metric': {'__name__': 'machine_cpu_cores',\n",
       "     'boot_id': '9919c256-c20c-4985-a3c4-08a7955d958c',\n",
       "     'endpoint': 'https-metrics',\n",
       "     'instance': '10.0.89.183:10250',\n",
       "     'job': 'kubelet',\n",
       "     'machine_id': '9ddbd9420f1f4ca8948fce00f162f86a',\n",
       "     'metrics_path': '/metrics/cadvisor',\n",
       "     'namespace': 'kube-system',\n",
       "     'node': 'worker-1.sharedocp4upi411ovn.lab.upshift.rdu2.redhat.com',\n",
       "     'service': 'kubelet',\n",
       "     'system_uuid': '9ddbd942-0f1f-4ca8-948f-ce00f162f86a'},\n",
       "    'value': [1671051626.318, '4']},\n",
       "   {'metric': {'__name__': 'machine_cpu_cores',\n",
       "     'boot_id': 'a2ad0e5b-6ace-45c7-9b13-f8e6f20cf101',\n",
       "     'endpoint': 'https-metrics',\n",
       "     'instance': '10.0.88.85:10250',\n",
       "     'job': 'kubelet',\n",
       "     'machine_id': '96f3e336c8334c1f885f9c97979d8fd1',\n",
       "     'metrics_path': '/metrics/cadvisor',\n",
       "     'namespace': 'kube-system',\n",
       "     'node': 'worker-0.sharedocp4upi411ovn.lab.upshift.rdu2.redhat.com',\n",
       "     'service': 'kubelet',\n",
       "     'system_uuid': '96f3e336-c833-4c1f-885f-9c97979d8fd1'},\n",
       "    'value': [1671051626.318, '4']},\n",
       "   {'metric': {'__name__': 'machine_cpu_cores',\n",
       "     'boot_id': 'cdf055d0-11ae-49e1-8729-7b12b132033c',\n",
       "     'endpoint': 'https-metrics',\n",
       "     'instance': '10.0.89.216:10250',\n",
       "     'job': 'kubelet',\n",
       "     'machine_id': 'a3acce379195428a8aca89c548c266c0',\n",
       "     'metrics_path': '/metrics/cadvisor',\n",
       "     'namespace': 'kube-system',\n",
       "     'node': 'master-0.sharedocp4upi411ovn.lab.upshift.rdu2.redhat.com',\n",
       "     'service': 'kubelet',\n",
       "     'system_uuid': 'a3acce37-9195-428a-8aca-89c548c266c0'},\n",
       "    'value': [1671051626.318, '4']}]}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\n",
    "    f\"https://{prometheus_host}/v1/query?query=machine_cpu_cores&query=machine_cpu_sockets\",\n",
    "    verify=False,\n",
    "    headers=kubeConfig.api_key,\n",
    ")\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\n",
    "    f\"https://{prometheus_host}/v1/metrics\",\n",
    "    verify=False,\n",
    "    headers=kubeConfig.api_key,\n",
    ")\n",
    "r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('openshift-poc-LA5XNyZa-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5137217e5745a6a9de879e2c6663edb7f019d88c9c3887719f710afebf6e60a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
